{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3_series_feature.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available gpus: [0]\n"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc, os, random\n",
    "import time, datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from utils import *\n",
    "root = args.root\n",
    "seed = args.seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_feather(\"./input/train_denoise.feather\")\n",
    "test = pd.read_feather(\"./input/test_denoise.feather\")\n",
    "\n",
    "train_y = pd.read_csv(f\"/mnt/sdb/KAGGLE_DATA/amex-default-prediction/train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## debug --\n",
    "## train = train.head(10000)\n",
    "## test = test.head(10000)\n",
    "## train_y = train_y.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(train_y, how=\"left\", on=id_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(df, cols, is_drop=True):\n",
    "    for col in cols:\n",
    "        print(\"one hot encoding:\", col)\n",
    "        dummies = pd.get_dummies(pd.Series(df[col]), prefix=\"oneHot_%s\"%col)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    if is_drop:\n",
    "        df.drop(cols, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n",
    "eps = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5531451, 191) (11363762, 190)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_config = {\n",
    "    \"lgb_params\": {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"boosting\": \"dart\",\n",
    "        \"max_depth\": -1,\n",
    "        \"num_leaves\": 64,\n",
    "        \"learning_rate\": 0.035,\n",
    "        \"bagging_freq\": 5,\n",
    "        \"bagging_fraction\": 0.7,\n",
    "        \"feature_fraction\": 0.7,\n",
    "        \"min_data_in_leaf\": 256,\n",
    "        \"max_bin\": 63,\n",
    "        #\"min_sum_hessian_in_leaf\": 10,\n",
    "        \"tree_learner\": \"serial\",\n",
    "        \"boost_from_average\": \"false\",\n",
    "        \"lambda_l1\": 0.1,\n",
    "        \"lambda_l2\": 30,\n",
    "        \"num_threads\": 16,\n",
    "        \"verbosity\": 1,\n",
    "    },\n",
    "    \"feature_name\": [col for col in train.columns if col not in [id_name, label_name, \"S_2\"]],\n",
    "    \"rounds\": 4500,\n",
    "    \"early_stopping_rounds\": 100,\n",
    "    \"verbose_eval\": 50,\n",
    "    \"folds\": 5,\n",
    "    \"seed\": seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: cannot stat './*.sh': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1102596, number of negative: 3323080\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.574514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6733\n",
      "[LightGBM] [Info] Number of data points in the train set: 4425676, number of used features: 188\n",
      "[50]\ttraining's binary_logloss: 0.424557\tvalid_1's binary_logloss: 0.423357\n",
      "[100]\ttraining's binary_logloss: 0.37102\tvalid_1's binary_logloss: 0.369618\n",
      "[150]\ttraining's binary_logloss: 0.348986\tvalid_1's binary_logloss: 0.34755\n",
      "[200]\ttraining's binary_logloss: 0.338116\tvalid_1's binary_logloss: 0.336706\n",
      "[250]\ttraining's binary_logloss: 0.314579\tvalid_1's binary_logloss: 0.313154\n",
      "[300]\ttraining's binary_logloss: 0.302556\tvalid_1's binary_logloss: 0.301218\n",
      "[350]\ttraining's binary_logloss: 0.294663\tvalid_1's binary_logloss: 0.293401\n",
      "[400]\ttraining's binary_logloss: 0.291736\tvalid_1's binary_logloss: 0.290555\n",
      "[450]\ttraining's binary_logloss: 0.290291\tvalid_1's binary_logloss: 0.289136\n",
      "[500]\ttraining's binary_logloss: 0.281771\tvalid_1's binary_logloss: 0.280769\n",
      "[550]\ttraining's binary_logloss: 0.279231\tvalid_1's binary_logloss: 0.278332\n",
      "[600]\ttraining's binary_logloss: 0.277519\tvalid_1's binary_logloss: 0.276696\n",
      "[650]\ttraining's binary_logloss: 0.27362\tvalid_1's binary_logloss: 0.273018\n",
      "[700]\ttraining's binary_logloss: 0.271569\tvalid_1's binary_logloss: 0.271208\n",
      "[750]\ttraining's binary_logloss: 0.269374\tvalid_1's binary_logloss: 0.269348\n",
      "[800]\ttraining's binary_logloss: 0.268024\tvalid_1's binary_logloss: 0.268267\n",
      "[850]\ttraining's binary_logloss: 0.266137\tvalid_1's binary_logloss: 0.266791\n",
      "[900]\ttraining's binary_logloss: 0.265249\tvalid_1's binary_logloss: 0.26617\n",
      "[950]\ttraining's binary_logloss: 0.265206\tvalid_1's binary_logloss: 0.266277\n",
      "[1000]\ttraining's binary_logloss: 0.264162\tvalid_1's binary_logloss: 0.265505\n",
      "[1050]\ttraining's binary_logloss: 0.263532\tvalid_1's binary_logloss: 0.265097\n",
      "[1100]\ttraining's binary_logloss: 0.262061\tvalid_1's binary_logloss: 0.264026\n",
      "[1150]\ttraining's binary_logloss: 0.262009\tvalid_1's binary_logloss: 0.264121\n",
      "[1200]\ttraining's binary_logloss: 0.26141\tvalid_1's binary_logloss: 0.263769\n",
      "[1250]\ttraining's binary_logloss: 0.260841\tvalid_1's binary_logloss: 0.263447\n",
      "[1300]\ttraining's binary_logloss: 0.260546\tvalid_1's binary_logloss: 0.26332\n",
      "[1350]\ttraining's binary_logloss: 0.260284\tvalid_1's binary_logloss: 0.263229\n",
      "[1400]\ttraining's binary_logloss: 0.259324\tvalid_1's binary_logloss: 0.262625\n",
      "[1450]\ttraining's binary_logloss: 0.259045\tvalid_1's binary_logloss: 0.262554\n",
      "[1500]\ttraining's binary_logloss: 0.258759\tvalid_1's binary_logloss: 0.262459\n",
      "[1550]\ttraining's binary_logloss: 0.257744\tvalid_1's binary_logloss: 0.261871\n",
      "[1600]\ttraining's binary_logloss: 0.257016\tvalid_1's binary_logloss: 0.2615\n",
      "[1650]\ttraining's binary_logloss: 0.256435\tvalid_1's binary_logloss: 0.261235\n",
      "[1700]\ttraining's binary_logloss: 0.256138\tvalid_1's binary_logloss: 0.261177\n",
      "[1750]\ttraining's binary_logloss: 0.255834\tvalid_1's binary_logloss: 0.261105\n",
      "[1800]\ttraining's binary_logloss: 0.255678\tvalid_1's binary_logloss: 0.26113\n",
      "[1850]\ttraining's binary_logloss: 0.25525\tvalid_1's binary_logloss: 0.260992\n",
      "[1900]\ttraining's binary_logloss: 0.254821\tvalid_1's binary_logloss: 0.260832\n",
      "[1950]\ttraining's binary_logloss: 0.254242\tvalid_1's binary_logloss: 0.260589\n",
      "[2000]\ttraining's binary_logloss: 0.253818\tvalid_1's binary_logloss: 0.260472\n",
      "[2050]\ttraining's binary_logloss: 0.253529\tvalid_1's binary_logloss: 0.26044\n",
      "[2100]\ttraining's binary_logloss: 0.253218\tvalid_1's binary_logloss: 0.260379\n",
      "[2150]\ttraining's binary_logloss: 0.253196\tvalid_1's binary_logloss: 0.260467\n",
      "[2200]\ttraining's binary_logloss: 0.252723\tvalid_1's binary_logloss: 0.260316\n",
      "[2250]\ttraining's binary_logloss: 0.252369\tvalid_1's binary_logloss: 0.260227\n",
      "[2300]\ttraining's binary_logloss: 0.251881\tvalid_1's binary_logloss: 0.260051\n",
      "[2350]\ttraining's binary_logloss: 0.251606\tvalid_1's binary_logloss: 0.260016\n",
      "[2400]\ttraining's binary_logloss: 0.251374\tvalid_1's binary_logloss: 0.260004\n",
      "[2450]\ttraining's binary_logloss: 0.251002\tvalid_1's binary_logloss: 0.259916\n",
      "[2500]\ttraining's binary_logloss: 0.250812\tvalid_1's binary_logloss: 0.25991\n",
      "[2550]\ttraining's binary_logloss: 0.250486\tvalid_1's binary_logloss: 0.259855\n",
      "[2600]\ttraining's binary_logloss: 0.250163\tvalid_1's binary_logloss: 0.259806\n",
      "[2650]\ttraining's binary_logloss: 0.249839\tvalid_1's binary_logloss: 0.259731\n",
      "[2700]\ttraining's binary_logloss: 0.249419\tvalid_1's binary_logloss: 0.259618\n",
      "[2750]\ttraining's binary_logloss: 0.249269\tvalid_1's binary_logloss: 0.259644\n",
      "[2800]\ttraining's binary_logloss: 0.249039\tvalid_1's binary_logloss: 0.259611\n",
      "[2850]\ttraining's binary_logloss: 0.248512\tvalid_1's binary_logloss: 0.259473\n",
      "[2900]\ttraining's binary_logloss: 0.248266\tvalid_1's binary_logloss: 0.259433\n",
      "[2950]\ttraining's binary_logloss: 0.247825\tvalid_1's binary_logloss: 0.259338\n",
      "[3000]\ttraining's binary_logloss: 0.247468\tvalid_1's binary_logloss: 0.259294\n",
      "[3050]\ttraining's binary_logloss: 0.247166\tvalid_1's binary_logloss: 0.259245\n",
      "[3100]\ttraining's binary_logloss: 0.246898\tvalid_1's binary_logloss: 0.259211\n",
      "[3150]\ttraining's binary_logloss: 0.246659\tvalid_1's binary_logloss: 0.259182\n",
      "[3200]\ttraining's binary_logloss: 0.246249\tvalid_1's binary_logloss: 0.259114\n",
      "[3250]\ttraining's binary_logloss: 0.245924\tvalid_1's binary_logloss: 0.259085\n",
      "[3300]\ttraining's binary_logloss: 0.245632\tvalid_1's binary_logloss: 0.259054\n",
      "[3350]\ttraining's binary_logloss: 0.245226\tvalid_1's binary_logloss: 0.258987\n",
      "[3400]\ttraining's binary_logloss: 0.244828\tvalid_1's binary_logloss: 0.258935\n",
      "[3450]\ttraining's binary_logloss: 0.244454\tvalid_1's binary_logloss: 0.258892\n",
      "[3500]\ttraining's binary_logloss: 0.244215\tvalid_1's binary_logloss: 0.258887\n",
      "[3550]\ttraining's binary_logloss: 0.243923\tvalid_1's binary_logloss: 0.258859\n",
      "[3600]\ttraining's binary_logloss: 0.243634\tvalid_1's binary_logloss: 0.258835\n",
      "[3650]\ttraining's binary_logloss: 0.243329\tvalid_1's binary_logloss: 0.258802\n",
      "[3700]\ttraining's binary_logloss: 0.242795\tvalid_1's binary_logloss: 0.258706\n",
      "[3750]\ttraining's binary_logloss: 0.2425\tvalid_1's binary_logloss: 0.25869\n",
      "[3800]\ttraining's binary_logloss: 0.242255\tvalid_1's binary_logloss: 0.258702\n",
      "[3850]\ttraining's binary_logloss: 0.241966\tvalid_1's binary_logloss: 0.258696\n",
      "[3900]\ttraining's binary_logloss: 0.241754\tvalid_1's binary_logloss: 0.2587\n",
      "[3950]\ttraining's binary_logloss: 0.241679\tvalid_1's binary_logloss: 0.258737\n",
      "[4000]\ttraining's binary_logloss: 0.241265\tvalid_1's binary_logloss: 0.258683\n",
      "[4050]\ttraining's binary_logloss: 0.241005\tvalid_1's binary_logloss: 0.258655\n",
      "[4100]\ttraining's binary_logloss: 0.240668\tvalid_1's binary_logloss: 0.258627\n",
      "[4150]\ttraining's binary_logloss: 0.240224\tvalid_1's binary_logloss: 0.258561\n",
      "[4200]\ttraining's binary_logloss: 0.239929\tvalid_1's binary_logloss: 0.258555\n",
      "[4250]\ttraining's binary_logloss: 0.239621\tvalid_1's binary_logloss: 0.258546\n",
      "[4300]\ttraining's binary_logloss: 0.239185\tvalid_1's binary_logloss: 0.258483\n",
      "[4350]\ttraining's binary_logloss: 0.239026\tvalid_1's binary_logloss: 0.258506\n",
      "[4400]\ttraining's binary_logloss: 0.238832\tvalid_1's binary_logloss: 0.258512\n",
      "[4450]\ttraining's binary_logloss: 0.238468\tvalid_1's binary_logloss: 0.258471\n",
      "[4500]\ttraining's binary_logloss: 0.238258\tvalid_1's binary_logloss: 0.258468\n",
      " - 0 round - train_metric: 0.672415 - valid_metric: 0.672318\n",
      "\n",
      " - 50 round - train_metric: 0.426814 - valid_metric: 0.425624\n",
      "\n",
      " - 100 round - train_metric: 0.367434 - valid_metric: 0.366020\n",
      "\n",
      " - 150 round - train_metric: 0.350622 - valid_metric: 0.349194\n",
      "\n",
      " - 200 round - train_metric: 0.339175 - valid_metric: 0.337769\n",
      "\n",
      " - 250 round - train_metric: 0.313202 - valid_metric: 0.311781\n",
      "\n",
      " - 300 round - train_metric: 0.301562 - valid_metric: 0.300228\n",
      "\n",
      " - 350 round - train_metric: 0.295099 - valid_metric: 0.293835\n",
      "\n",
      " - 400 round - train_metric: 0.292069 - valid_metric: 0.290887\n",
      "\n",
      " - 450 round - train_metric: 0.290586 - valid_metric: 0.289429\n",
      "\n",
      " - 500 round - train_metric: 0.281437 - valid_metric: 0.280444\n",
      "\n",
      " - 550 round - train_metric: 0.278941 - valid_metric: 0.278051\n",
      "\n",
      " - 600 round - train_metric: 0.277268 - valid_metric: 0.276453\n",
      "\n",
      " - 650 round - train_metric: 0.273711 - valid_metric: 0.273108\n",
      "\n",
      " - 700 round - train_metric: 0.271654 - valid_metric: 0.271291\n",
      "\n",
      " - 750 round - train_metric: 0.269449 - valid_metric: 0.269418\n",
      "\n",
      " - 800 round - train_metric: 0.268077 - valid_metric: 0.268318\n",
      "\n",
      " - 850 round - train_metric: 0.266052 - valid_metric: 0.266721\n",
      "\n",
      " - 900 round - train_metric: 0.265169 - valid_metric: 0.266104\n",
      "\n",
      " - 950 round - train_metric: 0.265132 - valid_metric: 0.266215\n",
      "\n",
      " - 1000 round - train_metric: 0.264204 - valid_metric: 0.265543\n",
      "\n",
      " - 1050 round - train_metric: 0.263462 - valid_metric: 0.265040\n",
      "\n",
      " - 1100 round - train_metric: 0.262008 - valid_metric: 0.263986\n",
      "\n",
      " - 1150 round - train_metric: 0.262038 - valid_metric: 0.264147\n",
      "\n",
      " - 1200 round - train_metric: 0.261440 - valid_metric: 0.263794\n",
      "\n",
      " - 1250 round - train_metric: 0.260865 - valid_metric: 0.263469\n",
      "\n",
      " - 1300 round - train_metric: 0.260571 - valid_metric: 0.263342\n",
      "\n",
      " - 1350 round - train_metric: 0.260312 - valid_metric: 0.263252\n",
      "\n",
      " - 1400 round - train_metric: 0.259283 - valid_metric: 0.262595\n",
      "\n",
      " - 1450 round - train_metric: 0.259070 - valid_metric: 0.262574\n",
      "\n",
      " - 1500 round - train_metric: 0.258720 - valid_metric: 0.262431\n",
      "\n",
      " - 1550 round - train_metric: 0.257704 - valid_metric: 0.261851\n",
      "\n",
      " - 1600 round - train_metric: 0.257037 - valid_metric: 0.261515\n",
      "\n",
      " - 1650 round - train_metric: 0.256405 - valid_metric: 0.261218\n",
      "\n",
      " - 1700 round - train_metric: 0.256106 - valid_metric: 0.261160\n",
      "\n",
      " - 1750 round - train_metric: 0.255852 - valid_metric: 0.261118\n",
      "\n",
      " - 1800 round - train_metric: 0.255643 - valid_metric: 0.261114\n",
      "\n",
      " - 1850 round - train_metric: 0.255272 - valid_metric: 0.261008\n",
      "\n",
      " - 1900 round - train_metric: 0.254791 - valid_metric: 0.260818\n",
      "\n",
      " - 1950 round - train_metric: 0.254256 - valid_metric: 0.260599\n",
      "\n",
      " - 2000 round - train_metric: 0.253790 - valid_metric: 0.260462\n",
      "\n",
      " - 2050 round - train_metric: 0.253542 - valid_metric: 0.260449\n",
      "\n",
      " - 2100 round - train_metric: 0.253229 - valid_metric: 0.260387\n",
      "\n",
      " - 2150 round - train_metric: 0.253213 - valid_metric: 0.260479\n",
      "\n",
      " - 2200 round - train_metric: 0.252738 - valid_metric: 0.260325\n",
      "\n",
      " - 2250 round - train_metric: 0.252339 - valid_metric: 0.260212\n",
      "\n",
      " - 2300 round - train_metric: 0.251895 - valid_metric: 0.260060\n",
      "\n",
      " - 2350 round - train_metric: 0.251621 - valid_metric: 0.260026\n",
      "\n",
      " - 2400 round - train_metric: 0.251348 - valid_metric: 0.259993\n",
      "\n",
      " - 2450 round - train_metric: 0.250975 - valid_metric: 0.259903\n",
      "\n",
      " - 2500 round - train_metric: 0.250821 - valid_metric: 0.259916\n",
      "\n",
      " - 2550 round - train_metric: 0.250462 - valid_metric: 0.259844\n",
      "\n",
      " - 2600 round - train_metric: 0.250170 - valid_metric: 0.259812\n",
      "\n",
      " - 2650 round - train_metric: 0.249847 - valid_metric: 0.259735\n",
      "\n",
      " - 2700 round - train_metric: 0.249430 - valid_metric: 0.259625\n",
      "\n",
      " - 2750 round - train_metric: 0.249278 - valid_metric: 0.259649\n",
      "\n",
      " - 2800 round - train_metric: 0.249014 - valid_metric: 0.259603\n",
      "\n",
      " - 2850 round - train_metric: 0.248520 - valid_metric: 0.259479\n",
      "\n",
      " - 2900 round - train_metric: 0.248278 - valid_metric: 0.259440\n",
      "\n",
      " - 2950 round - train_metric: 0.247833 - valid_metric: 0.259343\n",
      "\n",
      " - 3000 round - train_metric: 0.247475 - valid_metric: 0.259299\n",
      "\n",
      " - 3050 round - train_metric: 0.247150 - valid_metric: 0.259239\n",
      "\n",
      " - 3100 round - train_metric: 0.246904 - valid_metric: 0.259215\n",
      "\n",
      " - 3150 round - train_metric: 0.246668 - valid_metric: 0.259187\n",
      "\n",
      " - 3200 round - train_metric: 0.246225 - valid_metric: 0.259112\n",
      "\n",
      " - 3250 round - train_metric: 0.245930 - valid_metric: 0.259088\n",
      "\n",
      " - 3300 round - train_metric: 0.245640 - valid_metric: 0.259058\n",
      "\n",
      " - 3350 round - train_metric: 0.245234 - valid_metric: 0.258991\n",
      "\n",
      " - 3400 round - train_metric: 0.244836 - valid_metric: 0.258939\n",
      "\n",
      " - 3450 round - train_metric: 0.244436 - valid_metric: 0.258890\n",
      "\n",
      " - 3500 round - train_metric: 0.244196 - valid_metric: 0.258878\n",
      "\n",
      " - 3550 round - train_metric: 0.243929 - valid_metric: 0.258862\n",
      "\n",
      " - 3600 round - train_metric: 0.243641 - valid_metric: 0.258839\n",
      "\n",
      " - 3650 round - train_metric: 0.243339 - valid_metric: 0.258807\n",
      "\n",
      " - 3700 round - train_metric: 0.242776 - valid_metric: 0.258701\n",
      "\n",
      " - 3750 round - train_metric: 0.242480 - valid_metric: 0.258691\n",
      "\n",
      " - 3800 round - train_metric: 0.242234 - valid_metric: 0.258700\n",
      "\n",
      " - 3850 round - train_metric: 0.241946 - valid_metric: 0.258691\n",
      "\n",
      " - 3900 round - train_metric: 0.241759 - valid_metric: 0.258702\n",
      "\n",
      " - 3950 round - train_metric: 0.241659 - valid_metric: 0.258730\n",
      "\n",
      " - 4000 round - train_metric: 0.241248 - valid_metric: 0.258679\n",
      "\n",
      " - 4050 round - train_metric: 0.241011 - valid_metric: 0.258657\n",
      "\n",
      " - 4100 round - train_metric: 0.240675 - valid_metric: 0.258631\n",
      "\n",
      " - 4150 round - train_metric: 0.240231 - valid_metric: 0.258563\n",
      "\n",
      " - 4200 round - train_metric: 0.239935 - valid_metric: 0.258558\n",
      "\n",
      " - 4250 round - train_metric: 0.239627 - valid_metric: 0.258548\n",
      "\n",
      " - 4300 round - train_metric: 0.239194 - valid_metric: 0.258487\n",
      "\n",
      " - 4350 round - train_metric: 0.239010 - valid_metric: 0.258504\n",
      "\n",
      " - 4400 round - train_metric: 0.238817 - valid_metric: 0.258510\n",
      "\n",
      " - 4450 round - train_metric: 0.238474 - valid_metric: 0.258473\n",
      "\n",
      "- fold0 valid metric: 0.717571\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 1102318, number of negative: 3323374\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.422301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6736\n",
      "[LightGBM] [Info] Number of data points in the train set: 4425692, number of used features: 188\n",
      "[50]\ttraining's binary_logloss: 0.424062\tvalid_1's binary_logloss: 0.424581\n",
      "[100]\ttraining's binary_logloss: 0.370351\tvalid_1's binary_logloss: 0.371198\n",
      "[150]\ttraining's binary_logloss: 0.348319\tvalid_1's binary_logloss: 0.349336\n",
      "[200]\ttraining's binary_logloss: 0.337469\tvalid_1's binary_logloss: 0.338648\n",
      "[250]\ttraining's binary_logloss: 0.31382\tvalid_1's binary_logloss: 0.315329\n",
      "[300]\ttraining's binary_logloss: 0.301838\tvalid_1's binary_logloss: 0.303614\n",
      "[350]\ttraining's binary_logloss: 0.293893\tvalid_1's binary_logloss: 0.295957\n",
      "[400]\ttraining's binary_logloss: 0.290964\tvalid_1's binary_logloss: 0.293161\n",
      "[450]\ttraining's binary_logloss: 0.289541\tvalid_1's binary_logloss: 0.291805\n",
      "[500]\ttraining's binary_logloss: 0.280908\tvalid_1's binary_logloss: 0.283614\n",
      "[550]\ttraining's binary_logloss: 0.278378\tvalid_1's binary_logloss: 0.281292\n",
      "[600]\ttraining's binary_logloss: 0.276618\tvalid_1's binary_logloss: 0.27977\n",
      "[650]\ttraining's binary_logloss: 0.272707\tvalid_1's binary_logloss: 0.276325\n",
      "[700]\ttraining's binary_logloss: 0.270614\tvalid_1's binary_logloss: 0.274572\n",
      "[750]\ttraining's binary_logloss: 0.26835\tvalid_1's binary_logloss: 0.272751\n",
      "[800]\ttraining's binary_logloss: 0.266968\tvalid_1's binary_logloss: 0.271677\n",
      "[850]\ttraining's binary_logloss: 0.265069\tvalid_1's binary_logloss: 0.270234\n",
      "[900]\ttraining's binary_logloss: 0.264209\tvalid_1's binary_logloss: 0.269676\n",
      "[950]\ttraining's binary_logloss: 0.264202\tvalid_1's binary_logloss: 0.269768\n",
      "[1000]\ttraining's binary_logloss: 0.263134\tvalid_1's binary_logloss: 0.269043\n",
      "[1050]\ttraining's binary_logloss: 0.262487\tvalid_1's binary_logloss: 0.268649\n",
      "[1100]\ttraining's binary_logloss: 0.261061\tvalid_1's binary_logloss: 0.267747\n",
      "[1150]\ttraining's binary_logloss: 0.261014\tvalid_1's binary_logloss: 0.267818\n",
      "[1200]\ttraining's binary_logloss: 0.260402\tvalid_1's binary_logloss: 0.26748\n",
      "[1250]\ttraining's binary_logloss: 0.259815\tvalid_1's binary_logloss: 0.267178\n",
      "[1300]\ttraining's binary_logloss: 0.259505\tvalid_1's binary_logloss: 0.267044\n",
      "[1350]\ttraining's binary_logloss: 0.259245\tvalid_1's binary_logloss: 0.266961\n",
      "[1400]\ttraining's binary_logloss: 0.258281\tvalid_1's binary_logloss: 0.266432\n",
      "[1450]\ttraining's binary_logloss: 0.257983\tvalid_1's binary_logloss: 0.266356\n",
      "[1500]\ttraining's binary_logloss: 0.257693\tvalid_1's binary_logloss: 0.266267\n",
      "[1550]\ttraining's binary_logloss: 0.256674\tvalid_1's binary_logloss: 0.265735\n",
      "[1600]\ttraining's binary_logloss: 0.25595\tvalid_1's binary_logloss: 0.265401\n",
      "[1650]\ttraining's binary_logloss: 0.25538\tvalid_1's binary_logloss: 0.26518\n",
      "[1700]\ttraining's binary_logloss: 0.255084\tvalid_1's binary_logloss: 0.265129\n",
      "[1750]\ttraining's binary_logloss: 0.254772\tvalid_1's binary_logloss: 0.265052\n",
      "[1800]\ttraining's binary_logloss: 0.254618\tvalid_1's binary_logloss: 0.265068\n",
      "[1850]\ttraining's binary_logloss: 0.254225\tvalid_1's binary_logloss: 0.264986\n",
      "[1900]\ttraining's binary_logloss: 0.253805\tvalid_1's binary_logloss: 0.264849\n",
      "[1950]\ttraining's binary_logloss: 0.253211\tvalid_1's binary_logloss: 0.26465\n",
      "[2000]\ttraining's binary_logloss: 0.252821\tvalid_1's binary_logloss: 0.264544\n",
      "[2050]\ttraining's binary_logloss: 0.252511\tvalid_1's binary_logloss: 0.264483\n",
      "[2100]\ttraining's binary_logloss: 0.252205\tvalid_1's binary_logloss: 0.264413\n",
      "[2150]\ttraining's binary_logloss: 0.252173\tvalid_1's binary_logloss: 0.264461\n",
      "[2200]\ttraining's binary_logloss: 0.25168\tvalid_1's binary_logloss: 0.264335\n",
      "[2250]\ttraining's binary_logloss: 0.251337\tvalid_1's binary_logloss: 0.264237\n",
      "[2300]\ttraining's binary_logloss: 0.250874\tvalid_1's binary_logloss: 0.264093\n",
      "[2350]\ttraining's binary_logloss: 0.25059\tvalid_1's binary_logloss: 0.264061\n",
      "[2400]\ttraining's binary_logloss: 0.250342\tvalid_1's binary_logloss: 0.264052\n",
      "[2450]\ttraining's binary_logloss: 0.249988\tvalid_1's binary_logloss: 0.263975\n",
      "[2500]\ttraining's binary_logloss: 0.249807\tvalid_1's binary_logloss: 0.263963\n",
      "[2550]\ttraining's binary_logloss: 0.249482\tvalid_1's binary_logloss: 0.263908\n",
      "[2600]\ttraining's binary_logloss: 0.249146\tvalid_1's binary_logloss: 0.263873\n",
      "[2650]\ttraining's binary_logloss: 0.248806\tvalid_1's binary_logloss: 0.263801\n",
      "[2700]\ttraining's binary_logloss: 0.248394\tvalid_1's binary_logloss: 0.263722\n",
      "[2750]\ttraining's binary_logloss: 0.248243\tvalid_1's binary_logloss: 0.263736\n",
      "[2800]\ttraining's binary_logloss: 0.247987\tvalid_1's binary_logloss: 0.263711\n",
      "[2850]\ttraining's binary_logloss: 0.24746\tvalid_1's binary_logloss: 0.26358\n",
      "[2900]\ttraining's binary_logloss: 0.247213\tvalid_1's binary_logloss: 0.263543\n",
      "[2950]\ttraining's binary_logloss: 0.246785\tvalid_1's binary_logloss: 0.263476\n",
      "[3000]\ttraining's binary_logloss: 0.246421\tvalid_1's binary_logloss: 0.263406\n",
      "[3050]\ttraining's binary_logloss: 0.246128\tvalid_1's binary_logloss: 0.263375\n",
      "[3100]\ttraining's binary_logloss: 0.245838\tvalid_1's binary_logloss: 0.263338\n",
      "[3150]\ttraining's binary_logloss: 0.245593\tvalid_1's binary_logloss: 0.26332\n",
      "[3200]\ttraining's binary_logloss: 0.245185\tvalid_1's binary_logloss: 0.263263\n",
      "[3250]\ttraining's binary_logloss: 0.244857\tvalid_1's binary_logloss: 0.263224\n",
      "[3300]\ttraining's binary_logloss: 0.244583\tvalid_1's binary_logloss: 0.263178\n",
      "[3350]\ttraining's binary_logloss: 0.244186\tvalid_1's binary_logloss: 0.263126\n",
      "[3400]\ttraining's binary_logloss: 0.243749\tvalid_1's binary_logloss: 0.263065\n",
      "[3450]\ttraining's binary_logloss: 0.243382\tvalid_1's binary_logloss: 0.263023\n",
      "[3500]\ttraining's binary_logloss: 0.243149\tvalid_1's binary_logloss: 0.26301\n",
      "[3550]\ttraining's binary_logloss: 0.242848\tvalid_1's binary_logloss: 0.262986\n",
      "[3600]\ttraining's binary_logloss: 0.242557\tvalid_1's binary_logloss: 0.262987\n",
      "[3650]\ttraining's binary_logloss: 0.242257\tvalid_1's binary_logloss: 0.26298\n",
      "[3700]\ttraining's binary_logloss: 0.241706\tvalid_1's binary_logloss: 0.262916\n",
      "[3750]\ttraining's binary_logloss: 0.241396\tvalid_1's binary_logloss: 0.262899\n",
      "[3800]\ttraining's binary_logloss: 0.241131\tvalid_1's binary_logloss: 0.262889\n",
      "[3850]\ttraining's binary_logloss: 0.240839\tvalid_1's binary_logloss: 0.262878\n",
      "[3900]\ttraining's binary_logloss: 0.240655\tvalid_1's binary_logloss: 0.262858\n",
      "[3950]\ttraining's binary_logloss: 0.24058\tvalid_1's binary_logloss: 0.262881\n",
      "[4000]\ttraining's binary_logloss: 0.240173\tvalid_1's binary_logloss: 0.262844\n",
      "[4050]\ttraining's binary_logloss: 0.239908\tvalid_1's binary_logloss: 0.262853\n",
      "[4100]\ttraining's binary_logloss: 0.239596\tvalid_1's binary_logloss: 0.262828\n",
      "[4150]\ttraining's binary_logloss: 0.239137\tvalid_1's binary_logloss: 0.262788\n",
      "[4200]\ttraining's binary_logloss: 0.238873\tvalid_1's binary_logloss: 0.262773\n",
      "[4250]\ttraining's binary_logloss: 0.23857\tvalid_1's binary_logloss: 0.262766\n",
      "[4300]\ttraining's binary_logloss: 0.238156\tvalid_1's binary_logloss: 0.262743\n",
      "[4350]\ttraining's binary_logloss: 0.238007\tvalid_1's binary_logloss: 0.262741\n",
      "[4400]\ttraining's binary_logloss: 0.237809\tvalid_1's binary_logloss: 0.262752\n",
      "[4450]\ttraining's binary_logloss: 0.237424\tvalid_1's binary_logloss: 0.262717\n",
      "[4500]\ttraining's binary_logloss: 0.2372\tvalid_1's binary_logloss: 0.262714\n",
      " - 0 round - train_metric: 0.672378 - valid_metric: 0.672388\n",
      "\n",
      " - 50 round - train_metric: 0.426325 - valid_metric: 0.426833\n",
      "\n",
      " - 100 round - train_metric: 0.366763 - valid_metric: 0.367651\n",
      "\n",
      " - 150 round - train_metric: 0.349958 - valid_metric: 0.350961\n",
      "\n",
      " - 200 round - train_metric: 0.338528 - valid_metric: 0.339695\n",
      "\n",
      " - 250 round - train_metric: 0.312404 - valid_metric: 0.313932\n",
      "\n",
      " - 300 round - train_metric: 0.300830 - valid_metric: 0.302632\n",
      "\n",
      " - 350 round - train_metric: 0.294331 - valid_metric: 0.296382\n",
      "\n",
      " - 400 round - train_metric: 0.291298 - valid_metric: 0.293485\n",
      "\n",
      " - 450 round - train_metric: 0.289836 - valid_metric: 0.292090\n",
      "\n",
      " - 500 round - train_metric: 0.280558 - valid_metric: 0.283288\n",
      "\n",
      " - 550 round - train_metric: 0.278073 - valid_metric: 0.281018\n",
      "\n",
      " - 600 round - train_metric: 0.276366 - valid_metric: 0.279536\n",
      "\n",
      " - 650 round - train_metric: 0.272799 - valid_metric: 0.276409\n",
      "\n",
      " - 700 round - train_metric: 0.270701 - valid_metric: 0.274649\n",
      "\n",
      " - 750 round - train_metric: 0.268425 - valid_metric: 0.272815\n",
      "\n",
      " - 800 round - train_metric: 0.267022 - valid_metric: 0.271723\n",
      "\n",
      " - 850 round - train_metric: 0.264984 - valid_metric: 0.270170\n",
      "\n",
      " - 900 round - train_metric: 0.264129 - valid_metric: 0.269615\n",
      "\n",
      " - 950 round - train_metric: 0.264122 - valid_metric: 0.269708\n",
      "\n",
      " - 1000 round - train_metric: 0.263176 - valid_metric: 0.269076\n",
      "\n",
      " - 1050 round - train_metric: 0.262424 - valid_metric: 0.268603\n",
      "\n",
      " - 1100 round - train_metric: 0.261013 - valid_metric: 0.267720\n",
      "\n",
      " - 1150 round - train_metric: 0.261043 - valid_metric: 0.267841\n",
      "\n",
      " - 1200 round - train_metric: 0.260431 - valid_metric: 0.267502\n",
      "\n",
      " - 1250 round - train_metric: 0.259840 - valid_metric: 0.267197\n",
      "\n",
      " - 1300 round - train_metric: 0.259530 - valid_metric: 0.267063\n",
      "\n",
      " - 1350 round - train_metric: 0.259273 - valid_metric: 0.266981\n",
      "\n",
      " - 1400 round - train_metric: 0.258237 - valid_metric: 0.266410\n",
      "\n",
      " - 1450 round - train_metric: 0.258008 - valid_metric: 0.266373\n",
      "\n",
      " - 1500 round - train_metric: 0.257648 - valid_metric: 0.266240\n",
      "\n",
      " - 1550 round - train_metric: 0.256633 - valid_metric: 0.265715\n",
      "\n",
      " - 1600 round - train_metric: 0.255972 - valid_metric: 0.265413\n",
      "\n",
      " - 1650 round - train_metric: 0.255345 - valid_metric: 0.265163\n",
      "\n",
      " - 1700 round - train_metric: 0.255054 - valid_metric: 0.265115\n",
      "\n",
      " - 1750 round - train_metric: 0.254789 - valid_metric: 0.265063\n",
      "\n",
      " - 1800 round - train_metric: 0.254582 - valid_metric: 0.265051\n",
      "\n",
      " - 1850 round - train_metric: 0.254247 - valid_metric: 0.264999\n",
      "\n",
      " - 1900 round - train_metric: 0.253773 - valid_metric: 0.264837\n",
      "\n",
      " - 1950 round - train_metric: 0.253225 - valid_metric: 0.264658\n",
      "\n",
      " - 2000 round - train_metric: 0.252792 - valid_metric: 0.264535\n",
      "\n",
      " - 2050 round - train_metric: 0.252525 - valid_metric: 0.264490\n",
      "\n",
      " - 2100 round - train_metric: 0.252216 - valid_metric: 0.264420\n",
      "\n",
      " - 2150 round - train_metric: 0.252190 - valid_metric: 0.264470\n",
      "\n",
      " - 2200 round - train_metric: 0.251695 - valid_metric: 0.264342\n",
      "\n",
      " - 2250 round - train_metric: 0.251312 - valid_metric: 0.264229\n",
      "\n",
      " - 2300 round - train_metric: 0.250888 - valid_metric: 0.264100\n",
      "\n",
      " - 2350 round - train_metric: 0.250606 - valid_metric: 0.264069\n",
      "\n",
      " - 2400 round - train_metric: 0.250317 - valid_metric: 0.264045\n",
      "\n",
      " - 2450 round - train_metric: 0.249961 - valid_metric: 0.263966\n",
      "\n",
      " - 2500 round - train_metric: 0.249815 - valid_metric: 0.263968\n",
      "\n",
      " - 2550 round - train_metric: 0.249454 - valid_metric: 0.263904\n",
      "\n",
      " - 2600 round - train_metric: 0.249153 - valid_metric: 0.263877\n",
      "\n",
      " - 2650 round - train_metric: 0.248814 - valid_metric: 0.263805\n",
      "\n",
      " - 2700 round - train_metric: 0.248404 - valid_metric: 0.263727\n",
      "\n",
      " - 2750 round - train_metric: 0.248252 - valid_metric: 0.263740\n",
      "\n",
      " - 2800 round - train_metric: 0.247961 - valid_metric: 0.263702\n",
      "\n",
      " - 2850 round - train_metric: 0.247468 - valid_metric: 0.263584\n",
      "\n",
      " - 2900 round - train_metric: 0.247225 - valid_metric: 0.263548\n",
      "\n",
      " - 2950 round - train_metric: 0.246793 - valid_metric: 0.263480\n",
      "\n",
      " - 3000 round - train_metric: 0.246429 - valid_metric: 0.263409\n",
      "\n",
      " - 3050 round - train_metric: 0.246108 - valid_metric: 0.263371\n",
      "\n",
      " - 3100 round - train_metric: 0.245845 - valid_metric: 0.263341\n",
      "\n",
      " - 3150 round - train_metric: 0.245602 - valid_metric: 0.263324\n",
      "\n",
      " - 3200 round - train_metric: 0.245163 - valid_metric: 0.263259\n",
      "\n",
      " - 3250 round - train_metric: 0.244862 - valid_metric: 0.263226\n",
      "\n",
      " - 3300 round - train_metric: 0.244590 - valid_metric: 0.263181\n",
      "\n",
      " - 3350 round - train_metric: 0.244193 - valid_metric: 0.263129\n",
      "\n",
      " - 3400 round - train_metric: 0.243756 - valid_metric: 0.263067\n",
      "\n",
      " - 3450 round - train_metric: 0.243362 - valid_metric: 0.263020\n",
      "\n",
      " - 3500 round - train_metric: 0.243129 - valid_metric: 0.263006\n",
      "\n",
      " - 3550 round - train_metric: 0.242854 - valid_metric: 0.262988\n",
      "\n",
      " - 3600 round - train_metric: 0.242564 - valid_metric: 0.262990\n",
      "\n",
      " - 3650 round - train_metric: 0.242267 - valid_metric: 0.262983\n",
      "\n",
      " - 3700 round - train_metric: 0.241687 - valid_metric: 0.262916\n",
      "\n",
      " - 3750 round - train_metric: 0.241378 - valid_metric: 0.262898\n",
      "\n",
      " - 3800 round - train_metric: 0.241108 - valid_metric: 0.262888\n",
      "\n",
      " - 3850 round - train_metric: 0.240821 - valid_metric: 0.262875\n",
      "\n",
      " - 3900 round - train_metric: 0.240660 - valid_metric: 0.262860\n",
      "\n",
      " - 3950 round - train_metric: 0.240560 - valid_metric: 0.262878\n",
      "\n",
      " - 4000 round - train_metric: 0.240150 - valid_metric: 0.262838\n",
      "\n",
      " - 4050 round - train_metric: 0.239914 - valid_metric: 0.262855\n",
      "\n",
      " - 4100 round - train_metric: 0.239604 - valid_metric: 0.262830\n",
      "\n",
      " - 4150 round - train_metric: 0.239144 - valid_metric: 0.262790\n",
      "\n",
      " - 4200 round - train_metric: 0.238879 - valid_metric: 0.262775\n",
      "\n",
      " - 4250 round - train_metric: 0.238576 - valid_metric: 0.262768\n",
      "\n",
      " - 4300 round - train_metric: 0.238164 - valid_metric: 0.262744\n",
      "\n",
      " - 4350 round - train_metric: 0.237991 - valid_metric: 0.262738\n",
      "\n",
      " - 4400 round - train_metric: 0.237792 - valid_metric: 0.262745\n",
      "\n",
      " - 4450 round - train_metric: 0.237429 - valid_metric: 0.262718\n",
      "\n",
      "- fold1 valid metric: 0.709866\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 1102169, number of negative: 3322260\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.422529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6723\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424429, number of used features: 188\n",
      "[50]\ttraining's binary_logloss: 0.424017\tvalid_1's binary_logloss: 0.424903\n",
      "[100]\ttraining's binary_logloss: 0.370372\tvalid_1's binary_logloss: 0.371462\n",
      "[150]\ttraining's binary_logloss: 0.348306\tvalid_1's binary_logloss: 0.349575\n",
      "[200]\ttraining's binary_logloss: 0.337482\tvalid_1's binary_logloss: 0.338752\n",
      "[250]\ttraining's binary_logloss: 0.31389\tvalid_1's binary_logloss: 0.315408\n",
      "[300]\ttraining's binary_logloss: 0.301932\tvalid_1's binary_logloss: 0.303645\n",
      "[350]\ttraining's binary_logloss: 0.294021\tvalid_1's binary_logloss: 0.295856\n",
      "[400]\ttraining's binary_logloss: 0.291073\tvalid_1's binary_logloss: 0.29303\n",
      "[450]\ttraining's binary_logloss: 0.289611\tvalid_1's binary_logloss: 0.291623\n",
      "[500]\ttraining's binary_logloss: 0.28097\tvalid_1's binary_logloss: 0.283276\n",
      "[550]\ttraining's binary_logloss: 0.278468\tvalid_1's binary_logloss: 0.280987\n",
      "[600]\ttraining's binary_logloss: 0.276745\tvalid_1's binary_logloss: 0.279439\n",
      "[650]\ttraining's binary_logloss: 0.272884\tvalid_1's binary_logloss: 0.275955\n",
      "[700]\ttraining's binary_logloss: 0.270821\tvalid_1's binary_logloss: 0.274196\n",
      "[750]\ttraining's binary_logloss: 0.268559\tvalid_1's binary_logloss: 0.272337\n",
      "[800]\ttraining's binary_logloss: 0.267153\tvalid_1's binary_logloss: 0.271254\n",
      "[850]\ttraining's binary_logloss: 0.265241\tvalid_1's binary_logloss: 0.269797\n",
      "[900]\ttraining's binary_logloss: 0.264365\tvalid_1's binary_logloss: 0.269239\n",
      "[950]\ttraining's binary_logloss: 0.264363\tvalid_1's binary_logloss: 0.269358\n",
      "[1000]\ttraining's binary_logloss: 0.263294\tvalid_1's binary_logloss: 0.26863\n",
      "[1050]\ttraining's binary_logloss: 0.262649\tvalid_1's binary_logloss: 0.268244\n",
      "[1100]\ttraining's binary_logloss: 0.261175\tvalid_1's binary_logloss: 0.26725\n",
      "[1150]\ttraining's binary_logloss: 0.261126\tvalid_1's binary_logloss: 0.26731\n",
      "[1200]\ttraining's binary_logloss: 0.260496\tvalid_1's binary_logloss: 0.266958\n",
      "[1250]\ttraining's binary_logloss: 0.259913\tvalid_1's binary_logloss: 0.266654\n",
      "[1300]\ttraining's binary_logloss: 0.259593\tvalid_1's binary_logloss: 0.266519\n",
      "[1350]\ttraining's binary_logloss: 0.259339\tvalid_1's binary_logloss: 0.266436\n",
      "[1400]\ttraining's binary_logloss: 0.258377\tvalid_1's binary_logloss: 0.265871\n",
      "[1450]\ttraining's binary_logloss: 0.258065\tvalid_1's binary_logloss: 0.265764\n",
      "[1500]\ttraining's binary_logloss: 0.257757\tvalid_1's binary_logloss: 0.265665\n",
      "[1550]\ttraining's binary_logloss: 0.25678\tvalid_1's binary_logloss: 0.265187\n",
      "[1600]\ttraining's binary_logloss: 0.256041\tvalid_1's binary_logloss: 0.264845\n",
      "[1650]\ttraining's binary_logloss: 0.255481\tvalid_1's binary_logloss: 0.264624\n",
      "[1700]\ttraining's binary_logloss: 0.255193\tvalid_1's binary_logloss: 0.264572\n",
      "[1750]\ttraining's binary_logloss: 0.254903\tvalid_1's binary_logloss: 0.264508\n",
      "[1800]\ttraining's binary_logloss: 0.254745\tvalid_1's binary_logloss: 0.264519\n",
      "[1850]\ttraining's binary_logloss: 0.254346\tvalid_1's binary_logloss: 0.264398\n",
      "[1900]\ttraining's binary_logloss: 0.253922\tvalid_1's binary_logloss: 0.264257\n",
      "[1950]\ttraining's binary_logloss: 0.253343\tvalid_1's binary_logloss: 0.264049\n",
      "[2000]\ttraining's binary_logloss: 0.252918\tvalid_1's binary_logloss: 0.263924\n",
      "[2050]\ttraining's binary_logloss: 0.25263\tvalid_1's binary_logloss: 0.263875\n",
      "[2100]\ttraining's binary_logloss: 0.252302\tvalid_1's binary_logloss: 0.263796\n",
      "[2150]\ttraining's binary_logloss: 0.252279\tvalid_1's binary_logloss: 0.263856\n",
      "[2200]\ttraining's binary_logloss: 0.251815\tvalid_1's binary_logloss: 0.263719\n",
      "[2250]\ttraining's binary_logloss: 0.251466\tvalid_1's binary_logloss: 0.263628\n",
      "[2300]\ttraining's binary_logloss: 0.250982\tvalid_1's binary_logloss: 0.263481\n",
      "[2350]\ttraining's binary_logloss: 0.250694\tvalid_1's binary_logloss: 0.263438\n",
      "[2400]\ttraining's binary_logloss: 0.25046\tvalid_1's binary_logloss: 0.26341\n",
      "[2450]\ttraining's binary_logloss: 0.25008\tvalid_1's binary_logloss: 0.263335\n",
      "[2500]\ttraining's binary_logloss: 0.249877\tvalid_1's binary_logloss: 0.263315\n",
      "[2550]\ttraining's binary_logloss: 0.249553\tvalid_1's binary_logloss: 0.26326\n",
      "[2600]\ttraining's binary_logloss: 0.249231\tvalid_1's binary_logloss: 0.263186\n",
      "[2650]\ttraining's binary_logloss: 0.248892\tvalid_1's binary_logloss: 0.263112\n",
      "[2700]\ttraining's binary_logloss: 0.24849\tvalid_1's binary_logloss: 0.263004\n",
      "[2750]\ttraining's binary_logloss: 0.24834\tvalid_1's binary_logloss: 0.263034\n",
      "[2800]\ttraining's binary_logloss: 0.248096\tvalid_1's binary_logloss: 0.263015\n",
      "[2850]\ttraining's binary_logloss: 0.247559\tvalid_1's binary_logloss: 0.262877\n",
      "[2900]\ttraining's binary_logloss: 0.247307\tvalid_1's binary_logloss: 0.262848\n",
      "[2950]\ttraining's binary_logloss: 0.246875\tvalid_1's binary_logloss: 0.262756\n",
      "[3000]\ttraining's binary_logloss: 0.246502\tvalid_1's binary_logloss: 0.262712\n",
      "[3050]\ttraining's binary_logloss: 0.246202\tvalid_1's binary_logloss: 0.262678\n",
      "[3100]\ttraining's binary_logloss: 0.245916\tvalid_1's binary_logloss: 0.262635\n",
      "[3150]\ttraining's binary_logloss: 0.245669\tvalid_1's binary_logloss: 0.262638\n",
      "[3200]\ttraining's binary_logloss: 0.245265\tvalid_1's binary_logloss: 0.262593\n",
      "[3250]\ttraining's binary_logloss: 0.244944\tvalid_1's binary_logloss: 0.262524\n",
      "[3300]\ttraining's binary_logloss: 0.244659\tvalid_1's binary_logloss: 0.262501\n",
      "[3350]\ttraining's binary_logloss: 0.244263\tvalid_1's binary_logloss: 0.262429\n",
      "[3400]\ttraining's binary_logloss: 0.243824\tvalid_1's binary_logloss: 0.262363\n",
      "[3450]\ttraining's binary_logloss: 0.243469\tvalid_1's binary_logloss: 0.262311\n",
      "[3500]\ttraining's binary_logloss: 0.243251\tvalid_1's binary_logloss: 0.262309\n",
      "[3550]\ttraining's binary_logloss: 0.242942\tvalid_1's binary_logloss: 0.262281\n",
      "[3600]\ttraining's binary_logloss: 0.242637\tvalid_1's binary_logloss: 0.262272\n",
      "[3650]\ttraining's binary_logloss: 0.242338\tvalid_1's binary_logloss: 0.262247\n",
      "[3700]\ttraining's binary_logloss: 0.241774\tvalid_1's binary_logloss: 0.262174\n",
      "[3750]\ttraining's binary_logloss: 0.241479\tvalid_1's binary_logloss: 0.262148\n",
      "[3800]\ttraining's binary_logloss: 0.241226\tvalid_1's binary_logloss: 0.262126\n",
      "[3850]\ttraining's binary_logloss: 0.240928\tvalid_1's binary_logloss: 0.262124\n",
      "[3900]\ttraining's binary_logloss: 0.240727\tvalid_1's binary_logloss: 0.262114\n",
      "[3950]\ttraining's binary_logloss: 0.240645\tvalid_1's binary_logloss: 0.262138\n",
      "[4000]\ttraining's binary_logloss: 0.240231\tvalid_1's binary_logloss: 0.262097\n",
      "[4050]\ttraining's binary_logloss: 0.239974\tvalid_1's binary_logloss: 0.262084\n",
      "[4100]\ttraining's binary_logloss: 0.239636\tvalid_1's binary_logloss: 0.262063\n",
      "[4150]\ttraining's binary_logloss: 0.239204\tvalid_1's binary_logloss: 0.261996\n",
      "[4200]\ttraining's binary_logloss: 0.238903\tvalid_1's binary_logloss: 0.262009\n",
      "[4250]\ttraining's binary_logloss: 0.238602\tvalid_1's binary_logloss: 0.261989\n",
      "[4300]\ttraining's binary_logloss: 0.238198\tvalid_1's binary_logloss: 0.261951\n",
      "[4350]\ttraining's binary_logloss: 0.238053\tvalid_1's binary_logloss: 0.26197\n",
      "[4400]\ttraining's binary_logloss: 0.237856\tvalid_1's binary_logloss: 0.261985\n",
      "[4450]\ttraining's binary_logloss: 0.237493\tvalid_1's binary_logloss: 0.261961\n",
      "[4500]\ttraining's binary_logloss: 0.237257\tvalid_1's binary_logloss: 0.261978\n",
      " - 0 round - train_metric: 0.672356 - valid_metric: 0.672422\n",
      "\n",
      " - 50 round - train_metric: 0.426277 - valid_metric: 0.427153\n",
      "\n",
      " - 100 round - train_metric: 0.366800 - valid_metric: 0.367901\n",
      "\n",
      " - 150 round - train_metric: 0.349945 - valid_metric: 0.351204\n",
      "\n",
      " - 200 round - train_metric: 0.338540 - valid_metric: 0.339805\n",
      "\n",
      " - 250 round - train_metric: 0.312483 - valid_metric: 0.314020\n",
      "\n",
      " - 300 round - train_metric: 0.300937 - valid_metric: 0.302676\n",
      "\n",
      " - 350 round - train_metric: 0.294457 - valid_metric: 0.296285\n",
      "\n",
      " - 400 round - train_metric: 0.291406 - valid_metric: 0.293357\n",
      "\n",
      " - 450 round - train_metric: 0.289908 - valid_metric: 0.291912\n",
      "\n",
      " - 500 round - train_metric: 0.280634 - valid_metric: 0.282954\n",
      "\n",
      " - 550 round - train_metric: 0.278182 - valid_metric: 0.280716\n",
      "\n",
      " - 600 round - train_metric: 0.276498 - valid_metric: 0.279206\n",
      "\n",
      " - 650 round - train_metric: 0.272975 - valid_metric: 0.276041\n",
      "\n",
      " - 700 round - train_metric: 0.270907 - valid_metric: 0.274277\n",
      "\n",
      " - 750 round - train_metric: 0.268633 - valid_metric: 0.272403\n",
      "\n",
      " - 800 round - train_metric: 0.267206 - valid_metric: 0.271302\n",
      "\n",
      " - 850 round - train_metric: 0.265160 - valid_metric: 0.269735\n",
      "\n",
      " - 900 round - train_metric: 0.264285 - valid_metric: 0.269176\n",
      "\n",
      " - 950 round - train_metric: 0.264283 - valid_metric: 0.269298\n",
      "\n",
      " - 1000 round - train_metric: 0.263336 - valid_metric: 0.268666\n",
      "\n",
      " - 1050 round - train_metric: 0.262583 - valid_metric: 0.268187\n",
      "\n",
      " - 1100 round - train_metric: 0.261125 - valid_metric: 0.267216\n",
      "\n",
      " - 1150 round - train_metric: 0.261155 - valid_metric: 0.267334\n",
      "\n",
      " - 1200 round - train_metric: 0.260526 - valid_metric: 0.266982\n",
      "\n",
      " - 1250 round - train_metric: 0.259937 - valid_metric: 0.266674\n",
      "\n",
      " - 1300 round - train_metric: 0.259618 - valid_metric: 0.266539\n",
      "\n",
      " - 1350 round - train_metric: 0.259368 - valid_metric: 0.266457\n",
      "\n",
      " - 1400 round - train_metric: 0.258329 - valid_metric: 0.265846\n",
      "\n",
      " - 1450 round - train_metric: 0.258089 - valid_metric: 0.265781\n",
      "\n",
      " - 1500 round - train_metric: 0.257713 - valid_metric: 0.265641\n",
      "\n",
      " - 1550 round - train_metric: 0.256743 - valid_metric: 0.265164\n",
      "\n",
      " - 1600 round - train_metric: 0.256062 - valid_metric: 0.264858\n",
      "\n",
      " - 1650 round - train_metric: 0.255447 - valid_metric: 0.264609\n",
      "\n",
      " - 1700 round - train_metric: 0.255160 - valid_metric: 0.264559\n",
      "\n",
      " - 1750 round - train_metric: 0.254920 - valid_metric: 0.264519\n",
      "\n",
      " - 1800 round - train_metric: 0.254707 - valid_metric: 0.264497\n",
      "\n",
      " - 1850 round - train_metric: 0.254369 - valid_metric: 0.264413\n",
      "\n",
      " - 1900 round - train_metric: 0.253893 - valid_metric: 0.264246\n",
      "\n",
      " - 1950 round - train_metric: 0.253358 - valid_metric: 0.264058\n",
      "\n",
      " - 2000 round - train_metric: 0.252893 - valid_metric: 0.263911\n",
      "\n",
      " - 2050 round - train_metric: 0.252643 - valid_metric: 0.263883\n",
      "\n",
      " - 2100 round - train_metric: 0.252313 - valid_metric: 0.263802\n",
      "\n",
      " - 2150 round - train_metric: 0.252296 - valid_metric: 0.263866\n",
      "\n",
      " - 2200 round - train_metric: 0.251830 - valid_metric: 0.263728\n",
      "\n",
      " - 2250 round - train_metric: 0.251439 - valid_metric: 0.263619\n",
      "\n",
      " - 2300 round - train_metric: 0.250996 - valid_metric: 0.263489\n",
      "\n",
      " - 2350 round - train_metric: 0.250710 - valid_metric: 0.263447\n",
      "\n",
      " - 2400 round - train_metric: 0.250432 - valid_metric: 0.263396\n",
      "\n",
      " - 2450 round - train_metric: 0.250046 - valid_metric: 0.263324\n",
      "\n",
      " - 2500 round - train_metric: 0.249886 - valid_metric: 0.263320\n",
      "\n",
      " - 2550 round - train_metric: 0.249528 - valid_metric: 0.263251\n",
      "\n",
      " - 2600 round - train_metric: 0.249238 - valid_metric: 0.263191\n",
      "\n",
      " - 2650 round - train_metric: 0.248899 - valid_metric: 0.263116\n",
      "\n",
      " - 2700 round - train_metric: 0.248500 - valid_metric: 0.263009\n",
      "\n",
      " - 2750 round - train_metric: 0.248349 - valid_metric: 0.263039\n",
      "\n",
      " - 2800 round - train_metric: 0.248071 - valid_metric: 0.263008\n",
      "\n",
      " - 2850 round - train_metric: 0.247567 - valid_metric: 0.262881\n",
      "\n",
      " - 2900 round - train_metric: 0.247319 - valid_metric: 0.262854\n",
      "\n",
      " - 2950 round - train_metric: 0.246883 - valid_metric: 0.262761\n",
      "\n",
      " - 3000 round - train_metric: 0.246511 - valid_metric: 0.262716\n",
      "\n",
      " - 3050 round - train_metric: 0.246178 - valid_metric: 0.262669\n",
      "\n",
      " - 3100 round - train_metric: 0.245922 - valid_metric: 0.262638\n",
      "\n",
      " - 3150 round - train_metric: 0.245678 - valid_metric: 0.262641\n",
      "\n",
      " - 3200 round - train_metric: 0.245241 - valid_metric: 0.262584\n",
      "\n",
      " - 3250 round - train_metric: 0.244950 - valid_metric: 0.262527\n",
      "\n",
      " - 3300 round - train_metric: 0.244667 - valid_metric: 0.262504\n",
      "\n",
      " - 3350 round - train_metric: 0.244271 - valid_metric: 0.262431\n",
      "\n",
      " - 3400 round - train_metric: 0.243831 - valid_metric: 0.262366\n",
      "\n",
      " - 3450 round - train_metric: 0.243450 - valid_metric: 0.262307\n",
      "\n",
      " - 3500 round - train_metric: 0.243230 - valid_metric: 0.262305\n",
      "\n",
      " - 3550 round - train_metric: 0.242948 - valid_metric: 0.262283\n",
      "\n",
      " - 3600 round - train_metric: 0.242644 - valid_metric: 0.262275\n",
      "\n",
      " - 3650 round - train_metric: 0.242348 - valid_metric: 0.262250\n",
      "\n",
      " - 3700 round - train_metric: 0.241755 - valid_metric: 0.262170\n",
      "\n",
      " - 3750 round - train_metric: 0.241463 - valid_metric: 0.262148\n",
      "\n",
      " - 3800 round - train_metric: 0.241206 - valid_metric: 0.262122\n",
      "\n",
      " - 3850 round - train_metric: 0.240911 - valid_metric: 0.262118\n",
      "\n",
      " - 3900 round - train_metric: 0.240732 - valid_metric: 0.262116\n",
      "\n",
      " - 3950 round - train_metric: 0.240628 - valid_metric: 0.262135\n",
      "\n",
      " - 4000 round - train_metric: 0.240212 - valid_metric: 0.262092\n",
      "\n",
      " - 4050 round - train_metric: 0.239980 - valid_metric: 0.262086\n",
      "\n",
      " - 4100 round - train_metric: 0.239644 - valid_metric: 0.262065\n",
      "\n",
      " - 4150 round - train_metric: 0.239211 - valid_metric: 0.261998\n",
      "\n",
      " - 4200 round - train_metric: 0.238910 - valid_metric: 0.262010\n",
      "\n",
      " - 4250 round - train_metric: 0.238608 - valid_metric: 0.261991\n",
      "\n",
      " - 4300 round - train_metric: 0.238206 - valid_metric: 0.261953\n",
      "\n",
      " - 4350 round - train_metric: 0.238037 - valid_metric: 0.261970\n",
      "\n",
      " - 4400 round - train_metric: 0.237840 - valid_metric: 0.261984\n",
      "\n",
      " - 4450 round - train_metric: 0.237499 - valid_metric: 0.261963\n",
      "\n",
      "- fold2 valid metric: 0.712072\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 1101674, number of negative: 3322741\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.422476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6712\n",
      "[LightGBM] [Info] Number of data points in the train set: 4424415, number of used features: 188\n",
      "[50]\ttraining's binary_logloss: 0.423814\tvalid_1's binary_logloss: 0.425683\n",
      "[100]\ttraining's binary_logloss: 0.370034\tvalid_1's binary_logloss: 0.372487\n",
      "[150]\ttraining's binary_logloss: 0.347983\tvalid_1's binary_logloss: 0.350719\n",
      "[200]\ttraining's binary_logloss: 0.337071\tvalid_1's binary_logloss: 0.34004\n",
      "[250]\ttraining's binary_logloss: 0.313431\tvalid_1's binary_logloss: 0.316858\n",
      "[300]\ttraining's binary_logloss: 0.30138\tvalid_1's binary_logloss: 0.305142\n",
      "[350]\ttraining's binary_logloss: 0.293516\tvalid_1's binary_logloss: 0.297599\n",
      "[400]\ttraining's binary_logloss: 0.290545\tvalid_1's binary_logloss: 0.294761\n",
      "[450]\ttraining's binary_logloss: 0.289133\tvalid_1's binary_logloss: 0.293459\n",
      "[500]\ttraining's binary_logloss: 0.280504\tvalid_1's binary_logloss: 0.285323\n",
      "[550]\ttraining's binary_logloss: 0.277926\tvalid_1's binary_logloss: 0.28295\n",
      "[600]\ttraining's binary_logloss: 0.276161\tvalid_1's binary_logloss: 0.281333\n",
      "[650]\ttraining's binary_logloss: 0.272249\tvalid_1's binary_logloss: 0.277887\n",
      "[700]\ttraining's binary_logloss: 0.270227\tvalid_1's binary_logloss: 0.276168\n",
      "[750]\ttraining's binary_logloss: 0.268017\tvalid_1's binary_logloss: 0.27437\n",
      "[800]\ttraining's binary_logloss: 0.266626\tvalid_1's binary_logloss: 0.2733\n",
      "[850]\ttraining's binary_logloss: 0.264744\tvalid_1's binary_logloss: 0.271892\n",
      "[900]\ttraining's binary_logloss: 0.263878\tvalid_1's binary_logloss: 0.271316\n",
      "[950]\ttraining's binary_logloss: 0.263862\tvalid_1's binary_logloss: 0.271375\n",
      "[1000]\ttraining's binary_logloss: 0.262798\tvalid_1's binary_logloss: 0.270657\n",
      "[1050]\ttraining's binary_logloss: 0.262174\tvalid_1's binary_logloss: 0.270285\n",
      "[1100]\ttraining's binary_logloss: 0.260747\tvalid_1's binary_logloss: 0.269365\n",
      "[1150]\ttraining's binary_logloss: 0.260672\tvalid_1's binary_logloss: 0.269408\n",
      "[1200]\ttraining's binary_logloss: 0.260049\tvalid_1's binary_logloss: 0.269068\n",
      "[1250]\ttraining's binary_logloss: 0.259468\tvalid_1's binary_logloss: 0.268731\n",
      "[1300]\ttraining's binary_logloss: 0.259163\tvalid_1's binary_logloss: 0.268629\n",
      "[1350]\ttraining's binary_logloss: 0.258906\tvalid_1's binary_logloss: 0.268516\n",
      "[1400]\ttraining's binary_logloss: 0.257931\tvalid_1's binary_logloss: 0.267956\n",
      "[1450]\ttraining's binary_logloss: 0.257609\tvalid_1's binary_logloss: 0.267829\n",
      "[1500]\ttraining's binary_logloss: 0.257315\tvalid_1's binary_logloss: 0.267727\n",
      "[1550]\ttraining's binary_logloss: 0.256334\tvalid_1's binary_logloss: 0.267201\n",
      "[1600]\ttraining's binary_logloss: 0.255574\tvalid_1's binary_logloss: 0.266877\n",
      "[1650]\ttraining's binary_logloss: 0.255002\tvalid_1's binary_logloss: 0.266627\n",
      "[1700]\ttraining's binary_logloss: 0.254714\tvalid_1's binary_logloss: 0.266532\n",
      "[1750]\ttraining's binary_logloss: 0.254396\tvalid_1's binary_logloss: 0.266448\n",
      "[1800]\ttraining's binary_logloss: 0.254236\tvalid_1's binary_logloss: 0.266444\n",
      "[1850]\ttraining's binary_logloss: 0.253825\tvalid_1's binary_logloss: 0.266306\n",
      "[1900]\ttraining's binary_logloss: 0.253413\tvalid_1's binary_logloss: 0.266161\n",
      "[1950]\ttraining's binary_logloss: 0.252839\tvalid_1's binary_logloss: 0.265949\n",
      "[2000]\ttraining's binary_logloss: 0.252401\tvalid_1's binary_logloss: 0.265813\n",
      "[2050]\ttraining's binary_logloss: 0.252095\tvalid_1's binary_logloss: 0.265734\n",
      "[2100]\ttraining's binary_logloss: 0.251787\tvalid_1's binary_logloss: 0.265689\n",
      "[2150]\ttraining's binary_logloss: 0.251747\tvalid_1's binary_logloss: 0.265739\n",
      "[2200]\ttraining's binary_logloss: 0.251275\tvalid_1's binary_logloss: 0.265609\n",
      "[2250]\ttraining's binary_logloss: 0.250934\tvalid_1's binary_logloss: 0.265516\n",
      "[2300]\ttraining's binary_logloss: 0.250463\tvalid_1's binary_logloss: 0.265382\n",
      "[2350]\ttraining's binary_logloss: 0.250177\tvalid_1's binary_logloss: 0.265361\n",
      "[2400]\ttraining's binary_logloss: 0.249941\tvalid_1's binary_logloss: 0.265355\n",
      "[2450]\ttraining's binary_logloss: 0.249568\tvalid_1's binary_logloss: 0.265269\n",
      "[2500]\ttraining's binary_logloss: 0.249386\tvalid_1's binary_logloss: 0.26526\n",
      "[2550]\ttraining's binary_logloss: 0.249049\tvalid_1's binary_logloss: 0.265186\n",
      "[2600]\ttraining's binary_logloss: 0.248739\tvalid_1's binary_logloss: 0.265106\n",
      "[2650]\ttraining's binary_logloss: 0.248404\tvalid_1's binary_logloss: 0.265037\n",
      "[2700]\ttraining's binary_logloss: 0.248005\tvalid_1's binary_logloss: 0.264947\n",
      "[2750]\ttraining's binary_logloss: 0.247844\tvalid_1's binary_logloss: 0.264955\n",
      "[2800]\ttraining's binary_logloss: 0.247589\tvalid_1's binary_logloss: 0.264938\n",
      "[2850]\ttraining's binary_logloss: 0.247064\tvalid_1's binary_logloss: 0.264797\n",
      "[2900]\ttraining's binary_logloss: 0.246797\tvalid_1's binary_logloss: 0.264787\n",
      "[2950]\ttraining's binary_logloss: 0.246356\tvalid_1's binary_logloss: 0.264718\n",
      "[3000]\ttraining's binary_logloss: 0.245993\tvalid_1's binary_logloss: 0.26468\n",
      "[3050]\ttraining's binary_logloss: 0.245693\tvalid_1's binary_logloss: 0.264649\n",
      "[3100]\ttraining's binary_logloss: 0.245409\tvalid_1's binary_logloss: 0.264623\n",
      "[3150]\ttraining's binary_logloss: 0.24516\tvalid_1's binary_logloss: 0.264597\n",
      "[3200]\ttraining's binary_logloss: 0.244759\tvalid_1's binary_logloss: 0.264539\n",
      "[3250]\ttraining's binary_logloss: 0.244446\tvalid_1's binary_logloss: 0.26449\n",
      "[3300]\ttraining's binary_logloss: 0.24416\tvalid_1's binary_logloss: 0.26448\n",
      "[3350]\ttraining's binary_logloss: 0.243759\tvalid_1's binary_logloss: 0.264451\n",
      "[3400]\ttraining's binary_logloss: 0.243333\tvalid_1's binary_logloss: 0.264409\n",
      "[3450]\ttraining's binary_logloss: 0.242968\tvalid_1's binary_logloss: 0.26437\n",
      "[3500]\ttraining's binary_logloss: 0.242735\tvalid_1's binary_logloss: 0.264362\n",
      "[3550]\ttraining's binary_logloss: 0.242427\tvalid_1's binary_logloss: 0.264334\n",
      "[3600]\ttraining's binary_logloss: 0.242142\tvalid_1's binary_logloss: 0.264329\n",
      "[3650]\ttraining's binary_logloss: 0.241832\tvalid_1's binary_logloss: 0.264303\n",
      "[3700]\ttraining's binary_logloss: 0.241279\tvalid_1's binary_logloss: 0.264221\n",
      "[3750]\ttraining's binary_logloss: 0.240978\tvalid_1's binary_logloss: 0.264197\n",
      "[3800]\ttraining's binary_logloss: 0.240727\tvalid_1's binary_logloss: 0.264183\n",
      "[3850]\ttraining's binary_logloss: 0.24044\tvalid_1's binary_logloss: 0.264174\n",
      "[3900]\ttraining's binary_logloss: 0.240239\tvalid_1's binary_logloss: 0.264195\n",
      "[3950]\ttraining's binary_logloss: 0.240156\tvalid_1's binary_logloss: 0.264227\n",
      "[4000]\ttraining's binary_logloss: 0.239744\tvalid_1's binary_logloss: 0.264192\n",
      "[4050]\ttraining's binary_logloss: 0.239484\tvalid_1's binary_logloss: 0.264177\n",
      "[4100]\ttraining's binary_logloss: 0.239153\tvalid_1's binary_logloss: 0.264163\n",
      "[4150]\ttraining's binary_logloss: 0.238687\tvalid_1's binary_logloss: 0.264126\n",
      "[4200]\ttraining's binary_logloss: 0.238406\tvalid_1's binary_logloss: 0.264126\n",
      "[4250]\ttraining's binary_logloss: 0.238091\tvalid_1's binary_logloss: 0.264098\n",
      "[4300]\ttraining's binary_logloss: 0.237672\tvalid_1's binary_logloss: 0.264075\n",
      "[4350]\ttraining's binary_logloss: 0.237523\tvalid_1's binary_logloss: 0.2641\n",
      "[4400]\ttraining's binary_logloss: 0.237321\tvalid_1's binary_logloss: 0.26411\n",
      "[4450]\ttraining's binary_logloss: 0.236956\tvalid_1's binary_logloss: 0.264089\n",
      "[4500]\ttraining's binary_logloss: 0.236741\tvalid_1's binary_logloss: 0.264114\n",
      " - 0 round - train_metric: 0.672373 - valid_metric: 0.672480\n",
      "\n",
      " - 50 round - train_metric: 0.426075 - valid_metric: 0.427928\n",
      "\n",
      " - 100 round - train_metric: 0.366436 - valid_metric: 0.368938\n",
      "\n",
      " - 150 round - train_metric: 0.349626 - valid_metric: 0.352340\n",
      "\n",
      " - 200 round - train_metric: 0.338133 - valid_metric: 0.341085\n",
      "\n",
      " - 250 round - train_metric: 0.312025 - valid_metric: 0.315486\n",
      "\n",
      " - 300 round - train_metric: 0.300394 - valid_metric: 0.304199\n",
      "\n",
      " - 350 round - train_metric: 0.293952 - valid_metric: 0.298020\n",
      "\n",
      " - 400 round - train_metric: 0.290878 - valid_metric: 0.295082\n",
      "\n",
      " - 450 round - train_metric: 0.289429 - valid_metric: 0.293743\n",
      "\n",
      " - 500 round - train_metric: 0.280153 - valid_metric: 0.284995\n",
      "\n",
      " - 550 round - train_metric: 0.277625 - valid_metric: 0.282675\n",
      "\n",
      " - 600 round - train_metric: 0.275914 - valid_metric: 0.281108\n",
      "\n",
      " - 650 round - train_metric: 0.272340 - valid_metric: 0.277969\n",
      "\n",
      " - 700 round - train_metric: 0.270313 - valid_metric: 0.276243\n",
      "\n",
      " - 750 round - train_metric: 0.268091 - valid_metric: 0.274432\n",
      "\n",
      " - 800 round - train_metric: 0.266680 - valid_metric: 0.273345\n",
      "\n",
      " - 850 round - train_metric: 0.264660 - valid_metric: 0.271838\n",
      "\n",
      " - 900 round - train_metric: 0.263805 - valid_metric: 0.271260\n",
      "\n",
      " - 950 round - train_metric: 0.263782 - valid_metric: 0.271316\n",
      "\n",
      " - 1000 round - train_metric: 0.262839 - valid_metric: 0.270689\n",
      "\n",
      " - 1050 round - train_metric: 0.262110 - valid_metric: 0.270244\n",
      "\n",
      " - 1100 round - train_metric: 0.260690 - valid_metric: 0.269328\n",
      "\n",
      " - 1150 round - train_metric: 0.260701 - valid_metric: 0.269429\n",
      "\n",
      " - 1200 round - train_metric: 0.260078 - valid_metric: 0.269089\n",
      "\n",
      " - 1250 round - train_metric: 0.259492 - valid_metric: 0.268749\n",
      "\n",
      " - 1300 round - train_metric: 0.259189 - valid_metric: 0.268646\n",
      "\n",
      " - 1350 round - train_metric: 0.258935 - valid_metric: 0.268537\n",
      "\n",
      " - 1400 round - train_metric: 0.257887 - valid_metric: 0.267926\n",
      "\n",
      " - 1450 round - train_metric: 0.257633 - valid_metric: 0.267845\n",
      "\n",
      " - 1500 round - train_metric: 0.257275 - valid_metric: 0.267700\n",
      "\n",
      " - 1550 round - train_metric: 0.256296 - valid_metric: 0.267180\n",
      "\n",
      " - 1600 round - train_metric: 0.255595 - valid_metric: 0.266888\n",
      "\n",
      " - 1650 round - train_metric: 0.254966 - valid_metric: 0.266612\n",
      "\n",
      " - 1700 round - train_metric: 0.254684 - valid_metric: 0.266518\n",
      "\n",
      " - 1750 round - train_metric: 0.254414 - valid_metric: 0.266458\n",
      "\n",
      " - 1800 round - train_metric: 0.254201 - valid_metric: 0.266419\n",
      "\n",
      " - 1850 round - train_metric: 0.253847 - valid_metric: 0.266318\n",
      "\n",
      " - 1900 round - train_metric: 0.253381 - valid_metric: 0.266149\n",
      "\n",
      " - 1950 round - train_metric: 0.252853 - valid_metric: 0.265957\n",
      "\n",
      " - 2000 round - train_metric: 0.252374 - valid_metric: 0.265804\n",
      "\n",
      " - 2050 round - train_metric: 0.252108 - valid_metric: 0.265741\n",
      "\n",
      " - 2100 round - train_metric: 0.251798 - valid_metric: 0.265694\n",
      "\n",
      " - 2150 round - train_metric: 0.251764 - valid_metric: 0.265748\n",
      "\n",
      " - 2200 round - train_metric: 0.251290 - valid_metric: 0.265616\n",
      "\n",
      " - 2250 round - train_metric: 0.250907 - valid_metric: 0.265509\n",
      "\n",
      " - 2300 round - train_metric: 0.250477 - valid_metric: 0.265388\n",
      "\n",
      " - 2350 round - train_metric: 0.250193 - valid_metric: 0.265369\n",
      "\n",
      " - 2400 round - train_metric: 0.249915 - valid_metric: 0.265343\n",
      "\n",
      " - 2450 round - train_metric: 0.249541 - valid_metric: 0.265261\n",
      "\n",
      " - 2500 round - train_metric: 0.249394 - valid_metric: 0.265265\n",
      "\n",
      " - 2550 round - train_metric: 0.249026 - valid_metric: 0.265179\n",
      "\n",
      " - 2600 round - train_metric: 0.248746 - valid_metric: 0.265110\n",
      "\n",
      " - 2650 round - train_metric: 0.248411 - valid_metric: 0.265041\n",
      "\n",
      " - 2700 round - train_metric: 0.248015 - valid_metric: 0.264952\n",
      "\n",
      " - 2750 round - train_metric: 0.247853 - valid_metric: 0.264959\n",
      "\n",
      " - 2800 round - train_metric: 0.247565 - valid_metric: 0.264931\n",
      "\n",
      " - 2850 round - train_metric: 0.247072 - valid_metric: 0.264800\n",
      "\n",
      " - 2900 round - train_metric: 0.246809 - valid_metric: 0.264792\n",
      "\n",
      " - 2950 round - train_metric: 0.246364 - valid_metric: 0.264721\n",
      "\n",
      " - 3000 round - train_metric: 0.246001 - valid_metric: 0.264683\n",
      "\n",
      " - 3050 round - train_metric: 0.245672 - valid_metric: 0.264643\n",
      "\n",
      " - 3100 round - train_metric: 0.245416 - valid_metric: 0.264625\n",
      "\n",
      " - 3150 round - train_metric: 0.245169 - valid_metric: 0.264601\n",
      "\n",
      " - 3200 round - train_metric: 0.244741 - valid_metric: 0.264533\n",
      "\n",
      " - 3250 round - train_metric: 0.244451 - valid_metric: 0.264492\n",
      "\n",
      " - 3300 round - train_metric: 0.244168 - valid_metric: 0.264483\n",
      "\n",
      " - 3350 round - train_metric: 0.243766 - valid_metric: 0.264454\n",
      "\n",
      " - 3400 round - train_metric: 0.243340 - valid_metric: 0.264411\n",
      "\n",
      " - 3450 round - train_metric: 0.242946 - valid_metric: 0.264366\n",
      "\n",
      " - 3500 round - train_metric: 0.242715 - valid_metric: 0.264360\n",
      "\n",
      " - 3550 round - train_metric: 0.242433 - valid_metric: 0.264336\n",
      "\n",
      " - 3600 round - train_metric: 0.242149 - valid_metric: 0.264332\n",
      "\n",
      " - 3650 round - train_metric: 0.241842 - valid_metric: 0.264306\n",
      "\n",
      " - 3700 round - train_metric: 0.241258 - valid_metric: 0.264220\n",
      "\n",
      " - 3750 round - train_metric: 0.240962 - valid_metric: 0.264193\n",
      "\n",
      " - 3800 round - train_metric: 0.240708 - valid_metric: 0.264181\n",
      "\n",
      " - 3850 round - train_metric: 0.240422 - valid_metric: 0.264173\n",
      "\n",
      " - 3900 round - train_metric: 0.240243 - valid_metric: 0.264197\n",
      "\n",
      " - 3950 round - train_metric: 0.240133 - valid_metric: 0.264223\n",
      "\n",
      " - 4000 round - train_metric: 0.239728 - valid_metric: 0.264190\n",
      "\n",
      " - 4050 round - train_metric: 0.239490 - valid_metric: 0.264179\n",
      "\n",
      " - 4100 round - train_metric: 0.239161 - valid_metric: 0.264165\n",
      "\n",
      " - 4150 round - train_metric: 0.238694 - valid_metric: 0.264127\n",
      "\n",
      " - 4200 round - train_metric: 0.238413 - valid_metric: 0.264127\n",
      "\n",
      " - 4250 round - train_metric: 0.238098 - valid_metric: 0.264100\n",
      "\n",
      " - 4300 round - train_metric: 0.237680 - valid_metric: 0.264077\n",
      "\n",
      " - 4350 round - train_metric: 0.237503 - valid_metric: 0.264097\n",
      "\n",
      " - 4400 round - train_metric: 0.237305 - valid_metric: 0.264109\n",
      "\n",
      " - 4450 round - train_metric: 0.236961 - valid_metric: 0.264090\n",
      "\n",
      "- fold3 valid metric: 0.707199\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 1102719, number of negative: 3322873\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.419042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6720\n",
      "[LightGBM] [Info] Number of data points in the train set: 4425592, number of used features: 188\n",
      "[50]\ttraining's binary_logloss: 0.424093\tvalid_1's binary_logloss: 0.424533\n",
      "[100]\ttraining's binary_logloss: 0.370457\tvalid_1's binary_logloss: 0.371142\n",
      "[150]\ttraining's binary_logloss: 0.34841\tvalid_1's binary_logloss: 0.349214\n",
      "[200]\ttraining's binary_logloss: 0.337496\tvalid_1's binary_logloss: 0.338424\n",
      "[250]\ttraining's binary_logloss: 0.313899\tvalid_1's binary_logloss: 0.3151\n",
      "[300]\ttraining's binary_logloss: 0.301849\tvalid_1's binary_logloss: 0.303332\n",
      "[350]\ttraining's binary_logloss: 0.293928\tvalid_1's binary_logloss: 0.295671\n",
      "[400]\ttraining's binary_logloss: 0.290989\tvalid_1's binary_logloss: 0.29288\n",
      "[450]\ttraining's binary_logloss: 0.28954\tvalid_1's binary_logloss: 0.291545\n",
      "[500]\ttraining's binary_logloss: 0.280955\tvalid_1's binary_logloss: 0.283365\n",
      "[550]\ttraining's binary_logloss: 0.278402\tvalid_1's binary_logloss: 0.281018\n",
      "[600]\ttraining's binary_logloss: 0.27668\tvalid_1's binary_logloss: 0.279474\n",
      "[650]\ttraining's binary_logloss: 0.272821\tvalid_1's binary_logloss: 0.276047\n",
      "[700]\ttraining's binary_logloss: 0.270777\tvalid_1's binary_logloss: 0.274315\n",
      "[750]\ttraining's binary_logloss: 0.268574\tvalid_1's binary_logloss: 0.272475\n",
      "[800]\ttraining's binary_logloss: 0.26721\tvalid_1's binary_logloss: 0.271371\n",
      "[850]\ttraining's binary_logloss: 0.265315\tvalid_1's binary_logloss: 0.269919\n",
      "[900]\ttraining's binary_logloss: 0.264414\tvalid_1's binary_logloss: 0.269244\n",
      "[950]\ttraining's binary_logloss: 0.264387\tvalid_1's binary_logloss: 0.269332\n",
      "[1000]\ttraining's binary_logloss: 0.26334\tvalid_1's binary_logloss: 0.268575\n",
      "[1050]\ttraining's binary_logloss: 0.262719\tvalid_1's binary_logloss: 0.268179\n",
      "[1100]\ttraining's binary_logloss: 0.261275\tvalid_1's binary_logloss: 0.267219\n",
      "[1150]\ttraining's binary_logloss: 0.261224\tvalid_1's binary_logloss: 0.267268\n",
      "[1200]\ttraining's binary_logloss: 0.260609\tvalid_1's binary_logloss: 0.266933\n",
      "[1250]\ttraining's binary_logloss: 0.260038\tvalid_1's binary_logloss: 0.266643\n",
      "[1300]\ttraining's binary_logloss: 0.259739\tvalid_1's binary_logloss: 0.266511\n",
      "[1350]\ttraining's binary_logloss: 0.259495\tvalid_1's binary_logloss: 0.266429\n",
      "[1400]\ttraining's binary_logloss: 0.258533\tvalid_1's binary_logloss: 0.265853\n",
      "[1450]\ttraining's binary_logloss: 0.258232\tvalid_1's binary_logloss: 0.26575\n",
      "[1500]\ttraining's binary_logloss: 0.257935\tvalid_1's binary_logloss: 0.265634\n",
      "[1550]\ttraining's binary_logloss: 0.256956\tvalid_1's binary_logloss: 0.26509\n",
      "[1600]\ttraining's binary_logloss: 0.25624\tvalid_1's binary_logloss: 0.264761\n",
      "[1650]\ttraining's binary_logloss: 0.255688\tvalid_1's binary_logloss: 0.264512\n",
      "[1700]\ttraining's binary_logloss: 0.255384\tvalid_1's binary_logloss: 0.264427\n",
      "[1750]\ttraining's binary_logloss: 0.255071\tvalid_1's binary_logloss: 0.264351\n",
      "[1800]\ttraining's binary_logloss: 0.254926\tvalid_1's binary_logloss: 0.264366\n",
      "[1850]\ttraining's binary_logloss: 0.254527\tvalid_1's binary_logloss: 0.264228\n",
      "[1900]\ttraining's binary_logloss: 0.254102\tvalid_1's binary_logloss: 0.264094\n",
      "[1950]\ttraining's binary_logloss: 0.253489\tvalid_1's binary_logloss: 0.26388\n",
      "[2000]\ttraining's binary_logloss: 0.253075\tvalid_1's binary_logloss: 0.263751\n",
      "[2050]\ttraining's binary_logloss: 0.25278\tvalid_1's binary_logloss: 0.263689\n",
      "[2100]\ttraining's binary_logloss: 0.252475\tvalid_1's binary_logloss: 0.263634\n",
      "[2150]\ttraining's binary_logloss: 0.252452\tvalid_1's binary_logloss: 0.263692\n",
      "[2200]\ttraining's binary_logloss: 0.251986\tvalid_1's binary_logloss: 0.263547\n",
      "[2250]\ttraining's binary_logloss: 0.251638\tvalid_1's binary_logloss: 0.263468\n",
      "[2300]\ttraining's binary_logloss: 0.251158\tvalid_1's binary_logloss: 0.263335\n",
      "[2350]\ttraining's binary_logloss: 0.250871\tvalid_1's binary_logloss: 0.263307\n",
      "[2400]\ttraining's binary_logloss: 0.250633\tvalid_1's binary_logloss: 0.263255\n",
      "[2450]\ttraining's binary_logloss: 0.250239\tvalid_1's binary_logloss: 0.263181\n",
      "[2500]\ttraining's binary_logloss: 0.250054\tvalid_1's binary_logloss: 0.263182\n",
      "[2550]\ttraining's binary_logloss: 0.249721\tvalid_1's binary_logloss: 0.263098\n",
      "[2600]\ttraining's binary_logloss: 0.249398\tvalid_1's binary_logloss: 0.263039\n",
      "[2650]\ttraining's binary_logloss: 0.249058\tvalid_1's binary_logloss: 0.262968\n",
      "[2700]\ttraining's binary_logloss: 0.248653\tvalid_1's binary_logloss: 0.262888\n",
      "[2750]\ttraining's binary_logloss: 0.248501\tvalid_1's binary_logloss: 0.262895\n",
      "[2800]\ttraining's binary_logloss: 0.248241\tvalid_1's binary_logloss: 0.262871\n",
      "[2850]\ttraining's binary_logloss: 0.247729\tvalid_1's binary_logloss: 0.262737\n",
      "[2900]\ttraining's binary_logloss: 0.247474\tvalid_1's binary_logloss: 0.262704\n",
      "[2950]\ttraining's binary_logloss: 0.247029\tvalid_1's binary_logloss: 0.262613\n",
      "[3000]\ttraining's binary_logloss: 0.246662\tvalid_1's binary_logloss: 0.262573\n",
      "[3050]\ttraining's binary_logloss: 0.246343\tvalid_1's binary_logloss: 0.262523\n",
      "[3100]\ttraining's binary_logloss: 0.246036\tvalid_1's binary_logloss: 0.262502\n",
      "[3150]\ttraining's binary_logloss: 0.245785\tvalid_1's binary_logloss: 0.262487\n",
      "[3200]\ttraining's binary_logloss: 0.24538\tvalid_1's binary_logloss: 0.262422\n",
      "[3250]\ttraining's binary_logloss: 0.245056\tvalid_1's binary_logloss: 0.262374\n",
      "[3300]\ttraining's binary_logloss: 0.244789\tvalid_1's binary_logloss: 0.262356\n",
      "[3350]\ttraining's binary_logloss: 0.244384\tvalid_1's binary_logloss: 0.262301\n",
      "[3400]\ttraining's binary_logloss: 0.243995\tvalid_1's binary_logloss: 0.262244\n",
      "[3450]\ttraining's binary_logloss: 0.243617\tvalid_1's binary_logloss: 0.262203\n",
      "[3500]\ttraining's binary_logloss: 0.24337\tvalid_1's binary_logloss: 0.26221\n",
      "[3550]\ttraining's binary_logloss: 0.243058\tvalid_1's binary_logloss: 0.262203\n",
      "[3600]\ttraining's binary_logloss: 0.24278\tvalid_1's binary_logloss: 0.262189\n",
      "[3650]\ttraining's binary_logloss: 0.242493\tvalid_1's binary_logloss: 0.262151\n",
      "[3700]\ttraining's binary_logloss: 0.24195\tvalid_1's binary_logloss: 0.262101\n",
      "[3750]\ttraining's binary_logloss: 0.241652\tvalid_1's binary_logloss: 0.262088\n",
      "[3800]\ttraining's binary_logloss: 0.241414\tvalid_1's binary_logloss: 0.262065\n",
      "[3850]\ttraining's binary_logloss: 0.241108\tvalid_1's binary_logloss: 0.262057\n",
      "[3900]\ttraining's binary_logloss: 0.240904\tvalid_1's binary_logloss: 0.26207\n",
      "[3950]\ttraining's binary_logloss: 0.240821\tvalid_1's binary_logloss: 0.2621\n",
      "[4000]\ttraining's binary_logloss: 0.240401\tvalid_1's binary_logloss: 0.262051\n",
      "[4050]\ttraining's binary_logloss: 0.240144\tvalid_1's binary_logloss: 0.262044\n",
      "[4100]\ttraining's binary_logloss: 0.239793\tvalid_1's binary_logloss: 0.262018\n",
      "[4150]\ttraining's binary_logloss: 0.239349\tvalid_1's binary_logloss: 0.261951\n",
      "[4200]\ttraining's binary_logloss: 0.239067\tvalid_1's binary_logloss: 0.26195\n",
      "[4250]\ttraining's binary_logloss: 0.238761\tvalid_1's binary_logloss: 0.261937\n",
      "[4300]\ttraining's binary_logloss: 0.238346\tvalid_1's binary_logloss: 0.261901\n",
      "[4350]\ttraining's binary_logloss: 0.238202\tvalid_1's binary_logloss: 0.26191\n",
      "[4400]\ttraining's binary_logloss: 0.23801\tvalid_1's binary_logloss: 0.261909\n",
      "[4450]\ttraining's binary_logloss: 0.237648\tvalid_1's binary_logloss: 0.261886\n",
      "[4500]\ttraining's binary_logloss: 0.237434\tvalid_1's binary_logloss: 0.261884\n",
      " - 0 round - train_metric: 0.672385 - valid_metric: 0.672404\n",
      "\n",
      " - 50 round - train_metric: 0.426356 - valid_metric: 0.426792\n",
      "\n",
      " - 100 round - train_metric: 0.366879 - valid_metric: 0.367584\n",
      "\n",
      " - 150 round - train_metric: 0.350049 - valid_metric: 0.350846\n",
      "\n",
      " - 200 round - train_metric: 0.338558 - valid_metric: 0.339479\n",
      "\n",
      " - 250 round - train_metric: 0.312492 - valid_metric: 0.313708\n",
      "\n",
      " - 300 round - train_metric: 0.300861 - valid_metric: 0.302368\n",
      "\n",
      " - 350 round - train_metric: 0.294365 - valid_metric: 0.296098\n",
      "\n",
      " - 400 round - train_metric: 0.291323 - valid_metric: 0.293206\n",
      "\n",
      " - 450 round - train_metric: 0.289836 - valid_metric: 0.291833\n",
      "\n",
      " - 500 round - train_metric: 0.280592 - valid_metric: 0.283024\n",
      "\n",
      " - 550 round - train_metric: 0.278119 - valid_metric: 0.280745\n",
      "\n",
      " - 600 round - train_metric: 0.276430 - valid_metric: 0.279242\n",
      "\n",
      " - 650 round - train_metric: 0.272913 - valid_metric: 0.276131\n",
      "\n",
      " - 700 round - train_metric: 0.270863 - valid_metric: 0.274393\n",
      "\n",
      " - 750 round - train_metric: 0.268649 - valid_metric: 0.272540\n",
      "\n",
      " - 800 round - train_metric: 0.267264 - valid_metric: 0.271418\n",
      "\n",
      " - 850 round - train_metric: 0.265239 - valid_metric: 0.269856\n",
      "\n",
      " - 900 round - train_metric: 0.264340 - valid_metric: 0.269191\n",
      "\n",
      " - 950 round - train_metric: 0.264308 - valid_metric: 0.269269\n",
      "\n",
      " - 1000 round - train_metric: 0.263382 - valid_metric: 0.268609\n",
      "\n",
      " - 1050 round - train_metric: 0.262650 - valid_metric: 0.268137\n",
      "\n",
      " - 1100 round - train_metric: 0.261222 - valid_metric: 0.267186\n",
      "\n",
      " - 1150 round - train_metric: 0.261252 - valid_metric: 0.267291\n",
      "\n",
      " - 1200 round - train_metric: 0.260638 - valid_metric: 0.266956\n",
      "\n",
      " - 1250 round - train_metric: 0.260063 - valid_metric: 0.266663\n",
      "\n",
      " - 1300 round - train_metric: 0.259764 - valid_metric: 0.266531\n",
      "\n",
      " - 1350 round - train_metric: 0.259523 - valid_metric: 0.266451\n",
      "\n",
      " - 1400 round - train_metric: 0.258491 - valid_metric: 0.265833\n",
      "\n",
      " - 1450 round - train_metric: 0.258256 - valid_metric: 0.265768\n",
      "\n",
      " - 1500 round - train_metric: 0.257894 - valid_metric: 0.265604\n",
      "\n",
      " - 1550 round - train_metric: 0.256917 - valid_metric: 0.265069\n",
      "\n",
      " - 1600 round - train_metric: 0.256261 - valid_metric: 0.264774\n",
      "\n",
      " - 1650 round - train_metric: 0.255653 - valid_metric: 0.264492\n",
      "\n",
      " - 1700 round - train_metric: 0.255348 - valid_metric: 0.264410\n",
      "\n",
      " - 1750 round - train_metric: 0.255089 - valid_metric: 0.264362\n",
      "\n",
      " - 1800 round - train_metric: 0.254892 - valid_metric: 0.264347\n",
      "\n",
      " - 1850 round - train_metric: 0.254549 - valid_metric: 0.264242\n",
      "\n",
      " - 1900 round - train_metric: 0.254070 - valid_metric: 0.264077\n",
      "\n",
      " - 1950 round - train_metric: 0.253503 - valid_metric: 0.263889\n",
      "\n",
      " - 2000 round - train_metric: 0.253048 - valid_metric: 0.263738\n",
      "\n",
      " - 2050 round - train_metric: 0.252794 - valid_metric: 0.263697\n",
      "\n",
      " - 2100 round - train_metric: 0.252486 - valid_metric: 0.263641\n",
      "\n",
      " - 2150 round - train_metric: 0.252469 - valid_metric: 0.263703\n",
      "\n",
      " - 2200 round - train_metric: 0.252001 - valid_metric: 0.263556\n",
      "\n",
      " - 2250 round - train_metric: 0.251608 - valid_metric: 0.263459\n",
      "\n",
      " - 2300 round - train_metric: 0.251173 - valid_metric: 0.263343\n",
      "\n",
      " - 2350 round - train_metric: 0.250887 - valid_metric: 0.263316\n",
      "\n",
      " - 2400 round - train_metric: 0.250604 - valid_metric: 0.263245\n",
      "\n",
      " - 2450 round - train_metric: 0.250212 - valid_metric: 0.263174\n",
      "\n",
      " - 2500 round - train_metric: 0.250063 - valid_metric: 0.263187\n",
      "\n",
      " - 2550 round - train_metric: 0.249697 - valid_metric: 0.263086\n",
      "\n",
      " - 2600 round - train_metric: 0.249406 - valid_metric: 0.263044\n",
      "\n",
      " - 2650 round - train_metric: 0.249065 - valid_metric: 0.262972\n",
      "\n",
      " - 2700 round - train_metric: 0.248663 - valid_metric: 0.262893\n",
      "\n",
      " - 2750 round - train_metric: 0.248510 - valid_metric: 0.262899\n",
      "\n",
      " - 2800 round - train_metric: 0.248218 - valid_metric: 0.262863\n",
      "\n",
      " - 2850 round - train_metric: 0.247737 - valid_metric: 0.262741\n",
      "\n",
      " - 2900 round - train_metric: 0.247486 - valid_metric: 0.262710\n",
      "\n",
      " - 2950 round - train_metric: 0.247037 - valid_metric: 0.262617\n",
      "\n",
      " - 3000 round - train_metric: 0.246670 - valid_metric: 0.262577\n",
      "\n",
      " - 3050 round - train_metric: 0.246322 - valid_metric: 0.262518\n",
      "\n",
      " - 3100 round - train_metric: 0.246042 - valid_metric: 0.262505\n",
      "\n",
      " - 3150 round - train_metric: 0.245794 - valid_metric: 0.262491\n",
      "\n",
      " - 3200 round - train_metric: 0.245360 - valid_metric: 0.262417\n",
      "\n",
      " - 3250 round - train_metric: 0.245062 - valid_metric: 0.262377\n",
      "\n",
      " - 3300 round - train_metric: 0.244796 - valid_metric: 0.262359\n",
      "\n",
      " - 3350 round - train_metric: 0.244391 - valid_metric: 0.262304\n",
      "\n",
      " - 3400 round - train_metric: 0.244002 - valid_metric: 0.262247\n",
      "\n",
      " - 3450 round - train_metric: 0.243601 - valid_metric: 0.262199\n",
      "\n",
      " - 3500 round - train_metric: 0.243347 - valid_metric: 0.262208\n",
      "\n",
      " - 3550 round - train_metric: 0.243064 - valid_metric: 0.262206\n",
      "\n",
      " - 3600 round - train_metric: 0.242787 - valid_metric: 0.262191\n",
      "\n",
      " - 3650 round - train_metric: 0.242502 - valid_metric: 0.262155\n",
      "\n",
      " - 3700 round - train_metric: 0.241933 - valid_metric: 0.262095\n",
      "\n",
      " - 3750 round - train_metric: 0.241635 - valid_metric: 0.262082\n",
      "\n",
      " - 3800 round - train_metric: 0.241394 - valid_metric: 0.262061\n",
      "\n",
      " - 3850 round - train_metric: 0.241087 - valid_metric: 0.262056\n",
      "\n",
      " - 3900 round - train_metric: 0.240908 - valid_metric: 0.262072\n",
      "\n",
      " - 3950 round - train_metric: 0.240803 - valid_metric: 0.262092\n",
      "\n",
      " - 4000 round - train_metric: 0.240380 - valid_metric: 0.262046\n",
      "\n",
      " - 4050 round - train_metric: 0.240150 - valid_metric: 0.262046\n",
      "\n",
      " - 4100 round - train_metric: 0.239801 - valid_metric: 0.262020\n",
      "\n",
      " - 4150 round - train_metric: 0.239357 - valid_metric: 0.261953\n",
      "\n",
      " - 4200 round - train_metric: 0.239074 - valid_metric: 0.261952\n",
      "\n",
      " - 4250 round - train_metric: 0.238767 - valid_metric: 0.261939\n",
      "\n",
      " - 4300 round - train_metric: 0.238354 - valid_metric: 0.261903\n",
      "\n",
      " - 4350 round - train_metric: 0.238185 - valid_metric: 0.261906\n",
      "\n",
      " - 4400 round - train_metric: 0.237995 - valid_metric: 0.261907\n",
      "\n",
      " - 4450 round - train_metric: 0.237654 - valid_metric: 0.261888\n",
      "\n",
      "- fold4 valid metric: 0.711078\n",
      "\n",
      "all valid mean metric:0.711557, global valid metric:0.711526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                               customer_ID    target\n",
       " 0        0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  0.001787\n",
       " 1        0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  0.001801\n",
       " 2        0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  0.002086\n",
       " 3        0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  0.001897\n",
       " 4        0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  0.001420\n",
       " ...                                                    ...       ...\n",
       " 5531446  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...  0.003283\n",
       " 5531447  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...  0.002004\n",
       " 5531448  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...  0.002349\n",
       " 5531449  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...  0.001699\n",
       " 5531450  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...  0.002380\n",
       " \n",
       " [5531451 rows x 2 columns],\n",
       "                                                 customer_ID  prediction\n",
       " 0         00000469ba478561f23a92a868bd366de6f6527a684c9a...    0.164115\n",
       " 1         00000469ba478561f23a92a868bd366de6f6527a684c9a...    0.142807\n",
       " 2         00000469ba478561f23a92a868bd366de6f6527a684c9a...    0.163166\n",
       " 3         00000469ba478561f23a92a868bd366de6f6527a684c9a...    0.174766\n",
       " 4         00000469ba478561f23a92a868bd366de6f6527a684c9a...    0.139819\n",
       " ...                                                     ...         ...\n",
       " 11363757  fffffa7cf7e453e1acc6a1426475d5cb9400859f82ff61...    0.114035\n",
       " 11363758  fffffa7cf7e453e1acc6a1426475d5cb9400859f82ff61...    0.100951\n",
       " 11363759  fffffa7cf7e453e1acc6a1426475d5cb9400859f82ff61...    0.086864\n",
       " 11363760  fffffa7cf7e453e1acc6a1426475d5cb9400859f82ff61...    0.063666\n",
       " 11363761  fffffa7cf7e453e1acc6a1426475d5cb9400859f82ff61...    0.079598\n",
       " \n",
       " [11363762 rows x 2 columns],\n",
       " (0.7115572973601801, 0.7115255568439258))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lgb_train_and_predict(train, test, lgb_config, gkf=True, aug=None, run_id=\"LGB_with_series_feature\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c90c4efae355953581131fe5eb685f809605ddb2abb7edf8826c72b30e930c0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
