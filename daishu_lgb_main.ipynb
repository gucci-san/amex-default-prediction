{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S5_LGB_main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available gpus: [0]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc,os,random\n",
    "import time,datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from utils import *\n",
    "from model import *\n",
    "root = args.root\n",
    "seed = args.seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(f\"{root}/all_feature.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_csv(f\"/mnt/sdb/KAGGLE_DATA/amex-default-prediction/train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df[:train_y.shape[0]]\n",
    "train[\"target\"] = train_y[\"target\"]\n",
    "test = df[train_y.shape[0]:].reset_index(drop=True)\n",
    "del df; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458913, 6390) (924621, 6389)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_config = {\n",
    "    \"lgb_params\": {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"boosting\": \"dart\",\n",
    "        \"max_depth\": -1,\n",
    "        \"num_leaves\": 64,\n",
    "        \"learning_rate\": 0.035,\n",
    "        \"bagging_freq\": 5,\n",
    "        \"bagging_fraction\": 0.75,\n",
    "        \"feature_fraction\": 0.05,\n",
    "        \"min_data_in_leaf\": 256,\n",
    "        \"max_bin\": 63,\n",
    "        \"min_data_in_bin\": 256,\n",
    "        #\"min_sum_hessian_in_leaf\": 10,\n",
    "        \"tree_learner\": \"serial\",\n",
    "        \"boost_from_average\": \"false\",\n",
    "        \"lambda_l1\": 0.1,\n",
    "        \"lambda_l2\": 30,\n",
    "        \"num_threads\": 16,\n",
    "        \"verbosity\": 1\n",
    "    },\n",
    "    \"feature_name\": [],\n",
    "    \"rounds\": 4500,\n",
    "    \"early_stopping_rounds\": 100,\n",
    "    \"verbose_eval\": 50,\n",
    "    \"folds\": 5,\n",
    "    \"seed\": seed,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainとtest同時にやろうとするとメモリ終わるので、分けてやる必要アリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del test; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m Lgb_train_and_predict : LGB_with_manual_feature\u001b[39m\n",
      "\u001b[32m     ... start train\u001b[39m\n",
      "\u001b[32m     ... folds defined\u001b[39m\n",
      "\u001b[34m        ... fold : 0\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: cannot stat './*.sh': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.571873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 212196\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 6285\n",
      "[50]\ttraining's binary_logloss: 0.399496\tvalid_1's binary_logloss: 0.399914\n",
      "[100]\ttraining's binary_logloss: 0.339088\tvalid_1's binary_logloss: 0.339829\n",
      "[150]\ttraining's binary_logloss: 0.313336\tvalid_1's binary_logloss: 0.314433\n",
      "[200]\ttraining's binary_logloss: 0.300538\tvalid_1's binary_logloss: 0.30186\n",
      "[250]\ttraining's binary_logloss: 0.272821\tvalid_1's binary_logloss: 0.274745\n",
      "[300]\ttraining's binary_logloss: 0.258082\tvalid_1's binary_logloss: 0.260577\n",
      "[350]\ttraining's binary_logloss: 0.24827\tvalid_1's binary_logloss: 0.251258\n",
      "[400]\ttraining's binary_logloss: 0.244573\tvalid_1's binary_logloss: 0.247888\n",
      "[450]\ttraining's binary_logloss: 0.242633\tvalid_1's binary_logloss: 0.246146\n",
      "[500]\ttraining's binary_logloss: 0.231773\tvalid_1's binary_logloss: 0.23621\n",
      "[550]\ttraining's binary_logloss: 0.228483\tvalid_1's binary_logloss: 0.233411\n",
      "[600]\ttraining's binary_logloss: 0.226099\tvalid_1's binary_logloss: 0.231482\n",
      "[650]\ttraining's binary_logloss: 0.2209\tvalid_1's binary_logloss: 0.227189\n",
      "[700]\ttraining's binary_logloss: 0.217961\tvalid_1's binary_logloss: 0.225026\n",
      "[750]\ttraining's binary_logloss: 0.214782\tvalid_1's binary_logloss: 0.22275\n",
      "[800]\ttraining's binary_logloss: 0.212684\tvalid_1's binary_logloss: 0.221406\n",
      "[850]\ttraining's binary_logloss: 0.209921\tvalid_1's binary_logloss: 0.219723\n",
      "[900]\ttraining's binary_logloss: 0.208503\tvalid_1's binary_logloss: 0.21905\n",
      "[950]\ttraining's binary_logloss: 0.208264\tvalid_1's binary_logloss: 0.219111\n",
      "[1000]\ttraining's binary_logloss: 0.206545\tvalid_1's binary_logloss: 0.218287\n",
      "[1050]\ttraining's binary_logloss: 0.205445\tvalid_1's binary_logloss: 0.217823\n",
      "[1100]\ttraining's binary_logloss: 0.203056\tvalid_1's binary_logloss: 0.216761\n",
      "[1150]\ttraining's binary_logloss: 0.202828\tvalid_1's binary_logloss: 0.216869\n",
      "[1200]\ttraining's binary_logloss: 0.201705\tvalid_1's binary_logloss: 0.216497\n",
      "[1250]\ttraining's binary_logloss: 0.200593\tvalid_1's binary_logloss: 0.216126\n",
      "[1300]\ttraining's binary_logloss: 0.199902\tvalid_1's binary_logloss: 0.215962\n",
      "[1350]\ttraining's binary_logloss: 0.19932\tvalid_1's binary_logloss: 0.215825\n",
      "[1400]\ttraining's binary_logloss: 0.197617\tvalid_1's binary_logloss: 0.215204\n",
      "[1450]\ttraining's binary_logloss: 0.196988\tvalid_1's binary_logloss: 0.215105\n",
      "[1500]\ttraining's binary_logloss: 0.196354\tvalid_1's binary_logloss: 0.21496\n",
      "[1550]\ttraining's binary_logloss: 0.194577\tvalid_1's binary_logloss: 0.214402\n",
      "[1600]\ttraining's binary_logloss: 0.193251\tvalid_1's binary_logloss: 0.214058\n",
      "[1650]\ttraining's binary_logloss: 0.192178\tvalid_1's binary_logloss: 0.213876\n",
      "[1700]\ttraining's binary_logloss: 0.191534\tvalid_1's binary_logloss: 0.213775\n",
      "[1750]\ttraining's binary_logloss: 0.190839\tvalid_1's binary_logloss: 0.213758\n",
      "[1800]\ttraining's binary_logloss: 0.190425\tvalid_1's binary_logloss: 0.213717\n",
      "[1850]\ttraining's binary_logloss: 0.189553\tvalid_1's binary_logloss: 0.213547\n",
      "[1900]\ttraining's binary_logloss: 0.188671\tvalid_1's binary_logloss: 0.213428\n",
      "[1950]\ttraining's binary_logloss: 0.18747\tvalid_1's binary_logloss: 0.213154\n",
      "[2000]\ttraining's binary_logloss: 0.186614\tvalid_1's binary_logloss: 0.212997\n",
      "[2050]\ttraining's binary_logloss: 0.185956\tvalid_1's binary_logloss: 0.212892\n",
      "[2100]\ttraining's binary_logloss: 0.185253\tvalid_1's binary_logloss: 0.212834\n",
      "[2150]\ttraining's binary_logloss: 0.185078\tvalid_1's binary_logloss: 0.212906\n",
      "[2200]\ttraining's binary_logloss: 0.184095\tvalid_1's binary_logloss: 0.212699\n",
      "[2250]\ttraining's binary_logloss: 0.183309\tvalid_1's binary_logloss: 0.212608\n",
      "[2300]\ttraining's binary_logloss: 0.182324\tvalid_1's binary_logloss: 0.212452\n",
      "[2350]\ttraining's binary_logloss: 0.181716\tvalid_1's binary_logloss: 0.21239\n",
      "[2400]\ttraining's binary_logloss: 0.181155\tvalid_1's binary_logloss: 0.212346\n",
      "[2450]\ttraining's binary_logloss: 0.180364\tvalid_1's binary_logloss: 0.212237\n",
      "[2500]\ttraining's binary_logloss: 0.179885\tvalid_1's binary_logloss: 0.212218\n",
      "[2550]\ttraining's binary_logloss: 0.179186\tvalid_1's binary_logloss: 0.212132\n",
      "[2600]\ttraining's binary_logloss: 0.178497\tvalid_1's binary_logloss: 0.212079\n",
      "[2650]\ttraining's binary_logloss: 0.177748\tvalid_1's binary_logloss: 0.212007\n",
      "[2700]\ttraining's binary_logloss: 0.176899\tvalid_1's binary_logloss: 0.211934\n",
      "[2750]\ttraining's binary_logloss: 0.176511\tvalid_1's binary_logloss: 0.211936\n",
      "[2800]\ttraining's binary_logloss: 0.175939\tvalid_1's binary_logloss: 0.211943\n",
      "[2850]\ttraining's binary_logloss: 0.174848\tvalid_1's binary_logloss: 0.211821\n",
      "[2900]\ttraining's binary_logloss: 0.174273\tvalid_1's binary_logloss: 0.21179\n",
      "[2950]\ttraining's binary_logloss: 0.173351\tvalid_1's binary_logloss: 0.211742\n",
      "[3000]\ttraining's binary_logloss: 0.172581\tvalid_1's binary_logloss: 0.211681\n",
      "[3050]\ttraining's binary_logloss: 0.1719\tvalid_1's binary_logloss: 0.211636\n",
      "[3100]\ttraining's binary_logloss: 0.171246\tvalid_1's binary_logloss: 0.211617\n",
      "[3150]\ttraining's binary_logloss: 0.170678\tvalid_1's binary_logloss: 0.211573\n",
      "[3200]\ttraining's binary_logloss: 0.16981\tvalid_1's binary_logloss: 0.211499\n",
      "[3250]\ttraining's binary_logloss: 0.169078\tvalid_1's binary_logloss: 0.211461\n",
      "[3300]\ttraining's binary_logloss: 0.168458\tvalid_1's binary_logloss: 0.211447\n",
      "[3350]\ttraining's binary_logloss: 0.167592\tvalid_1's binary_logloss: 0.211432\n",
      "[3400]\ttraining's binary_logloss: 0.166707\tvalid_1's binary_logloss: 0.211403\n",
      "[3450]\ttraining's binary_logloss: 0.16593\tvalid_1's binary_logloss: 0.211335\n",
      "[3500]\ttraining's binary_logloss: 0.165414\tvalid_1's binary_logloss: 0.211318\n",
      "[3550]\ttraining's binary_logloss: 0.164728\tvalid_1's binary_logloss: 0.211258\n",
      "[3600]\ttraining's binary_logloss: 0.164098\tvalid_1's binary_logloss: 0.211254\n",
      "[3650]\ttraining's binary_logloss: 0.16345\tvalid_1's binary_logloss: 0.211223\n",
      "[3700]\ttraining's binary_logloss: 0.162299\tvalid_1's binary_logloss: 0.211137\n",
      "[3750]\ttraining's binary_logloss: 0.161654\tvalid_1's binary_logloss: 0.211117\n",
      "[3800]\ttraining's binary_logloss: 0.161105\tvalid_1's binary_logloss: 0.211074\n",
      "[3850]\ttraining's binary_logloss: 0.160456\tvalid_1's binary_logloss: 0.211063\n",
      "[3900]\ttraining's binary_logloss: 0.160009\tvalid_1's binary_logloss: 0.211041\n",
      "[3950]\ttraining's binary_logloss: 0.159798\tvalid_1's binary_logloss: 0.21106\n",
      "[4000]\ttraining's binary_logloss: 0.158917\tvalid_1's binary_logloss: 0.211028\n",
      "[4050]\ttraining's binary_logloss: 0.158349\tvalid_1's binary_logloss: 0.211045\n",
      "[4100]\ttraining's binary_logloss: 0.15768\tvalid_1's binary_logloss: 0.211036\n",
      "[4150]\ttraining's binary_logloss: 0.156743\tvalid_1's binary_logloss: 0.211006\n",
      "[4200]\ttraining's binary_logloss: 0.156128\tvalid_1's binary_logloss: 0.210958\n",
      "[4250]\ttraining's binary_logloss: 0.155472\tvalid_1's binary_logloss: 0.210894\n",
      "[4300]\ttraining's binary_logloss: 0.154576\tvalid_1's binary_logloss: 0.210913\n",
      "[4350]\ttraining's binary_logloss: 0.154248\tvalid_1's binary_logloss: 0.210922\n",
      "[4400]\ttraining's binary_logloss: 0.153816\tvalid_1's binary_logloss: 0.210942\n",
      "[4450]\ttraining's binary_logloss: 0.153016\tvalid_1's binary_logloss: 0.210925\n",
      "[4500]\ttraining's binary_logloss: 0.152559\tvalid_1's binary_logloss: 0.210933\n",
      " - 0 round - train_metric: 0.670838 - valid_metric: 0.670833\n",
      "\n",
      " - 50 round - train_metric: 0.401958 - valid_metric: 0.402370\n",
      "\n",
      " - 100 round - train_metric: 0.335043 - valid_metric: 0.335827\n",
      "\n",
      " - 150 round - train_metric: 0.315207 - valid_metric: 0.316282\n",
      "\n",
      " - 200 round - train_metric: 0.301754 - valid_metric: 0.303060\n",
      "\n",
      " - 250 round - train_metric: 0.271206 - valid_metric: 0.273162\n",
      "\n",
      " - 300 round - train_metric: 0.256882 - valid_metric: 0.259416\n",
      "\n",
      " - 350 round - train_metric: 0.248795 - valid_metric: 0.251760\n",
      "\n",
      " - 400 round - train_metric: 0.244975 - valid_metric: 0.248271\n",
      "\n",
      " - 450 round - train_metric: 0.242983 - valid_metric: 0.246478\n",
      "\n",
      " - 500 round - train_metric: 0.231344 - valid_metric: 0.235827\n",
      "\n",
      " - 550 round - train_metric: 0.228106 - valid_metric: 0.233093\n",
      "\n",
      " - 600 round - train_metric: 0.225777 - valid_metric: 0.231199\n",
      "\n",
      " - 650 round - train_metric: 0.221018 - valid_metric: 0.227291\n",
      "\n",
      " - 700 round - train_metric: 0.218077 - valid_metric: 0.225121\n",
      "\n",
      " - 750 round - train_metric: 0.214881 - valid_metric: 0.222826\n",
      "\n",
      " - 800 round - train_metric: 0.212755 - valid_metric: 0.221461\n",
      "\n",
      " - 850 round - train_metric: 0.209803 - valid_metric: 0.219658\n",
      "\n",
      " - 900 round - train_metric: 0.208391 - valid_metric: 0.218988\n",
      "\n",
      " - 950 round - train_metric: 0.208156 - valid_metric: 0.219042\n",
      "\n",
      " - 1000 round - train_metric: 0.206602 - valid_metric: 0.218326\n",
      "\n",
      " - 1050 round - train_metric: 0.205352 - valid_metric: 0.217781\n",
      "\n",
      " - 1100 round - train_metric: 0.202973 - valid_metric: 0.216728\n",
      "\n",
      " - 1150 round - train_metric: 0.202869 - valid_metric: 0.216896\n",
      "\n",
      " - 1200 round - train_metric: 0.201746 - valid_metric: 0.216523\n",
      "\n",
      " - 1250 round - train_metric: 0.200628 - valid_metric: 0.216147\n",
      "\n",
      " - 1300 round - train_metric: 0.199937 - valid_metric: 0.215983\n",
      "\n",
      " - 1350 round - train_metric: 0.199362 - valid_metric: 0.215848\n",
      "\n",
      " - 1400 round - train_metric: 0.197545 - valid_metric: 0.215175\n",
      "\n",
      " - 1450 round - train_metric: 0.197024 - valid_metric: 0.215124\n",
      "\n",
      " - 1500 round - train_metric: 0.196281 - valid_metric: 0.214938\n",
      "\n",
      " - 1550 round - train_metric: 0.194514 - valid_metric: 0.214381\n",
      "\n",
      " - 1600 round - train_metric: 0.193286 - valid_metric: 0.214072\n",
      "\n",
      " - 1650 round - train_metric: 0.192115 - valid_metric: 0.213854\n",
      "\n",
      " - 1700 round - train_metric: 0.191478 - valid_metric: 0.213759\n",
      "\n",
      " - 1750 round - train_metric: 0.190868 - valid_metric: 0.213770\n",
      "\n",
      " - 1800 round - train_metric: 0.190361 - valid_metric: 0.213691\n",
      "\n",
      " - 1850 round - train_metric: 0.189589 - valid_metric: 0.213560\n",
      "\n",
      " - 1900 round - train_metric: 0.188614 - valid_metric: 0.213406\n",
      "\n",
      " - 1950 round - train_metric: 0.187494 - valid_metric: 0.213163\n",
      "\n",
      " - 2000 round - train_metric: 0.186560 - valid_metric: 0.212983\n",
      "\n",
      " - 2050 round - train_metric: 0.185978 - valid_metric: 0.212900\n",
      "\n",
      " - 2100 round - train_metric: 0.185271 - valid_metric: 0.212841\n",
      "\n",
      " - 2150 round - train_metric: 0.185105 - valid_metric: 0.212916\n",
      "\n",
      " - 2200 round - train_metric: 0.184120 - valid_metric: 0.212708\n",
      "\n",
      " - 2250 round - train_metric: 0.183251 - valid_metric: 0.212594\n",
      "\n",
      " - 2300 round - train_metric: 0.182348 - valid_metric: 0.212460\n",
      "\n",
      " - 2350 round - train_metric: 0.181745 - valid_metric: 0.212400\n",
      "\n",
      " - 2400 round - train_metric: 0.181107 - valid_metric: 0.212333\n",
      "\n",
      " - 2450 round - train_metric: 0.180314 - valid_metric: 0.212229\n",
      "\n",
      " - 2500 round - train_metric: 0.179899 - valid_metric: 0.212224\n",
      "\n",
      " - 2550 round - train_metric: 0.179133 - valid_metric: 0.212126\n",
      "\n",
      " - 2600 round - train_metric: 0.178509 - valid_metric: 0.212084\n",
      "\n",
      " - 2650 round - train_metric: 0.177761 - valid_metric: 0.212011\n",
      "\n",
      " - 2700 round - train_metric: 0.176918 - valid_metric: 0.211939\n",
      "\n",
      " - 2750 round - train_metric: 0.176526 - valid_metric: 0.211939\n",
      "\n",
      " - 2800 round - train_metric: 0.175892 - valid_metric: 0.211934\n",
      "\n",
      " - 2850 round - train_metric: 0.174862 - valid_metric: 0.211825\n",
      "\n",
      " - 2900 round - train_metric: 0.174295 - valid_metric: 0.211795\n",
      "\n",
      " - 2950 round - train_metric: 0.173366 - valid_metric: 0.211746\n",
      "\n",
      " - 3000 round - train_metric: 0.172596 - valid_metric: 0.211684\n",
      "\n",
      " - 3050 round - train_metric: 0.171854 - valid_metric: 0.211631\n",
      "\n",
      " - 3100 round - train_metric: 0.171258 - valid_metric: 0.211619\n",
      "\n",
      " - 3150 round - train_metric: 0.170695 - valid_metric: 0.211576\n",
      "\n",
      " - 3200 round - train_metric: 0.169768 - valid_metric: 0.211493\n",
      "\n",
      " - 3250 round - train_metric: 0.169088 - valid_metric: 0.211463\n",
      "\n",
      " - 3300 round - train_metric: 0.168473 - valid_metric: 0.211450\n",
      "\n",
      " - 3350 round - train_metric: 0.167607 - valid_metric: 0.211435\n",
      "\n",
      " - 3400 round - train_metric: 0.166721 - valid_metric: 0.211405\n",
      "\n",
      " - 3450 round - train_metric: 0.165887 - valid_metric: 0.211329\n",
      "\n",
      " - 3500 round - train_metric: 0.165372 - valid_metric: 0.211308\n",
      "\n",
      " - 3550 round - train_metric: 0.164739 - valid_metric: 0.211260\n",
      "\n",
      " - 3600 round - train_metric: 0.164111 - valid_metric: 0.211256\n",
      "\n",
      " - 3650 round - train_metric: 0.163469 - valid_metric: 0.211225\n",
      "\n",
      " - 3700 round - train_metric: 0.162265 - valid_metric: 0.211139\n",
      "\n",
      " - 3750 round - train_metric: 0.161614 - valid_metric: 0.211118\n",
      "\n",
      " - 3800 round - train_metric: 0.161065 - valid_metric: 0.211071\n",
      "\n",
      " - 3850 round - train_metric: 0.160418 - valid_metric: 0.211067\n",
      "\n",
      " - 3900 round - train_metric: 0.160017 - valid_metric: 0.211043\n",
      "\n",
      " - 3950 round - train_metric: 0.159758 - valid_metric: 0.211062\n",
      "\n",
      " - 4000 round - train_metric: 0.158877 - valid_metric: 0.211029\n",
      "\n",
      " - 4050 round - train_metric: 0.158361 - valid_metric: 0.211046\n",
      "\n",
      " - 4100 round - train_metric: 0.157695 - valid_metric: 0.211038\n",
      "\n",
      " - 4150 round - train_metric: 0.156757 - valid_metric: 0.211007\n",
      "\n",
      " - 4200 round - train_metric: 0.156141 - valid_metric: 0.210959\n",
      "\n",
      " - 4250 round - train_metric: 0.155484 - valid_metric: 0.210895\n",
      "\n",
      " - 4300 round - train_metric: 0.154592 - valid_metric: 0.210915\n",
      "\n",
      " - 4350 round - train_metric: 0.154212 - valid_metric: 0.210921\n",
      "\n",
      " - 4400 round - train_metric: 0.153778 - valid_metric: 0.210937\n",
      "\n",
      " - 4450 round - train_metric: 0.153028 - valid_metric: 0.210926\n",
      "\n",
      "- fold0 valid metric: 0.804380\n",
      "\n",
      "\u001b[34m        ... fold : 1\u001b[39m\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.662132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 212191\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 6284\n",
      "[50]\ttraining's binary_logloss: 0.398866\tvalid_1's binary_logloss: 0.400066\n",
      "[100]\ttraining's binary_logloss: 0.33857\tvalid_1's binary_logloss: 0.340429\n",
      "[150]\ttraining's binary_logloss: 0.312984\tvalid_1's binary_logloss: 0.315387\n",
      "[200]\ttraining's binary_logloss: 0.299948\tvalid_1's binary_logloss: 0.302615\n",
      "[250]\ttraining's binary_logloss: 0.272171\tvalid_1's binary_logloss: 0.275728\n",
      "[300]\ttraining's binary_logloss: 0.2576\tvalid_1's binary_logloss: 0.261857\n",
      "[350]\ttraining's binary_logloss: 0.247936\tvalid_1's binary_logloss: 0.2529\n",
      "[400]\ttraining's binary_logloss: 0.243912\tvalid_1's binary_logloss: 0.249193\n",
      "[450]\ttraining's binary_logloss: 0.241916\tvalid_1's binary_logloss: 0.247478\n",
      "[500]\ttraining's binary_logloss: 0.231012\tvalid_1's binary_logloss: 0.237792\n",
      "[550]\ttraining's binary_logloss: 0.227705\tvalid_1's binary_logloss: 0.235113\n",
      "[600]\ttraining's binary_logloss: 0.225331\tvalid_1's binary_logloss: 0.233192\n",
      "[650]\ttraining's binary_logloss: 0.220066\tvalid_1's binary_logloss: 0.229154\n",
      "[700]\ttraining's binary_logloss: 0.217038\tvalid_1's binary_logloss: 0.227037\n",
      "[750]\ttraining's binary_logloss: 0.213827\tvalid_1's binary_logloss: 0.224959\n",
      "[800]\ttraining's binary_logloss: 0.211752\tvalid_1's binary_logloss: 0.22379\n",
      "[850]\ttraining's binary_logloss: 0.208955\tvalid_1's binary_logloss: 0.222279\n",
      "[900]\ttraining's binary_logloss: 0.207528\tvalid_1's binary_logloss: 0.221737\n",
      "[950]\ttraining's binary_logloss: 0.207304\tvalid_1's binary_logloss: 0.221791\n",
      "[1000]\ttraining's binary_logloss: 0.205491\tvalid_1's binary_logloss: 0.220972\n",
      "[1050]\ttraining's binary_logloss: 0.204366\tvalid_1's binary_logloss: 0.220574\n",
      "[1100]\ttraining's binary_logloss: 0.201966\tvalid_1's binary_logloss: 0.219614\n",
      "[1150]\ttraining's binary_logloss: 0.201733\tvalid_1's binary_logloss: 0.219693\n",
      "[1200]\ttraining's binary_logloss: 0.200572\tvalid_1's binary_logloss: 0.219317\n",
      "[1250]\ttraining's binary_logloss: 0.199451\tvalid_1's binary_logloss: 0.219033\n",
      "[1300]\ttraining's binary_logloss: 0.198784\tvalid_1's binary_logloss: 0.218893\n",
      "[1350]\ttraining's binary_logloss: 0.198183\tvalid_1's binary_logloss: 0.218783\n",
      "[1400]\ttraining's binary_logloss: 0.196484\tvalid_1's binary_logloss: 0.21826\n",
      "[1450]\ttraining's binary_logloss: 0.195836\tvalid_1's binary_logloss: 0.218155\n",
      "[1500]\ttraining's binary_logloss: 0.195207\tvalid_1's binary_logloss: 0.218023\n",
      "[1550]\ttraining's binary_logloss: 0.193431\tvalid_1's binary_logloss: 0.21754\n",
      "[1600]\ttraining's binary_logloss: 0.192102\tvalid_1's binary_logloss: 0.217235\n",
      "[1650]\ttraining's binary_logloss: 0.191015\tvalid_1's binary_logloss: 0.217013\n",
      "[1700]\ttraining's binary_logloss: 0.190363\tvalid_1's binary_logloss: 0.216952\n",
      "[1750]\ttraining's binary_logloss: 0.189689\tvalid_1's binary_logloss: 0.216822\n",
      "[1800]\ttraining's binary_logloss: 0.189258\tvalid_1's binary_logloss: 0.216824\n",
      "[1850]\ttraining's binary_logloss: 0.188379\tvalid_1's binary_logloss: 0.21667\n",
      "[1900]\ttraining's binary_logloss: 0.187505\tvalid_1's binary_logloss: 0.216542\n",
      "[1950]\ttraining's binary_logloss: 0.186286\tvalid_1's binary_logloss: 0.216336\n",
      "[2000]\ttraining's binary_logloss: 0.185423\tvalid_1's binary_logloss: 0.216216\n",
      "[2050]\ttraining's binary_logloss: 0.184773\tvalid_1's binary_logloss: 0.216156\n",
      "[2100]\ttraining's binary_logloss: 0.184071\tvalid_1's binary_logloss: 0.216092\n",
      "[2150]\ttraining's binary_logloss: 0.183885\tvalid_1's binary_logloss: 0.216159\n",
      "[2200]\ttraining's binary_logloss: 0.182884\tvalid_1's binary_logloss: 0.216034\n",
      "[2250]\ttraining's binary_logloss: 0.182093\tvalid_1's binary_logloss: 0.215966\n",
      "[2300]\ttraining's binary_logloss: 0.181101\tvalid_1's binary_logloss: 0.215876\n",
      "[2350]\ttraining's binary_logloss: 0.180481\tvalid_1's binary_logloss: 0.215817\n",
      "[2400]\ttraining's binary_logloss: 0.179932\tvalid_1's binary_logloss: 0.215769\n",
      "[2450]\ttraining's binary_logloss: 0.17913\tvalid_1's binary_logloss: 0.21572\n",
      "[2500]\ttraining's binary_logloss: 0.178667\tvalid_1's binary_logloss: 0.215687\n",
      "[2550]\ttraining's binary_logloss: 0.177945\tvalid_1's binary_logloss: 0.215658\n",
      "[2600]\ttraining's binary_logloss: 0.177251\tvalid_1's binary_logloss: 0.215573\n",
      "[2650]\ttraining's binary_logloss: 0.17651\tvalid_1's binary_logloss: 0.215554\n",
      "[2700]\ttraining's binary_logloss: 0.175638\tvalid_1's binary_logloss: 0.215511\n",
      "[2750]\ttraining's binary_logloss: 0.175257\tvalid_1's binary_logloss: 0.215499\n",
      "[2800]\ttraining's binary_logloss: 0.174655\tvalid_1's binary_logloss: 0.215462\n",
      "[2850]\ttraining's binary_logloss: 0.173544\tvalid_1's binary_logloss: 0.21539\n",
      "[2900]\ttraining's binary_logloss: 0.172977\tvalid_1's binary_logloss: 0.215363\n",
      "[2950]\ttraining's binary_logloss: 0.172047\tvalid_1's binary_logloss: 0.215346\n",
      "[3000]\ttraining's binary_logloss: 0.171289\tvalid_1's binary_logloss: 0.215357\n",
      "[3050]\ttraining's binary_logloss: 0.17061\tvalid_1's binary_logloss: 0.215325\n",
      "[3100]\ttraining's binary_logloss: 0.169935\tvalid_1's binary_logloss: 0.21527\n",
      "[3150]\ttraining's binary_logloss: 0.169364\tvalid_1's binary_logloss: 0.215248\n",
      "[3200]\ttraining's binary_logloss: 0.168518\tvalid_1's binary_logloss: 0.21521\n",
      "[3250]\ttraining's binary_logloss: 0.167796\tvalid_1's binary_logloss: 0.215208\n",
      "[3300]\ttraining's binary_logloss: 0.167161\tvalid_1's binary_logloss: 0.215198\n",
      "[3350]\ttraining's binary_logloss: 0.166288\tvalid_1's binary_logloss: 0.215151\n",
      "[3400]\ttraining's binary_logloss: 0.165388\tvalid_1's binary_logloss: 0.215142\n",
      "[3450]\ttraining's binary_logloss: 0.164616\tvalid_1's binary_logloss: 0.215136\n",
      "[3500]\ttraining's binary_logloss: 0.164089\tvalid_1's binary_logloss: 0.215128\n",
      "[3550]\ttraining's binary_logloss: 0.163418\tvalid_1's binary_logloss: 0.215107\n",
      "[3600]\ttraining's binary_logloss: 0.162781\tvalid_1's binary_logloss: 0.215135\n",
      "[3650]\ttraining's binary_logloss: 0.162106\tvalid_1's binary_logloss: 0.215152\n",
      "[3700]\ttraining's binary_logloss: 0.160926\tvalid_1's binary_logloss: 0.215124\n",
      "[3750]\ttraining's binary_logloss: 0.160274\tvalid_1's binary_logloss: 0.215068\n",
      "[3800]\ttraining's binary_logloss: 0.159721\tvalid_1's binary_logloss: 0.215046\n",
      "[3850]\ttraining's binary_logloss: 0.159075\tvalid_1's binary_logloss: 0.215051\n",
      "[3900]\ttraining's binary_logloss: 0.158623\tvalid_1's binary_logloss: 0.215029\n",
      "[3950]\ttraining's binary_logloss: 0.158415\tvalid_1's binary_logloss: 0.215045\n",
      "[4000]\ttraining's binary_logloss: 0.15754\tvalid_1's binary_logloss: 0.215027\n",
      "[4050]\ttraining's binary_logloss: 0.15699\tvalid_1's binary_logloss: 0.215068\n",
      "[4100]\ttraining's binary_logloss: 0.156286\tvalid_1's binary_logloss: 0.215023\n",
      "[4150]\ttraining's binary_logloss: 0.155338\tvalid_1's binary_logloss: 0.215031\n",
      "[4200]\ttraining's binary_logloss: 0.154728\tvalid_1's binary_logloss: 0.215033\n",
      "[4250]\ttraining's binary_logloss: 0.154056\tvalid_1's binary_logloss: 0.215066\n",
      "[4300]\ttraining's binary_logloss: 0.153144\tvalid_1's binary_logloss: 0.215044\n",
      "[4350]\ttraining's binary_logloss: 0.152803\tvalid_1's binary_logloss: 0.21506\n",
      "[4400]\ttraining's binary_logloss: 0.152367\tvalid_1's binary_logloss: 0.215057\n",
      "[4450]\ttraining's binary_logloss: 0.151577\tvalid_1's binary_logloss: 0.215037\n",
      "[4500]\ttraining's binary_logloss: 0.151112\tvalid_1's binary_logloss: 0.215064\n",
      " - 0 round - train_metric: 0.670828 - valid_metric: 0.670851\n",
      "\n",
      " - 50 round - train_metric: 0.401364 - valid_metric: 0.402547\n",
      "\n",
      " - 100 round - train_metric: 0.334529 - valid_metric: 0.336427\n",
      "\n",
      " - 150 round - train_metric: 0.314866 - valid_metric: 0.317239\n",
      "\n",
      " - 200 round - train_metric: 0.301175 - valid_metric: 0.303821\n",
      "\n",
      " - 250 round - train_metric: 0.270521 - valid_metric: 0.274159\n",
      "\n",
      " - 300 round - train_metric: 0.256438 - valid_metric: 0.260764\n",
      "\n",
      " - 350 round - train_metric: 0.248465 - valid_metric: 0.253397\n",
      "\n",
      " - 400 round - train_metric: 0.244316 - valid_metric: 0.249571\n",
      "\n",
      " - 450 round - train_metric: 0.242273 - valid_metric: 0.247810\n",
      "\n",
      " - 500 round - train_metric: 0.230542 - valid_metric: 0.237381\n",
      "\n",
      " - 550 round - train_metric: 0.227334 - valid_metric: 0.234786\n",
      "\n",
      " - 600 round - train_metric: 0.224990 - valid_metric: 0.232909\n",
      "\n",
      " - 650 round - train_metric: 0.220186 - valid_metric: 0.229253\n",
      "\n",
      " - 700 round - train_metric: 0.217153 - valid_metric: 0.227128\n",
      "\n",
      " - 750 round - train_metric: 0.213928 - valid_metric: 0.225032\n",
      "\n",
      " - 800 round - train_metric: 0.211822 - valid_metric: 0.223842\n",
      "\n",
      " - 850 round - train_metric: 0.208836 - valid_metric: 0.222210\n",
      "\n",
      " - 900 round - train_metric: 0.207407 - valid_metric: 0.221676\n",
      "\n",
      " - 950 round - train_metric: 0.207183 - valid_metric: 0.221721\n",
      "\n",
      " - 1000 round - train_metric: 0.205548 - valid_metric: 0.221007\n",
      "\n",
      " - 1050 round - train_metric: 0.204272 - valid_metric: 0.220549\n",
      "\n",
      " - 1100 round - train_metric: 0.201888 - valid_metric: 0.219586\n",
      "\n",
      " - 1150 round - train_metric: 0.201772 - valid_metric: 0.219716\n",
      "\n",
      " - 1200 round - train_metric: 0.200613 - valid_metric: 0.219339\n",
      "\n",
      " - 1250 round - train_metric: 0.199486 - valid_metric: 0.219051\n",
      "\n",
      " - 1300 round - train_metric: 0.198820 - valid_metric: 0.218911\n",
      "\n",
      " - 1350 round - train_metric: 0.198226 - valid_metric: 0.218803\n",
      "\n",
      " - 1400 round - train_metric: 0.196411 - valid_metric: 0.218244\n",
      "\n",
      " - 1450 round - train_metric: 0.195873 - valid_metric: 0.218170\n",
      "\n",
      " - 1500 round - train_metric: 0.195139 - valid_metric: 0.217988\n",
      "\n",
      " - 1550 round - train_metric: 0.193364 - valid_metric: 0.217519\n",
      "\n",
      " - 1600 round - train_metric: 0.192137 - valid_metric: 0.217247\n",
      "\n",
      " - 1650 round - train_metric: 0.190956 - valid_metric: 0.216996\n",
      "\n",
      " - 1700 round - train_metric: 0.190301 - valid_metric: 0.216937\n",
      "\n",
      " - 1750 round - train_metric: 0.189718 - valid_metric: 0.216833\n",
      "\n",
      " - 1800 round - train_metric: 0.189195 - valid_metric: 0.216806\n",
      "\n",
      " - 1850 round - train_metric: 0.188415 - valid_metric: 0.216682\n",
      "\n",
      " - 1900 round - train_metric: 0.187446 - valid_metric: 0.216525\n",
      "\n",
      " - 1950 round - train_metric: 0.186309 - valid_metric: 0.216343\n",
      "\n",
      " - 2000 round - train_metric: 0.185367 - valid_metric: 0.216217\n",
      "\n",
      " - 2050 round - train_metric: 0.184795 - valid_metric: 0.216162\n",
      "\n",
      " - 2100 round - train_metric: 0.184089 - valid_metric: 0.216097\n",
      "\n",
      " - 2150 round - train_metric: 0.183913 - valid_metric: 0.216166\n",
      "\n",
      " - 2200 round - train_metric: 0.182910 - valid_metric: 0.216041\n",
      "\n",
      " - 2250 round - train_metric: 0.182037 - valid_metric: 0.215948\n",
      "\n",
      " - 2300 round - train_metric: 0.181126 - valid_metric: 0.215882\n",
      "\n",
      " - 2350 round - train_metric: 0.180510 - valid_metric: 0.215823\n",
      "\n",
      " - 2400 round - train_metric: 0.179882 - valid_metric: 0.215749\n",
      "\n",
      " - 2450 round - train_metric: 0.179080 - valid_metric: 0.215723\n",
      "\n",
      " - 2500 round - train_metric: 0.178682 - valid_metric: 0.215690\n",
      "\n",
      " - 2550 round - train_metric: 0.177898 - valid_metric: 0.215647\n",
      "\n",
      " - 2600 round - train_metric: 0.177264 - valid_metric: 0.215576\n",
      "\n",
      " - 2650 round - train_metric: 0.176523 - valid_metric: 0.215557\n",
      "\n",
      " - 2700 round - train_metric: 0.175657 - valid_metric: 0.215515\n",
      "\n",
      " - 2750 round - train_metric: 0.175273 - valid_metric: 0.215503\n",
      "\n",
      " - 2800 round - train_metric: 0.174605 - valid_metric: 0.215462\n",
      "\n",
      " - 2850 round - train_metric: 0.173558 - valid_metric: 0.215393\n",
      "\n",
      " - 2900 round - train_metric: 0.172999 - valid_metric: 0.215366\n",
      "\n",
      " - 2950 round - train_metric: 0.172062 - valid_metric: 0.215348\n",
      "\n",
      " - 3000 round - train_metric: 0.171304 - valid_metric: 0.215359\n",
      "\n",
      " - 3050 round - train_metric: 0.170567 - valid_metric: 0.215323\n",
      "\n",
      " - 3100 round - train_metric: 0.169947 - valid_metric: 0.215272\n",
      "\n",
      " - 3150 round - train_metric: 0.169381 - valid_metric: 0.215251\n",
      "\n",
      " - 3200 round - train_metric: 0.168473 - valid_metric: 0.215206\n",
      "\n",
      " - 3250 round - train_metric: 0.167806 - valid_metric: 0.215210\n",
      "\n",
      " - 3300 round - train_metric: 0.167176 - valid_metric: 0.215199\n",
      "\n",
      " - 3350 round - train_metric: 0.166302 - valid_metric: 0.215152\n",
      "\n",
      " - 3400 round - train_metric: 0.165402 - valid_metric: 0.215142\n",
      "\n",
      " - 3450 round - train_metric: 0.164574 - valid_metric: 0.215127\n",
      "\n",
      " - 3500 round - train_metric: 0.164050 - valid_metric: 0.215132\n",
      "\n",
      " - 3550 round - train_metric: 0.163429 - valid_metric: 0.215108\n",
      "\n",
      " - 3600 round - train_metric: 0.162794 - valid_metric: 0.215136\n",
      "\n",
      " - 3650 round - train_metric: 0.162125 - valid_metric: 0.215154\n",
      "\n",
      " - 3700 round - train_metric: 0.160886 - valid_metric: 0.215121\n",
      "\n",
      " - 3750 round - train_metric: 0.160236 - valid_metric: 0.215072\n",
      "\n",
      " - 3800 round - train_metric: 0.159681 - valid_metric: 0.215047\n",
      "\n",
      " - 3850 round - train_metric: 0.159038 - valid_metric: 0.215047\n",
      "\n",
      " - 3900 round - train_metric: 0.158632 - valid_metric: 0.215030\n",
      "\n",
      " - 3950 round - train_metric: 0.158377 - valid_metric: 0.215046\n",
      "\n",
      " - 4000 round - train_metric: 0.157499 - valid_metric: 0.215031\n",
      "\n",
      " - 4050 round - train_metric: 0.157001 - valid_metric: 0.215070\n",
      "\n",
      " - 4100 round - train_metric: 0.156301 - valid_metric: 0.215024\n",
      "\n",
      " - 4150 round - train_metric: 0.155353 - valid_metric: 0.215031\n",
      "\n",
      " - 4200 round - train_metric: 0.154741 - valid_metric: 0.215034\n",
      "\n",
      " - 4250 round - train_metric: 0.154068 - valid_metric: 0.215066\n",
      "\n",
      " - 4300 round - train_metric: 0.153161 - valid_metric: 0.215044\n",
      "\n",
      " - 4350 round - train_metric: 0.152766 - valid_metric: 0.215062\n",
      "\n",
      " - 4400 round - train_metric: 0.152329 - valid_metric: 0.215062\n",
      "\n",
      " - 4450 round - train_metric: 0.151588 - valid_metric: 0.215037\n",
      "\n",
      "- fold1 valid metric: 0.795491\n",
      "\n",
      "\u001b[34m        ... fold : 2\u001b[39m\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 8.296191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 212219\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 6288\n",
      "[50]\ttraining's binary_logloss: 0.398895\tvalid_1's binary_logloss: 0.400688\n",
      "[100]\ttraining's binary_logloss: 0.33794\tvalid_1's binary_logloss: 0.340603\n",
      "[150]\ttraining's binary_logloss: 0.312228\tvalid_1's binary_logloss: 0.315417\n",
      "[200]\ttraining's binary_logloss: 0.299579\tvalid_1's binary_logloss: 0.303078\n",
      "[250]\ttraining's binary_logloss: 0.271945\tvalid_1's binary_logloss: 0.276254\n",
      "[300]\ttraining's binary_logloss: 0.25724\tvalid_1's binary_logloss: 0.262225\n",
      "[350]\ttraining's binary_logloss: 0.247479\tvalid_1's binary_logloss: 0.252987\n",
      "[400]\ttraining's binary_logloss: 0.24368\tvalid_1's binary_logloss: 0.249479\n",
      "[450]\ttraining's binary_logloss: 0.241859\tvalid_1's binary_logloss: 0.247947\n",
      "[500]\ttraining's binary_logloss: 0.230977\tvalid_1's binary_logloss: 0.238094\n",
      "[550]\ttraining's binary_logloss: 0.227611\tvalid_1's binary_logloss: 0.235266\n",
      "[600]\ttraining's binary_logloss: 0.225386\tvalid_1's binary_logloss: 0.233446\n",
      "[650]\ttraining's binary_logloss: 0.220086\tvalid_1's binary_logloss: 0.22922\n",
      "[700]\ttraining's binary_logloss: 0.217152\tvalid_1's binary_logloss: 0.227112\n",
      "[750]\ttraining's binary_logloss: 0.213949\tvalid_1's binary_logloss: 0.224884\n",
      "[800]\ttraining's binary_logloss: 0.211955\tvalid_1's binary_logloss: 0.223628\n",
      "[850]\ttraining's binary_logloss: 0.209209\tvalid_1's binary_logloss: 0.222038\n",
      "[900]\ttraining's binary_logloss: 0.207775\tvalid_1's binary_logloss: 0.221335\n",
      "[950]\ttraining's binary_logloss: 0.207578\tvalid_1's binary_logloss: 0.221407\n",
      "[1000]\ttraining's binary_logloss: 0.205775\tvalid_1's binary_logloss: 0.220527\n",
      "[1050]\ttraining's binary_logloss: 0.204675\tvalid_1's binary_logloss: 0.220105\n",
      "[1100]\ttraining's binary_logloss: 0.202275\tvalid_1's binary_logloss: 0.219048\n",
      "[1150]\ttraining's binary_logloss: 0.202023\tvalid_1's binary_logloss: 0.219111\n",
      "[1200]\ttraining's binary_logloss: 0.200861\tvalid_1's binary_logloss: 0.218684\n",
      "[1250]\ttraining's binary_logloss: 0.199756\tvalid_1's binary_logloss: 0.218249\n",
      "[1300]\ttraining's binary_logloss: 0.199076\tvalid_1's binary_logloss: 0.218098\n",
      "[1350]\ttraining's binary_logloss: 0.198498\tvalid_1's binary_logloss: 0.218019\n",
      "[1400]\ttraining's binary_logloss: 0.196809\tvalid_1's binary_logloss: 0.217472\n",
      "[1450]\ttraining's binary_logloss: 0.196176\tvalid_1's binary_logloss: 0.217351\n",
      "[1500]\ttraining's binary_logloss: 0.195526\tvalid_1's binary_logloss: 0.21727\n",
      "[1550]\ttraining's binary_logloss: 0.193741\tvalid_1's binary_logloss: 0.21667\n",
      "[1600]\ttraining's binary_logloss: 0.192403\tvalid_1's binary_logloss: 0.216407\n",
      "[1650]\ttraining's binary_logloss: 0.191309\tvalid_1's binary_logloss: 0.216167\n",
      "[1700]\ttraining's binary_logloss: 0.190654\tvalid_1's binary_logloss: 0.216075\n",
      "[1750]\ttraining's binary_logloss: 0.189978\tvalid_1's binary_logloss: 0.216045\n",
      "[1800]\ttraining's binary_logloss: 0.189578\tvalid_1's binary_logloss: 0.216013\n",
      "[1850]\ttraining's binary_logloss: 0.188694\tvalid_1's binary_logloss: 0.215813\n",
      "[1900]\ttraining's binary_logloss: 0.187795\tvalid_1's binary_logloss: 0.21566\n",
      "[1950]\ttraining's binary_logloss: 0.1866\tvalid_1's binary_logloss: 0.215453\n",
      "[2000]\ttraining's binary_logloss: 0.18573\tvalid_1's binary_logloss: 0.215321\n",
      "[2050]\ttraining's binary_logloss: 0.185072\tvalid_1's binary_logloss: 0.215286\n",
      "[2100]\ttraining's binary_logloss: 0.184372\tvalid_1's binary_logloss: 0.215198\n",
      "[2150]\ttraining's binary_logloss: 0.184177\tvalid_1's binary_logloss: 0.215268\n",
      "[2200]\ttraining's binary_logloss: 0.183193\tvalid_1's binary_logloss: 0.215089\n",
      "[2250]\ttraining's binary_logloss: 0.182412\tvalid_1's binary_logloss: 0.214997\n",
      "[2300]\ttraining's binary_logloss: 0.181441\tvalid_1's binary_logloss: 0.214874\n",
      "[2350]\ttraining's binary_logloss: 0.180852\tvalid_1's binary_logloss: 0.214834\n",
      "[2400]\ttraining's binary_logloss: 0.180293\tvalid_1's binary_logloss: 0.214823\n",
      "[2450]\ttraining's binary_logloss: 0.179494\tvalid_1's binary_logloss: 0.214738\n",
      "[2500]\ttraining's binary_logloss: 0.179039\tvalid_1's binary_logloss: 0.21467\n",
      "[2550]\ttraining's binary_logloss: 0.17832\tvalid_1's binary_logloss: 0.214607\n",
      "[2600]\ttraining's binary_logloss: 0.177628\tvalid_1's binary_logloss: 0.214535\n",
      "[2650]\ttraining's binary_logloss: 0.176872\tvalid_1's binary_logloss: 0.21445\n",
      "[2700]\ttraining's binary_logloss: 0.176011\tvalid_1's binary_logloss: 0.214374\n",
      "[2750]\ttraining's binary_logloss: 0.175624\tvalid_1's binary_logloss: 0.214406\n",
      "[2800]\ttraining's binary_logloss: 0.175065\tvalid_1's binary_logloss: 0.214353\n",
      "[2850]\ttraining's binary_logloss: 0.173936\tvalid_1's binary_logloss: 0.214255\n",
      "[2900]\ttraining's binary_logloss: 0.173371\tvalid_1's binary_logloss: 0.214243\n",
      "[2950]\ttraining's binary_logloss: 0.172433\tvalid_1's binary_logloss: 0.214191\n",
      "[3000]\ttraining's binary_logloss: 0.171659\tvalid_1's binary_logloss: 0.214129\n",
      "[3050]\ttraining's binary_logloss: 0.170984\tvalid_1's binary_logloss: 0.214081\n",
      "[3100]\ttraining's binary_logloss: 0.170319\tvalid_1's binary_logloss: 0.214038\n",
      "[3150]\ttraining's binary_logloss: 0.169757\tvalid_1's binary_logloss: 0.213992\n",
      "[3200]\ttraining's binary_logloss: 0.168892\tvalid_1's binary_logloss: 0.213959\n",
      "[3250]\ttraining's binary_logloss: 0.168177\tvalid_1's binary_logloss: 0.213931\n",
      "[3300]\ttraining's binary_logloss: 0.167553\tvalid_1's binary_logloss: 0.213858\n",
      "[3350]\ttraining's binary_logloss: 0.166677\tvalid_1's binary_logloss: 0.213832\n",
      "[3400]\ttraining's binary_logloss: 0.165788\tvalid_1's binary_logloss: 0.213764\n",
      "[3450]\ttraining's binary_logloss: 0.164982\tvalid_1's binary_logloss: 0.213751\n",
      "[3500]\ttraining's binary_logloss: 0.164446\tvalid_1's binary_logloss: 0.213743\n",
      "[3550]\ttraining's binary_logloss: 0.163753\tvalid_1's binary_logloss: 0.213713\n",
      "[3600]\ttraining's binary_logloss: 0.163125\tvalid_1's binary_logloss: 0.213679\n",
      "[3650]\ttraining's binary_logloss: 0.162471\tvalid_1's binary_logloss: 0.213692\n",
      "[3700]\ttraining's binary_logloss: 0.161319\tvalid_1's binary_logloss: 0.213613\n",
      "[3750]\ttraining's binary_logloss: 0.160646\tvalid_1's binary_logloss: 0.213583\n",
      "[3800]\ttraining's binary_logloss: 0.160105\tvalid_1's binary_logloss: 0.213579\n",
      "[3850]\ttraining's binary_logloss: 0.159461\tvalid_1's binary_logloss: 0.213526\n",
      "[3900]\ttraining's binary_logloss: 0.15899\tvalid_1's binary_logloss: 0.213528\n",
      "[3950]\ttraining's binary_logloss: 0.158779\tvalid_1's binary_logloss: 0.213528\n",
      "[4000]\ttraining's binary_logloss: 0.157907\tvalid_1's binary_logloss: 0.213542\n",
      "[4050]\ttraining's binary_logloss: 0.157352\tvalid_1's binary_logloss: 0.213547\n",
      "[4100]\ttraining's binary_logloss: 0.15665\tvalid_1's binary_logloss: 0.213525\n",
      "[4150]\ttraining's binary_logloss: 0.155712\tvalid_1's binary_logloss: 0.213444\n",
      "[4200]\ttraining's binary_logloss: 0.15511\tvalid_1's binary_logloss: 0.213429\n",
      "[4250]\ttraining's binary_logloss: 0.154446\tvalid_1's binary_logloss: 0.213415\n",
      "[4300]\ttraining's binary_logloss: 0.153552\tvalid_1's binary_logloss: 0.213412\n",
      "[4350]\ttraining's binary_logloss: 0.153222\tvalid_1's binary_logloss: 0.213433\n",
      "[4400]\ttraining's binary_logloss: 0.15278\tvalid_1's binary_logloss: 0.213446\n",
      "[4450]\ttraining's binary_logloss: 0.152\tvalid_1's binary_logloss: 0.213466\n",
      "[4500]\ttraining's binary_logloss: 0.151535\tvalid_1's binary_logloss: 0.213477\n",
      " - 0 round - train_metric: 0.671096 - valid_metric: 0.671203\n",
      "\n",
      " - 50 round - train_metric: 0.401376 - valid_metric: 0.403154\n",
      "\n",
      " - 100 round - train_metric: 0.333802 - valid_metric: 0.336511\n",
      "\n",
      " - 150 round - train_metric: 0.314105 - valid_metric: 0.317256\n",
      "\n",
      " - 200 round - train_metric: 0.300805 - valid_metric: 0.304279\n",
      "\n",
      " - 250 round - train_metric: 0.270366 - valid_metric: 0.274745\n",
      "\n",
      " - 300 round - train_metric: 0.256052 - valid_metric: 0.261083\n",
      "\n",
      " - 350 round - train_metric: 0.248004 - valid_metric: 0.253482\n",
      "\n",
      " - 400 round - train_metric: 0.244081 - valid_metric: 0.249857\n",
      "\n",
      " - 450 round - train_metric: 0.242216 - valid_metric: 0.248282\n",
      "\n",
      " - 500 round - train_metric: 0.230490 - valid_metric: 0.237665\n",
      "\n",
      " - 550 round - train_metric: 0.227253 - valid_metric: 0.234956\n",
      "\n",
      " - 600 round - train_metric: 0.225044 - valid_metric: 0.233153\n",
      "\n",
      " - 650 round - train_metric: 0.220204 - valid_metric: 0.229319\n",
      "\n",
      " - 700 round - train_metric: 0.217267 - valid_metric: 0.227203\n",
      "\n",
      " - 750 round - train_metric: 0.214050 - valid_metric: 0.224957\n",
      "\n",
      " - 800 round - train_metric: 0.212025 - valid_metric: 0.223680\n",
      "\n",
      " - 850 round - train_metric: 0.209091 - valid_metric: 0.221976\n",
      "\n",
      " - 900 round - train_metric: 0.207661 - valid_metric: 0.221277\n",
      "\n",
      " - 950 round - train_metric: 0.207465 - valid_metric: 0.221342\n",
      "\n",
      " - 1000 round - train_metric: 0.205832 - valid_metric: 0.220564\n",
      "\n",
      " - 1050 round - train_metric: 0.204574 - valid_metric: 0.220059\n",
      "\n",
      " - 1100 round - train_metric: 0.202186 - valid_metric: 0.219009\n",
      "\n",
      " - 1150 round - train_metric: 0.202063 - valid_metric: 0.219135\n",
      "\n",
      " - 1200 round - train_metric: 0.200903 - valid_metric: 0.218708\n",
      "\n",
      " - 1250 round - train_metric: 0.199791 - valid_metric: 0.218269\n",
      "\n",
      " - 1300 round - train_metric: 0.199111 - valid_metric: 0.218118\n",
      "\n",
      " - 1350 round - train_metric: 0.198539 - valid_metric: 0.218041\n",
      "\n",
      " - 1400 round - train_metric: 0.196735 - valid_metric: 0.217445\n",
      "\n",
      " - 1450 round - train_metric: 0.196213 - valid_metric: 0.217369\n",
      "\n",
      " - 1500 round - train_metric: 0.195455 - valid_metric: 0.217245\n",
      "\n",
      " - 1550 round - train_metric: 0.193675 - valid_metric: 0.216643\n",
      "\n",
      " - 1600 round - train_metric: 0.192438 - valid_metric: 0.216420\n",
      "\n",
      " - 1650 round - train_metric: 0.191247 - valid_metric: 0.216161\n",
      "\n",
      " - 1700 round - train_metric: 0.190589 - valid_metric: 0.216058\n",
      "\n",
      " - 1750 round - train_metric: 0.190006 - valid_metric: 0.216056\n",
      "\n",
      " - 1800 round - train_metric: 0.189513 - valid_metric: 0.215988\n",
      "\n",
      " - 1850 round - train_metric: 0.188730 - valid_metric: 0.215827\n",
      "\n",
      " - 1900 round - train_metric: 0.187739 - valid_metric: 0.215647\n",
      "\n",
      " - 1950 round - train_metric: 0.186623 - valid_metric: 0.215461\n",
      "\n",
      " - 2000 round - train_metric: 0.185675 - valid_metric: 0.215307\n",
      "\n",
      " - 2050 round - train_metric: 0.185094 - valid_metric: 0.215294\n",
      "\n",
      " - 2100 round - train_metric: 0.184390 - valid_metric: 0.215204\n",
      "\n",
      " - 2150 round - train_metric: 0.184205 - valid_metric: 0.215278\n",
      "\n",
      " - 2200 round - train_metric: 0.183218 - valid_metric: 0.215097\n",
      "\n",
      " - 2250 round - train_metric: 0.182357 - valid_metric: 0.214983\n",
      "\n",
      " - 2300 round - train_metric: 0.181466 - valid_metric: 0.214881\n",
      "\n",
      " - 2350 round - train_metric: 0.180880 - valid_metric: 0.214841\n",
      "\n",
      " - 2400 round - train_metric: 0.180241 - valid_metric: 0.214812\n",
      "\n",
      " - 2450 round - train_metric: 0.179438 - valid_metric: 0.214727\n",
      "\n",
      " - 2500 round - train_metric: 0.179054 - valid_metric: 0.214675\n",
      "\n",
      " - 2550 round - train_metric: 0.178269 - valid_metric: 0.214603\n",
      "\n",
      " - 2600 round - train_metric: 0.177640 - valid_metric: 0.214539\n",
      "\n",
      " - 2650 round - train_metric: 0.176885 - valid_metric: 0.214454\n",
      "\n",
      " - 2700 round - train_metric: 0.176030 - valid_metric: 0.214379\n",
      "\n",
      " - 2750 round - train_metric: 0.175639 - valid_metric: 0.214410\n",
      "\n",
      " - 2800 round - train_metric: 0.175013 - valid_metric: 0.214343\n",
      "\n",
      " - 2850 round - train_metric: 0.173951 - valid_metric: 0.214258\n",
      "\n",
      " - 2900 round - train_metric: 0.173392 - valid_metric: 0.214248\n",
      "\n",
      " - 2950 round - train_metric: 0.172448 - valid_metric: 0.214194\n",
      "\n",
      " - 3000 round - train_metric: 0.171674 - valid_metric: 0.214132\n",
      "\n",
      " - 3050 round - train_metric: 0.170942 - valid_metric: 0.214077\n",
      "\n",
      " - 3100 round - train_metric: 0.170331 - valid_metric: 0.214040\n",
      "\n",
      " - 3150 round - train_metric: 0.169773 - valid_metric: 0.213995\n",
      "\n",
      " - 3200 round - train_metric: 0.168849 - valid_metric: 0.213961\n",
      "\n",
      " - 3250 round - train_metric: 0.168187 - valid_metric: 0.213933\n",
      "\n",
      " - 3300 round - train_metric: 0.167568 - valid_metric: 0.213860\n",
      "\n",
      " - 3350 round - train_metric: 0.166692 - valid_metric: 0.213834\n",
      "\n",
      " - 3400 round - train_metric: 0.165802 - valid_metric: 0.213766\n",
      "\n",
      " - 3450 round - train_metric: 0.164939 - valid_metric: 0.213749\n",
      "\n",
      " - 3500 round - train_metric: 0.164401 - valid_metric: 0.213734\n",
      "\n",
      " - 3550 round - train_metric: 0.163764 - valid_metric: 0.213714\n",
      "\n",
      " - 3600 round - train_metric: 0.163138 - valid_metric: 0.213681\n",
      "\n",
      " - 3650 round - train_metric: 0.162490 - valid_metric: 0.213694\n",
      "\n",
      " - 3700 round - train_metric: 0.161279 - valid_metric: 0.213617\n",
      "\n",
      " - 3750 round - train_metric: 0.160605 - valid_metric: 0.213581\n",
      "\n",
      " - 3800 round - train_metric: 0.160066 - valid_metric: 0.213566\n",
      "\n",
      " - 3850 round - train_metric: 0.159420 - valid_metric: 0.213526\n",
      "\n",
      " - 3900 round - train_metric: 0.158998 - valid_metric: 0.213529\n",
      "\n",
      " - 3950 round - train_metric: 0.158741 - valid_metric: 0.213525\n",
      "\n",
      " - 4000 round - train_metric: 0.157867 - valid_metric: 0.213540\n",
      "\n",
      " - 4050 round - train_metric: 0.157364 - valid_metric: 0.213547\n",
      "\n",
      " - 4100 round - train_metric: 0.156665 - valid_metric: 0.213526\n",
      "\n",
      " - 4150 round - train_metric: 0.155726 - valid_metric: 0.213445\n",
      "\n",
      " - 4200 round - train_metric: 0.155123 - valid_metric: 0.213430\n",
      "\n",
      " - 4250 round - train_metric: 0.154458 - valid_metric: 0.213416\n",
      "\n",
      " - 4300 round - train_metric: 0.153569 - valid_metric: 0.213413\n",
      "\n",
      " - 4350 round - train_metric: 0.153187 - valid_metric: 0.213433\n",
      "\n",
      " - 4400 round - train_metric: 0.152741 - valid_metric: 0.213456\n",
      "\n",
      " - 4450 round - train_metric: 0.152011 - valid_metric: 0.213466\n",
      "\n",
      "- fold2 valid metric: 0.798855\n",
      "\n",
      "\u001b[34m        ... fold : 3\u001b[39m\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 6.042156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 212069\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 6287\n",
      "[50]\ttraining's binary_logloss: 0.399332\tvalid_1's binary_logloss: 0.402036\n",
      "[100]\ttraining's binary_logloss: 0.337999\tvalid_1's binary_logloss: 0.341728\n",
      "[150]\ttraining's binary_logloss: 0.312045\tvalid_1's binary_logloss: 0.316283\n",
      "[200]\ttraining's binary_logloss: 0.299372\tvalid_1's binary_logloss: 0.304009\n",
      "[250]\ttraining's binary_logloss: 0.271725\tvalid_1's binary_logloss: 0.277103\n",
      "[300]\ttraining's binary_logloss: 0.257285\tvalid_1's binary_logloss: 0.263415\n",
      "[350]\ttraining's binary_logloss: 0.247457\tvalid_1's binary_logloss: 0.254256\n",
      "[400]\ttraining's binary_logloss: 0.243733\tvalid_1's binary_logloss: 0.250822\n",
      "[450]\ttraining's binary_logloss: 0.241858\tvalid_1's binary_logloss: 0.249173\n",
      "[500]\ttraining's binary_logloss: 0.230849\tvalid_1's binary_logloss: 0.239313\n",
      "[550]\ttraining's binary_logloss: 0.227481\tvalid_1's binary_logloss: 0.236495\n",
      "[600]\ttraining's binary_logloss: 0.225098\tvalid_1's binary_logloss: 0.234631\n",
      "[650]\ttraining's binary_logloss: 0.21986\tvalid_1's binary_logloss: 0.230513\n",
      "[700]\ttraining's binary_logloss: 0.216876\tvalid_1's binary_logloss: 0.22836\n",
      "[750]\ttraining's binary_logloss: 0.213682\tvalid_1's binary_logloss: 0.226266\n",
      "[800]\ttraining's binary_logloss: 0.211649\tvalid_1's binary_logloss: 0.225052\n",
      "[850]\ttraining's binary_logloss: 0.208896\tvalid_1's binary_logloss: 0.223569\n",
      "[900]\ttraining's binary_logloss: 0.207508\tvalid_1's binary_logloss: 0.222967\n",
      "[950]\ttraining's binary_logloss: 0.207328\tvalid_1's binary_logloss: 0.22295\n",
      "[1000]\ttraining's binary_logloss: 0.205554\tvalid_1's binary_logloss: 0.222044\n",
      "[1050]\ttraining's binary_logloss: 0.204456\tvalid_1's binary_logloss: 0.221627\n",
      "[1100]\ttraining's binary_logloss: 0.20206\tvalid_1's binary_logloss: 0.220593\n",
      "[1150]\ttraining's binary_logloss: 0.201842\tvalid_1's binary_logloss: 0.220641\n",
      "[1200]\ttraining's binary_logloss: 0.200715\tvalid_1's binary_logloss: 0.220244\n",
      "[1250]\ttraining's binary_logloss: 0.199626\tvalid_1's binary_logloss: 0.219871\n",
      "[1300]\ttraining's binary_logloss: 0.198941\tvalid_1's binary_logloss: 0.219712\n",
      "[1350]\ttraining's binary_logloss: 0.198355\tvalid_1's binary_logloss: 0.219625\n",
      "[1400]\ttraining's binary_logloss: 0.196648\tvalid_1's binary_logloss: 0.218968\n",
      "[1450]\ttraining's binary_logloss: 0.196019\tvalid_1's binary_logloss: 0.218865\n",
      "[1500]\ttraining's binary_logloss: 0.195376\tvalid_1's binary_logloss: 0.218772\n",
      "[1550]\ttraining's binary_logloss: 0.193613\tvalid_1's binary_logloss: 0.218263\n",
      "[1600]\ttraining's binary_logloss: 0.192283\tvalid_1's binary_logloss: 0.217968\n",
      "[1650]\ttraining's binary_logloss: 0.191165\tvalid_1's binary_logloss: 0.217787\n",
      "[1700]\ttraining's binary_logloss: 0.190499\tvalid_1's binary_logloss: 0.217699\n",
      "[1750]\ttraining's binary_logloss: 0.189811\tvalid_1's binary_logloss: 0.217649\n",
      "[1800]\ttraining's binary_logloss: 0.18941\tvalid_1's binary_logloss: 0.217637\n",
      "[1850]\ttraining's binary_logloss: 0.188537\tvalid_1's binary_logloss: 0.217504\n",
      "[1900]\ttraining's binary_logloss: 0.18762\tvalid_1's binary_logloss: 0.217369\n",
      "[1950]\ttraining's binary_logloss: 0.186457\tvalid_1's binary_logloss: 0.217181\n",
      "[2000]\ttraining's binary_logloss: 0.185579\tvalid_1's binary_logloss: 0.21703\n",
      "[2050]\ttraining's binary_logloss: 0.184913\tvalid_1's binary_logloss: 0.216986\n",
      "[2100]\ttraining's binary_logloss: 0.184207\tvalid_1's binary_logloss: 0.216915\n",
      "[2150]\ttraining's binary_logloss: 0.184023\tvalid_1's binary_logloss: 0.216974\n",
      "[2200]\ttraining's binary_logloss: 0.183021\tvalid_1's binary_logloss: 0.216821\n",
      "[2250]\ttraining's binary_logloss: 0.18224\tvalid_1's binary_logloss: 0.216669\n",
      "[2300]\ttraining's binary_logloss: 0.181229\tvalid_1's binary_logloss: 0.216516\n",
      "[2350]\ttraining's binary_logloss: 0.180612\tvalid_1's binary_logloss: 0.216477\n",
      "[2400]\ttraining's binary_logloss: 0.180049\tvalid_1's binary_logloss: 0.216451\n",
      "[2450]\ttraining's binary_logloss: 0.179232\tvalid_1's binary_logloss: 0.216345\n",
      "[2500]\ttraining's binary_logloss: 0.178774\tvalid_1's binary_logloss: 0.216332\n",
      "[2550]\ttraining's binary_logloss: 0.178066\tvalid_1's binary_logloss: 0.216281\n",
      "[2600]\ttraining's binary_logloss: 0.177377\tvalid_1's binary_logloss: 0.216226\n",
      "[2650]\ttraining's binary_logloss: 0.176629\tvalid_1's binary_logloss: 0.216165\n",
      "[2700]\ttraining's binary_logloss: 0.175773\tvalid_1's binary_logloss: 0.216075\n",
      "[2750]\ttraining's binary_logloss: 0.175389\tvalid_1's binary_logloss: 0.216076\n",
      "[2800]\ttraining's binary_logloss: 0.174838\tvalid_1's binary_logloss: 0.216059\n",
      "[2850]\ttraining's binary_logloss: 0.173711\tvalid_1's binary_logloss: 0.215909\n",
      "[2900]\ttraining's binary_logloss: 0.173131\tvalid_1's binary_logloss: 0.215854\n",
      "[2950]\ttraining's binary_logloss: 0.172191\tvalid_1's binary_logloss: 0.215831\n",
      "[3000]\ttraining's binary_logloss: 0.171411\tvalid_1's binary_logloss: 0.21577\n",
      "[3050]\ttraining's binary_logloss: 0.170727\tvalid_1's binary_logloss: 0.21572\n",
      "[3100]\ttraining's binary_logloss: 0.170052\tvalid_1's binary_logloss: 0.215702\n",
      "[3150]\ttraining's binary_logloss: 0.169464\tvalid_1's binary_logloss: 0.215673\n",
      "[3200]\ttraining's binary_logloss: 0.168599\tvalid_1's binary_logloss: 0.215616\n",
      "[3250]\ttraining's binary_logloss: 0.167885\tvalid_1's binary_logloss: 0.215616\n",
      "[3300]\ttraining's binary_logloss: 0.167269\tvalid_1's binary_logloss: 0.215561\n",
      "[3350]\ttraining's binary_logloss: 0.166401\tvalid_1's binary_logloss: 0.215516\n",
      "[3400]\ttraining's binary_logloss: 0.165508\tvalid_1's binary_logloss: 0.215448\n",
      "[3450]\ttraining's binary_logloss: 0.164716\tvalid_1's binary_logloss: 0.215405\n",
      "[3500]\ttraining's binary_logloss: 0.164186\tvalid_1's binary_logloss: 0.215404\n",
      "[3550]\ttraining's binary_logloss: 0.163508\tvalid_1's binary_logloss: 0.215359\n",
      "[3600]\ttraining's binary_logloss: 0.16287\tvalid_1's binary_logloss: 0.215333\n",
      "[3650]\ttraining's binary_logloss: 0.162232\tvalid_1's binary_logloss: 0.215338\n",
      "[3700]\ttraining's binary_logloss: 0.161088\tvalid_1's binary_logloss: 0.215288\n",
      "[3750]\ttraining's binary_logloss: 0.160429\tvalid_1's binary_logloss: 0.215265\n",
      "[3800]\ttraining's binary_logloss: 0.159897\tvalid_1's binary_logloss: 0.215243\n",
      "[3850]\ttraining's binary_logloss: 0.159258\tvalid_1's binary_logloss: 0.215254\n",
      "[3900]\ttraining's binary_logloss: 0.15879\tvalid_1's binary_logloss: 0.215268\n",
      "[3950]\ttraining's binary_logloss: 0.158592\tvalid_1's binary_logloss: 0.215264\n",
      "[4000]\ttraining's binary_logloss: 0.157725\tvalid_1's binary_logloss: 0.215274\n",
      "[4050]\ttraining's binary_logloss: 0.157179\tvalid_1's binary_logloss: 0.215254\n",
      "[4100]\ttraining's binary_logloss: 0.156466\tvalid_1's binary_logloss: 0.215184\n",
      "[4150]\ttraining's binary_logloss: 0.155524\tvalid_1's binary_logloss: 0.215169\n",
      "[4200]\ttraining's binary_logloss: 0.154899\tvalid_1's binary_logloss: 0.215181\n",
      "[4250]\ttraining's binary_logloss: 0.154227\tvalid_1's binary_logloss: 0.215132\n",
      "[4300]\ttraining's binary_logloss: 0.153308\tvalid_1's binary_logloss: 0.215133\n",
      "[4350]\ttraining's binary_logloss: 0.152971\tvalid_1's binary_logloss: 0.215131\n",
      "[4400]\ttraining's binary_logloss: 0.152532\tvalid_1's binary_logloss: 0.215131\n",
      "[4450]\ttraining's binary_logloss: 0.151731\tvalid_1's binary_logloss: 0.215115\n",
      "[4500]\ttraining's binary_logloss: 0.151269\tvalid_1's binary_logloss: 0.21507\n",
      " - 0 round - train_metric: 0.671582 - valid_metric: 0.671733\n",
      "\n",
      " - 50 round - train_metric: 0.401838 - valid_metric: 0.404516\n",
      "\n",
      " - 100 round - train_metric: 0.333904 - valid_metric: 0.337702\n",
      "\n",
      " - 150 round - train_metric: 0.313913 - valid_metric: 0.318116\n",
      "\n",
      " - 200 round - train_metric: 0.300595 - valid_metric: 0.305203\n",
      "\n",
      " - 250 round - train_metric: 0.270101 - valid_metric: 0.275547\n",
      "\n",
      " - 300 round - train_metric: 0.256099 - valid_metric: 0.262272\n",
      "\n",
      " - 350 round - train_metric: 0.247984 - valid_metric: 0.254754\n",
      "\n",
      " - 400 round - train_metric: 0.244135 - valid_metric: 0.251200\n",
      "\n",
      " - 450 round - train_metric: 0.242214 - valid_metric: 0.249504\n",
      "\n",
      " - 500 round - train_metric: 0.230411 - valid_metric: 0.238944\n",
      "\n",
      " - 550 round - train_metric: 0.227124 - valid_metric: 0.236184\n",
      "\n",
      " - 600 round - train_metric: 0.224774 - valid_metric: 0.234343\n",
      "\n",
      " - 650 round - train_metric: 0.219976 - valid_metric: 0.230610\n",
      "\n",
      " - 700 round - train_metric: 0.216990 - valid_metric: 0.228450\n",
      "\n",
      " - 750 round - train_metric: 0.213780 - valid_metric: 0.226337\n",
      "\n",
      " - 800 round - train_metric: 0.211719 - valid_metric: 0.225103\n",
      "\n",
      " - 850 round - train_metric: 0.208785 - valid_metric: 0.223505\n",
      "\n",
      " - 900 round - train_metric: 0.207393 - valid_metric: 0.222907\n",
      "\n",
      " - 950 round - train_metric: 0.207205 - valid_metric: 0.222870\n",
      "\n",
      " - 1000 round - train_metric: 0.205610 - valid_metric: 0.222080\n",
      "\n",
      " - 1050 round - train_metric: 0.204363 - valid_metric: 0.221591\n",
      "\n",
      " - 1100 round - train_metric: 0.201974 - valid_metric: 0.220560\n",
      "\n",
      " - 1150 round - train_metric: 0.201883 - valid_metric: 0.220665\n",
      "\n",
      " - 1200 round - train_metric: 0.200757 - valid_metric: 0.220267\n",
      "\n",
      " - 1250 round - train_metric: 0.199661 - valid_metric: 0.219891\n",
      "\n",
      " - 1300 round - train_metric: 0.198977 - valid_metric: 0.219731\n",
      "\n",
      " - 1350 round - train_metric: 0.198397 - valid_metric: 0.219646\n",
      "\n",
      " - 1400 round - train_metric: 0.196574 - valid_metric: 0.218945\n",
      "\n",
      " - 1450 round - train_metric: 0.196056 - valid_metric: 0.218882\n",
      "\n",
      " - 1500 round - train_metric: 0.195307 - valid_metric: 0.218749\n",
      "\n",
      " - 1550 round - train_metric: 0.193546 - valid_metric: 0.218245\n",
      "\n",
      " - 1600 round - train_metric: 0.192318 - valid_metric: 0.217981\n",
      "\n",
      " - 1650 round - train_metric: 0.191103 - valid_metric: 0.217771\n",
      "\n",
      " - 1700 round - train_metric: 0.190438 - valid_metric: 0.217684\n",
      "\n",
      " - 1750 round - train_metric: 0.189839 - valid_metric: 0.217660\n",
      "\n",
      " - 1800 round - train_metric: 0.189350 - valid_metric: 0.217627\n",
      "\n",
      " - 1850 round - train_metric: 0.188574 - valid_metric: 0.217517\n",
      "\n",
      " - 1900 round - train_metric: 0.187563 - valid_metric: 0.217354\n",
      "\n",
      " - 1950 round - train_metric: 0.186480 - valid_metric: 0.217188\n",
      "\n",
      " - 2000 round - train_metric: 0.185524 - valid_metric: 0.217027\n",
      "\n",
      " - 2050 round - train_metric: 0.184936 - valid_metric: 0.216993\n",
      "\n",
      " - 2100 round - train_metric: 0.184225 - valid_metric: 0.216921\n",
      "\n",
      " - 2150 round - train_metric: 0.184051 - valid_metric: 0.216983\n",
      "\n",
      " - 2200 round - train_metric: 0.183047 - valid_metric: 0.216828\n",
      "\n",
      " - 2250 round - train_metric: 0.182188 - valid_metric: 0.216658\n",
      "\n",
      " - 2300 round - train_metric: 0.181254 - valid_metric: 0.216522\n",
      "\n",
      " - 2350 round - train_metric: 0.180640 - valid_metric: 0.216484\n",
      "\n",
      " - 2400 round - train_metric: 0.179996 - valid_metric: 0.216441\n",
      "\n",
      " - 2450 round - train_metric: 0.179183 - valid_metric: 0.216337\n",
      "\n",
      " - 2500 round - train_metric: 0.178788 - valid_metric: 0.216336\n",
      "\n",
      " - 2550 round - train_metric: 0.178016 - valid_metric: 0.216277\n",
      "\n",
      " - 2600 round - train_metric: 0.177389 - valid_metric: 0.216229\n",
      "\n",
      " - 2650 round - train_metric: 0.176642 - valid_metric: 0.216168\n",
      "\n",
      " - 2700 round - train_metric: 0.175792 - valid_metric: 0.216079\n",
      "\n",
      " - 2750 round - train_metric: 0.175404 - valid_metric: 0.216080\n",
      "\n",
      " - 2800 round - train_metric: 0.174787 - valid_metric: 0.216047\n",
      "\n",
      " - 2850 round - train_metric: 0.173726 - valid_metric: 0.215912\n",
      "\n",
      " - 2900 round - train_metric: 0.173152 - valid_metric: 0.215858\n",
      "\n",
      " - 2950 round - train_metric: 0.172206 - valid_metric: 0.215834\n",
      "\n",
      " - 3000 round - train_metric: 0.171426 - valid_metric: 0.215772\n",
      "\n",
      " - 3050 round - train_metric: 0.170678 - valid_metric: 0.215720\n",
      "\n",
      " - 3100 round - train_metric: 0.170064 - valid_metric: 0.215704\n",
      "\n",
      " - 3150 round - train_metric: 0.169481 - valid_metric: 0.215675\n",
      "\n",
      " - 3200 round - train_metric: 0.168557 - valid_metric: 0.215611\n",
      "\n",
      " - 3250 round - train_metric: 0.167895 - valid_metric: 0.215617\n",
      "\n",
      " - 3300 round - train_metric: 0.167283 - valid_metric: 0.215563\n",
      "\n",
      " - 3350 round - train_metric: 0.166416 - valid_metric: 0.215518\n",
      "\n",
      " - 3400 round - train_metric: 0.165522 - valid_metric: 0.215449\n",
      "\n",
      " - 3450 round - train_metric: 0.164671 - valid_metric: 0.215409\n",
      "\n",
      " - 3500 round - train_metric: 0.164143 - valid_metric: 0.215408\n",
      "\n",
      " - 3550 round - train_metric: 0.163519 - valid_metric: 0.215361\n",
      "\n",
      " - 3600 round - train_metric: 0.162883 - valid_metric: 0.215335\n",
      "\n",
      " - 3650 round - train_metric: 0.162251 - valid_metric: 0.215340\n",
      "\n",
      " - 3700 round - train_metric: 0.161052 - valid_metric: 0.215288\n",
      "\n",
      " - 3750 round - train_metric: 0.160388 - valid_metric: 0.215259\n",
      "\n",
      " - 3800 round - train_metric: 0.159857 - valid_metric: 0.215245\n",
      "\n",
      " - 3850 round - train_metric: 0.159219 - valid_metric: 0.215255\n",
      "\n",
      " - 3900 round - train_metric: 0.158798 - valid_metric: 0.215269\n",
      "\n",
      " - 3950 round - train_metric: 0.158553 - valid_metric: 0.215261\n",
      "\n",
      " - 4000 round - train_metric: 0.157688 - valid_metric: 0.215274\n",
      "\n",
      " - 4050 round - train_metric: 0.157190 - valid_metric: 0.215255\n",
      "\n",
      " - 4100 round - train_metric: 0.156481 - valid_metric: 0.215185\n",
      "\n",
      " - 4150 round - train_metric: 0.155538 - valid_metric: 0.215169\n",
      "\n",
      " - 4200 round - train_metric: 0.154912 - valid_metric: 0.215182\n",
      "\n",
      " - 4250 round - train_metric: 0.154239 - valid_metric: 0.215133\n",
      "\n",
      " - 4300 round - train_metric: 0.153325 - valid_metric: 0.215133\n",
      "\n",
      " - 4350 round - train_metric: 0.152935 - valid_metric: 0.215122\n",
      "\n",
      " - 4400 round - train_metric: 0.152495 - valid_metric: 0.215133\n",
      "\n",
      " - 4450 round - train_metric: 0.151742 - valid_metric: 0.215115\n",
      "\n",
      "- fold3 valid metric: 0.794310\n",
      "\n",
      "\u001b[34m        ... fold : 4\u001b[39m\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 5.826223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 212279\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 6285\n",
      "[50]\ttraining's binary_logloss: 0.399363\tvalid_1's binary_logloss: 0.400735\n",
      "[100]\ttraining's binary_logloss: 0.338771\tvalid_1's binary_logloss: 0.340879\n",
      "[150]\ttraining's binary_logloss: 0.313167\tvalid_1's binary_logloss: 0.315712\n",
      "[200]\ttraining's binary_logloss: 0.30032\tvalid_1's binary_logloss: 0.30302\n",
      "[250]\ttraining's binary_logloss: 0.27255\tvalid_1's binary_logloss: 0.275917\n",
      "[300]\ttraining's binary_logloss: 0.257922\tvalid_1's binary_logloss: 0.261873\n",
      "[350]\ttraining's binary_logloss: 0.248074\tvalid_1's binary_logloss: 0.252459\n",
      "[400]\ttraining's binary_logloss: 0.244345\tvalid_1's binary_logloss: 0.248979\n",
      "[450]\ttraining's binary_logloss: 0.242421\tvalid_1's binary_logloss: 0.24731\n",
      "[500]\ttraining's binary_logloss: 0.231589\tvalid_1's binary_logloss: 0.237333\n",
      "[550]\ttraining's binary_logloss: 0.228307\tvalid_1's binary_logloss: 0.234497\n",
      "[600]\ttraining's binary_logloss: 0.225933\tvalid_1's binary_logloss: 0.232537\n",
      "[650]\ttraining's binary_logloss: 0.220698\tvalid_1's binary_logloss: 0.228268\n",
      "[700]\ttraining's binary_logloss: 0.217763\tvalid_1's binary_logloss: 0.226047\n",
      "[750]\ttraining's binary_logloss: 0.214533\tvalid_1's binary_logloss: 0.223859\n",
      "[800]\ttraining's binary_logloss: 0.212444\tvalid_1's binary_logloss: 0.222532\n",
      "[850]\ttraining's binary_logloss: 0.209652\tvalid_1's binary_logloss: 0.220928\n",
      "[900]\ttraining's binary_logloss: 0.208235\tvalid_1's binary_logloss: 0.220225\n",
      "[950]\ttraining's binary_logloss: 0.208005\tvalid_1's binary_logloss: 0.22029\n",
      "[1000]\ttraining's binary_logloss: 0.206271\tvalid_1's binary_logloss: 0.219419\n",
      "[1050]\ttraining's binary_logloss: 0.205162\tvalid_1's binary_logloss: 0.218975\n",
      "[1100]\ttraining's binary_logloss: 0.202786\tvalid_1's binary_logloss: 0.217892\n",
      "[1150]\ttraining's binary_logloss: 0.202574\tvalid_1's binary_logloss: 0.218034\n",
      "[1200]\ttraining's binary_logloss: 0.201413\tvalid_1's binary_logloss: 0.217578\n",
      "[1250]\ttraining's binary_logloss: 0.200305\tvalid_1's binary_logloss: 0.217214\n",
      "[1300]\ttraining's binary_logloss: 0.199598\tvalid_1's binary_logloss: 0.216971\n",
      "[1350]\ttraining's binary_logloss: 0.199007\tvalid_1's binary_logloss: 0.216777\n",
      "[1400]\ttraining's binary_logloss: 0.197274\tvalid_1's binary_logloss: 0.216189\n",
      "[1450]\ttraining's binary_logloss: 0.196636\tvalid_1's binary_logloss: 0.216079\n",
      "[1500]\ttraining's binary_logloss: 0.196016\tvalid_1's binary_logloss: 0.216014\n",
      "[1550]\ttraining's binary_logloss: 0.194254\tvalid_1's binary_logloss: 0.215486\n",
      "[1600]\ttraining's binary_logloss: 0.192914\tvalid_1's binary_logloss: 0.215207\n",
      "[1650]\ttraining's binary_logloss: 0.191853\tvalid_1's binary_logloss: 0.215044\n",
      "[1700]\ttraining's binary_logloss: 0.191211\tvalid_1's binary_logloss: 0.214997\n",
      "[1750]\ttraining's binary_logloss: 0.190536\tvalid_1's binary_logloss: 0.214922\n",
      "[1800]\ttraining's binary_logloss: 0.190141\tvalid_1's binary_logloss: 0.214942\n",
      "[1850]\ttraining's binary_logloss: 0.189282\tvalid_1's binary_logloss: 0.214811\n",
      "[1900]\ttraining's binary_logloss: 0.188394\tvalid_1's binary_logloss: 0.214665\n",
      "[1950]\ttraining's binary_logloss: 0.18721\tvalid_1's binary_logloss: 0.214414\n",
      "[2000]\ttraining's binary_logloss: 0.18635\tvalid_1's binary_logloss: 0.214322\n",
      "[2050]\ttraining's binary_logloss: 0.1857\tvalid_1's binary_logloss: 0.214259\n",
      "[2100]\ttraining's binary_logloss: 0.185008\tvalid_1's binary_logloss: 0.214132\n",
      "[2150]\ttraining's binary_logloss: 0.184834\tvalid_1's binary_logloss: 0.214205\n",
      "[2200]\ttraining's binary_logloss: 0.183839\tvalid_1's binary_logloss: 0.214076\n",
      "[2250]\ttraining's binary_logloss: 0.183058\tvalid_1's binary_logloss: 0.213951\n",
      "[2300]\ttraining's binary_logloss: 0.182065\tvalid_1's binary_logloss: 0.213773\n",
      "[2350]\ttraining's binary_logloss: 0.181456\tvalid_1's binary_logloss: 0.213683\n",
      "[2400]\ttraining's binary_logloss: 0.180909\tvalid_1's binary_logloss: 0.213668\n",
      "[2450]\ttraining's binary_logloss: 0.180106\tvalid_1's binary_logloss: 0.213553\n",
      "[2500]\ttraining's binary_logloss: 0.179612\tvalid_1's binary_logloss: 0.213555\n",
      "[2550]\ttraining's binary_logloss: 0.178896\tvalid_1's binary_logloss: 0.213503\n",
      "[2600]\ttraining's binary_logloss: 0.178214\tvalid_1's binary_logloss: 0.213464\n",
      "[2650]\ttraining's binary_logloss: 0.177468\tvalid_1's binary_logloss: 0.213383\n",
      "[2700]\ttraining's binary_logloss: 0.176605\tvalid_1's binary_logloss: 0.21325\n",
      "[2750]\ttraining's binary_logloss: 0.176223\tvalid_1's binary_logloss: 0.213254\n",
      "[2800]\ttraining's binary_logloss: 0.175664\tvalid_1's binary_logloss: 0.213238\n",
      "[2850]\ttraining's binary_logloss: 0.174564\tvalid_1's binary_logloss: 0.213132\n",
      "[2900]\ttraining's binary_logloss: 0.173986\tvalid_1's binary_logloss: 0.213085\n",
      "[2950]\ttraining's binary_logloss: 0.173037\tvalid_1's binary_logloss: 0.21297\n",
      "[3000]\ttraining's binary_logloss: 0.172271\tvalid_1's binary_logloss: 0.212924\n",
      "[3050]\ttraining's binary_logloss: 0.171575\tvalid_1's binary_logloss: 0.212892\n",
      "[3100]\ttraining's binary_logloss: 0.170919\tvalid_1's binary_logloss: 0.212866\n",
      "[3150]\ttraining's binary_logloss: 0.170342\tvalid_1's binary_logloss: 0.212856\n",
      "[3200]\ttraining's binary_logloss: 0.169491\tvalid_1's binary_logloss: 0.212833\n",
      "[3250]\ttraining's binary_logloss: 0.168773\tvalid_1's binary_logloss: 0.212771\n",
      "[3300]\ttraining's binary_logloss: 0.168164\tvalid_1's binary_logloss: 0.212736\n",
      "[3350]\ttraining's binary_logloss: 0.167291\tvalid_1's binary_logloss: 0.212673\n",
      "[3400]\ttraining's binary_logloss: 0.166397\tvalid_1's binary_logloss: 0.212621\n",
      "[3450]\ttraining's binary_logloss: 0.165603\tvalid_1's binary_logloss: 0.212589\n",
      "[3500]\ttraining's binary_logloss: 0.165073\tvalid_1's binary_logloss: 0.212534\n",
      "[3550]\ttraining's binary_logloss: 0.164413\tvalid_1's binary_logloss: 0.212526\n",
      "[3600]\ttraining's binary_logloss: 0.163779\tvalid_1's binary_logloss: 0.212505\n",
      "[3650]\ttraining's binary_logloss: 0.163115\tvalid_1's binary_logloss: 0.212494\n",
      "[3700]\ttraining's binary_logloss: 0.161996\tvalid_1's binary_logloss: 0.212424\n",
      "[3750]\ttraining's binary_logloss: 0.161326\tvalid_1's binary_logloss: 0.212385\n",
      "[3800]\ttraining's binary_logloss: 0.160779\tvalid_1's binary_logloss: 0.212362\n",
      "[3850]\ttraining's binary_logloss: 0.160145\tvalid_1's binary_logloss: 0.212347\n",
      "[3900]\ttraining's binary_logloss: 0.159673\tvalid_1's binary_logloss: 0.212334\n",
      "[3950]\ttraining's binary_logloss: 0.159459\tvalid_1's binary_logloss: 0.212329\n",
      "[4000]\ttraining's binary_logloss: 0.158587\tvalid_1's binary_logloss: 0.212254\n",
      "[4050]\ttraining's binary_logloss: 0.15804\tvalid_1's binary_logloss: 0.212252\n",
      "[4100]\ttraining's binary_logloss: 0.157332\tvalid_1's binary_logloss: 0.212232\n",
      "[4150]\ttraining's binary_logloss: 0.156403\tvalid_1's binary_logloss: 0.212204\n",
      "[4200]\ttraining's binary_logloss: 0.155781\tvalid_1's binary_logloss: 0.212176\n",
      "[4250]\ttraining's binary_logloss: 0.155127\tvalid_1's binary_logloss: 0.212151\n",
      "[4300]\ttraining's binary_logloss: 0.154226\tvalid_1's binary_logloss: 0.212138\n",
      "[4350]\ttraining's binary_logloss: 0.153882\tvalid_1's binary_logloss: 0.212148\n",
      "[4400]\ttraining's binary_logloss: 0.153446\tvalid_1's binary_logloss: 0.212155\n",
      "[4450]\ttraining's binary_logloss: 0.15266\tvalid_1's binary_logloss: 0.212168\n",
      "[4500]\ttraining's binary_logloss: 0.152196\tvalid_1's binary_logloss: 0.212171\n",
      " - 0 round - train_metric: 0.670797 - valid_metric: 0.670888\n",
      "\n",
      " - 50 round - train_metric: 0.401824 - valid_metric: 0.403184\n",
      "\n",
      " - 100 round - train_metric: 0.334727 - valid_metric: 0.336882\n",
      "\n",
      " - 150 round - train_metric: 0.315038 - valid_metric: 0.317561\n",
      "\n",
      " - 200 round - train_metric: 0.301538 - valid_metric: 0.304224\n",
      "\n",
      " - 250 round - train_metric: 0.270919 - valid_metric: 0.274318\n",
      "\n",
      " - 300 round - train_metric: 0.256726 - valid_metric: 0.260721\n",
      "\n",
      " - 350 round - train_metric: 0.248601 - valid_metric: 0.252966\n",
      "\n",
      " - 400 round - train_metric: 0.244749 - valid_metric: 0.249365\n",
      "\n",
      " - 450 round - train_metric: 0.242773 - valid_metric: 0.247645\n",
      "\n",
      " - 500 round - train_metric: 0.231154 - valid_metric: 0.236937\n",
      "\n",
      " - 550 round - train_metric: 0.227930 - valid_metric: 0.234154\n",
      "\n",
      " - 600 round - train_metric: 0.225607 - valid_metric: 0.232255\n",
      "\n",
      " - 650 round - train_metric: 0.220816 - valid_metric: 0.228374\n",
      "\n",
      " - 700 round - train_metric: 0.217878 - valid_metric: 0.226144\n",
      "\n",
      " - 750 round - train_metric: 0.214632 - valid_metric: 0.223937\n",
      "\n",
      " - 800 round - train_metric: 0.212515 - valid_metric: 0.222589\n",
      "\n",
      " - 850 round - train_metric: 0.209534 - valid_metric: 0.220858\n",
      "\n",
      " - 900 round - train_metric: 0.208119 - valid_metric: 0.220164\n",
      "\n",
      " - 950 round - train_metric: 0.207894 - valid_metric: 0.220225\n",
      "\n",
      " - 1000 round - train_metric: 0.206327 - valid_metric: 0.219459\n",
      "\n",
      " - 1050 round - train_metric: 0.205062 - valid_metric: 0.218912\n",
      "\n",
      " - 1100 round - train_metric: 0.202701 - valid_metric: 0.217857\n",
      "\n",
      " - 1150 round - train_metric: 0.202615 - valid_metric: 0.218063\n",
      "\n",
      " - 1200 round - train_metric: 0.201454 - valid_metric: 0.217605\n",
      "\n",
      " - 1250 round - train_metric: 0.200340 - valid_metric: 0.217237\n",
      "\n",
      " - 1300 round - train_metric: 0.199634 - valid_metric: 0.216994\n",
      "\n",
      " - 1350 round - train_metric: 0.199049 - valid_metric: 0.216801\n",
      "\n",
      " - 1400 round - train_metric: 0.197197 - valid_metric: 0.216162\n",
      "\n",
      " - 1450 round - train_metric: 0.196672 - valid_metric: 0.216100\n",
      "\n",
      " - 1500 round - train_metric: 0.195942 - valid_metric: 0.215983\n",
      "\n",
      " - 1550 round - train_metric: 0.194184 - valid_metric: 0.215470\n",
      "\n",
      " - 1600 round - train_metric: 0.192949 - valid_metric: 0.215223\n",
      "\n",
      " - 1650 round - train_metric: 0.191794 - valid_metric: 0.215033\n",
      "\n",
      " - 1700 round - train_metric: 0.191147 - valid_metric: 0.214988\n",
      "\n",
      " - 1750 round - train_metric: 0.190564 - valid_metric: 0.214935\n",
      "\n",
      " - 1800 round - train_metric: 0.190078 - valid_metric: 0.214922\n",
      "\n",
      " - 1850 round - train_metric: 0.189318 - valid_metric: 0.214826\n",
      "\n",
      " - 1900 round - train_metric: 0.188336 - valid_metric: 0.214649\n",
      "\n",
      " - 1950 round - train_metric: 0.187233 - valid_metric: 0.214425\n",
      "\n",
      " - 2000 round - train_metric: 0.186295 - valid_metric: 0.214309\n",
      "\n",
      " - 2050 round - train_metric: 0.185722 - valid_metric: 0.214268\n",
      "\n",
      " - 2100 round - train_metric: 0.185026 - valid_metric: 0.214139\n",
      "\n",
      " - 2150 round - train_metric: 0.184862 - valid_metric: 0.214216\n",
      "\n",
      " - 2200 round - train_metric: 0.183865 - valid_metric: 0.214086\n",
      "\n",
      " - 2250 round - train_metric: 0.183005 - valid_metric: 0.213939\n",
      "\n",
      " - 2300 round - train_metric: 0.182090 - valid_metric: 0.213781\n",
      "\n",
      " - 2350 round - train_metric: 0.181485 - valid_metric: 0.213692\n",
      "\n",
      " - 2400 round - train_metric: 0.180857 - valid_metric: 0.213653\n",
      "\n",
      " - 2450 round - train_metric: 0.180055 - valid_metric: 0.213543\n",
      "\n",
      " - 2500 round - train_metric: 0.179626 - valid_metric: 0.213560\n",
      "\n",
      " - 2550 round - train_metric: 0.178848 - valid_metric: 0.213503\n",
      "\n",
      " - 2600 round - train_metric: 0.178227 - valid_metric: 0.213470\n",
      "\n",
      " - 2650 round - train_metric: 0.177481 - valid_metric: 0.213387\n",
      "\n",
      " - 2700 round - train_metric: 0.176625 - valid_metric: 0.213257\n",
      "\n",
      " - 2750 round - train_metric: 0.176238 - valid_metric: 0.213260\n",
      "\n",
      " - 2800 round - train_metric: 0.175617 - valid_metric: 0.213222\n",
      "\n",
      " - 2850 round - train_metric: 0.174579 - valid_metric: 0.213136\n",
      "\n",
      " - 2900 round - train_metric: 0.174007 - valid_metric: 0.213091\n",
      "\n",
      " - 2950 round - train_metric: 0.173053 - valid_metric: 0.212974\n",
      "\n",
      " - 3000 round - train_metric: 0.172286 - valid_metric: 0.212927\n",
      "\n",
      " - 3050 round - train_metric: 0.171529 - valid_metric: 0.212886\n",
      "\n",
      " - 3100 round - train_metric: 0.170931 - valid_metric: 0.212869\n",
      "\n",
      " - 3150 round - train_metric: 0.170359 - valid_metric: 0.212861\n",
      "\n",
      " - 3200 round - train_metric: 0.169447 - valid_metric: 0.212827\n",
      "\n",
      " - 3250 round - train_metric: 0.168784 - valid_metric: 0.212775\n",
      "\n",
      " - 3300 round - train_metric: 0.168178 - valid_metric: 0.212739\n",
      "\n",
      " - 3350 round - train_metric: 0.167305 - valid_metric: 0.212676\n",
      "\n",
      " - 3400 round - train_metric: 0.166411 - valid_metric: 0.212625\n",
      "\n",
      " - 3450 round - train_metric: 0.165563 - valid_metric: 0.212582\n",
      "\n",
      " - 3500 round - train_metric: 0.165034 - valid_metric: 0.212528\n",
      "\n",
      " - 3550 round - train_metric: 0.164424 - valid_metric: 0.212528\n",
      "\n",
      " - 3600 round - train_metric: 0.163793 - valid_metric: 0.212507\n",
      "\n",
      " - 3650 round - train_metric: 0.163134 - valid_metric: 0.212497\n",
      "\n",
      " - 3700 round - train_metric: 0.161957 - valid_metric: 0.212428\n",
      "\n",
      " - 3750 round - train_metric: 0.161287 - valid_metric: 0.212388\n",
      "\n",
      " - 3800 round - train_metric: 0.160740 - valid_metric: 0.212356\n",
      "\n",
      " - 3850 round - train_metric: 0.160107 - valid_metric: 0.212343\n",
      "\n",
      " - 3900 round - train_metric: 0.159682 - valid_metric: 0.212336\n",
      "\n",
      " - 3950 round - train_metric: 0.159417 - valid_metric: 0.212325\n",
      "\n",
      " - 4000 round - train_metric: 0.158548 - valid_metric: 0.212258\n",
      "\n",
      " - 4050 round - train_metric: 0.158052 - valid_metric: 0.212253\n",
      "\n",
      " - 4100 round - train_metric: 0.157347 - valid_metric: 0.212235\n",
      "\n",
      " - 4150 round - train_metric: 0.156417 - valid_metric: 0.212206\n",
      "\n",
      " - 4200 round - train_metric: 0.155794 - valid_metric: 0.212178\n",
      "\n",
      " - 4250 round - train_metric: 0.155139 - valid_metric: 0.212152\n",
      "\n",
      " - 4300 round - train_metric: 0.154243 - valid_metric: 0.212139\n",
      "\n",
      " - 4350 round - train_metric: 0.153844 - valid_metric: 0.212144\n",
      "\n",
      " - 4400 round - train_metric: 0.153411 - valid_metric: 0.212159\n",
      "\n",
      " - 4450 round - train_metric: 0.152671 - valid_metric: 0.212169\n",
      "\n",
      "- fold4 valid metric: 0.798117\n",
      "\n",
      "all valid mean metric:0.798231, global valid metric:0.798350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                              customer_ID    target\n",
       " 0       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  0.000397\n",
       " 1       00000fd6641609c6ece5454664794f0340ad84dddce9a2...  0.001275\n",
       " 2       00001b22f846c82c51f6e3958ccd81970162bae8b007e8...  0.000982\n",
       " 3       000041bdba6ecadd89a52d11886e8eaaec9325906c9723...  0.002219\n",
       " 4       00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...  0.000888\n",
       " ...                                                   ...       ...\n",
       " 458908  ffff41c8a52833b56430603969b9ca48d208e7c192c6a4...  0.002091\n",
       " 458909  ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fd...  0.028120\n",
       " 458910  ffff9984b999fccb2b6127635ed0736dda94e544e67e02...  0.001113\n",
       " 458911  ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf38814...  0.121225\n",
       " 458912  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...  0.002370\n",
       " \n",
       " [458913 rows x 2 columns],\n",
       " None,\n",
       " (0.7982306864785512, 0.7983499947470201))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_config['feature_name'] = [col for col in train.columns if col not in [id_name,label_name,'S_2'] and 'target' not in col]\n",
    "Lgb_train_and_predict(train=train, test=None, config=lgb_config, aug=None, run_id='LGB_with_manual_feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lgb_config['feature_name'] = [col for col in train.columns if col not in [id_name,label_name,'S_2']]\n",
    "## Lgb_train_and_predict(train,test,lgb_config,aug=None,run_id='LGB_with_manual_feature_and_series_oof')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c90c4efae355953581131fe5eb685f809605ddb2abb7edf8826c72b30e930c0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
