{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chris, TFの自作らへんを色々見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow version 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gc, os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"Using TensorFlow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, feat_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "                layers.Dense(feat_dim)\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "feat_dim = 188\n",
    "embed_dim = 64 # embedding size for attention --\n",
    "num_heads = 4 # number of attention heads --\n",
    "ff_dim = 128 # hidden layer size in feed forward network inside transformer --\n",
    "dropout_rate = 0.3\n",
    "num_blocks = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    # input embedding layer --\n",
    "    inp = layers.Input(shape=(13, 188))  # *\n",
    "    embeddings = []  # **\n",
    "    for k in range(11):   # たぶんindex=0~10がカテゴリfeatureで、それを分散表現に変えてる\n",
    "        emb = layers.Embedding(10, 4)\n",
    "        embeddings.append(emb(inp[:, :, k]))\n",
    "    x = layers.Concatenate()([inp[:, :, 11:]]+embeddings) # ***\n",
    "    x = layers.Dense(feat_dim)(x) # つまり、221:[category*4] + [num] -> 188に戻してる --\n",
    "\n",
    "    # transformer blocks --\n",
    "    for k in range(num_blocks):\n",
    "        x_old = x\n",
    "        transformer_block = TransformerBlock(embed_dim, feat_dim, num_heads, ff_dim, dropout_rate)\n",
    "        x = transformer_block(x)\n",
    "        x = 0.9*x + 0.1*x_old  # skip connection --\n",
    "    \n",
    "    # classification head --\n",
    "    x = layers.Dense(64, activation=\"relu\")(x[:,-1,:])\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inp, outputs=outputs)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(loss=loss, optimizer=opt)\n",
    "\n",
    "    return model\n",
    "\n",
    "model1 = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 13, 188)]    0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_28 (S  (None, 13)          0           ['input_7[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_29 (S  (None, 13)          0           ['input_7[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_30 (S  (None, 13)          0           ['input_7[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_31 (S  (None, 13)          0           ['input_7[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_32 (S  (None, 13)          0           ['input_7[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_33 (S  (None, 13)          0           ['input_7[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_34 (S  (None, 13)          0           ['input_7[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_35 (S  (None, 13)          0           ['input_7[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_36 (S  (None, 13)          0           ['input_7[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_37 (S  (None, 13)          0           ['input_7[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_38 (S  (None, 13)          0           ['input_7[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_39 (S  (None, 13, 177)     0           ['input_7[0][0]']                \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " embedding_23 (Embedding)       (None, 13, 4)        40          ['tf.__operators__.getitem_28[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_24 (Embedding)       (None, 13, 4)        40          ['tf.__operators__.getitem_29[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_25 (Embedding)       (None, 13, 4)        40          ['tf.__operators__.getitem_30[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_26 (Embedding)       (None, 13, 4)        40          ['tf.__operators__.getitem_31[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_27 (Embedding)       (None, 13, 4)        40          ['tf.__operators__.getitem_32[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_28 (Embedding)       (None, 13, 4)        40          ['tf.__operators__.getitem_33[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_29 (Embedding)       (None, 13, 4)        40          ['tf.__operators__.getitem_34[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_30 (Embedding)       (None, 13, 4)        40          ['tf.__operators__.getitem_35[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_31 (Embedding)       (None, 13, 4)        40          ['tf.__operators__.getitem_36[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_32 (Embedding)       (None, 13, 4)        40          ['tf.__operators__.getitem_37[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_33 (Embedding)       (None, 13, 4)        40          ['tf.__operators__.getitem_38[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 13, 221)      0           ['tf.__operators__.getitem_39[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'embedding_23[0][0]',           \n",
      "                                                                  'embedding_24[0][0]',           \n",
      "                                                                  'embedding_25[0][0]',           \n",
      "                                                                  'embedding_26[0][0]',           \n",
      "                                                                  'embedding_27[0][0]',           \n",
      "                                                                  'embedding_28[0][0]',           \n",
      "                                                                  'embedding_29[0][0]',           \n",
      "                                                                  'embedding_30[0][0]',           \n",
      "                                                                  'embedding_31[0][0]',           \n",
      "                                                                  'embedding_32[0][0]',           \n",
      "                                                                  'embedding_33[0][0]']           \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 13, 188)      41736       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " transformer_block_4 (Transform  (None, 13, 188)     242664      ['dense_25[0][0]']               \n",
      " erBlock)                                                                                         \n",
      "                                                                                                  \n",
      " tf.math.multiply_8 (TFOpLambda  (None, 13, 188)     0           ['transformer_block_4[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_9 (TFOpLambda  (None, 13, 188)     0           ['dense_25[0][0]']               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 13, 188)     0           ['tf.math.multiply_8[0][0]',     \n",
      " mbda)                                                            'tf.math.multiply_9[0][0]']     \n",
      "                                                                                                  \n",
      " transformer_block_5 (Transform  (None, 13, 188)     242664      ['tf.__operators__.add_5[0][0]'] \n",
      " erBlock)                                                                                         \n",
      "                                                                                                  \n",
      " tf.math.multiply_10 (TFOpLambd  (None, 13, 188)     0           ['transformer_block_5[0][0]']    \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_11 (TFOpLambd  (None, 13, 188)     0           ['tf.__operators__.add_5[0][0]'] \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 13, 188)     0           ['tf.math.multiply_10[0][0]',    \n",
      " mbda)                                                            'tf.math.multiply_11[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_40 (S  (None, 188)         0           ['tf.__operators__.add_6[0][0]'] \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 64)           12096       ['tf.__operators__.getitem_40[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 32)           2080        ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 1)            33          ['dense_31[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 541,713\n",
      "Trainable params: 541,713\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(13, 188)),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 2444)              0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 128)               312960    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 314,250\n",
      "Trainable params: 314,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メモ\n",
    "* inpはバッチサイズを落として2Dで指定してる\n",
    "* pytorchのforward(x)的な書き方で書いてあるからよくわからんな..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChrisのTFモデルを1行ずつやっていく --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 13, 188)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# デモ入力\n",
    "X_train = np.ones(100*13*188).reshape(100, 13, 188)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    inp = layers.Input(shape=(13, 188))\n",
    "    output = layers.Dense(1, activation=None)(inp)\n",
    "    model = keras.Model(inputs=inp, outputs=output)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(loss=loss, optimizer=opt)\n",
    "    return model\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = layers.Input(shape=(13, 188))\n",
    "embeddings = []\n",
    "for k in range(11):\n",
    "    emb = layers.Embedding(10, 4)\n",
    "    embeddings.append(emb(inp[:, :, k]))\n",
    "x1 = layers.Concatenate()([inp[:, :, 11:]]+embeddings) # ***\n",
    "x2 = layers.Dense(feat_dim)(x1)  # feat_dim=188, activation=None --\n",
    "\n",
    "tb1 = TransformerBlock(embed_dim, feat_dim, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "x3 = tb1(x2)\n",
    "x4 = 0.9*x3 + 0.1*x2\n",
    "\n",
    "tb2 = TransformerBlock(embed_dim, feat_dim, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "x5 = tb2(x4)\n",
    "x6 = 0.9*x5 + 0.1*x4\n",
    "\n",
    "x7 = layers.Dense(64, activation=\"relu\")(x6[:, -1, :])  # ここで時間方向の軸を最終行のみスライス --\n",
    "x8 = layers.Dense(32, activation=\"relu\")(x7)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 13)\n",
      "(None, 13, 4)\n",
      "(None, 13, 188)\n",
      "(None, 13, 221)\n",
      "(None, 13, 188)\n",
      "(None, 13, 188)\n",
      "(None, 13, 188)\n",
      "(None, 13, 188)\n",
      "(None, 13, 188)\n",
      "(None, 64)\n",
      "(None, 32)\n",
      "(None, 1)\n"
     ]
    }
   ],
   "source": [
    "print(inp[:, :, k].shape)\n",
    "print(emb(inp[:, :, k]).shape)  # (batch_size, sequence_length) -> <emb> -> (batch_size, sequence_length, embed_dim) --\n",
    "\n",
    "\n",
    "print(inp.shape)\n",
    "print(x1.shape)  # shape[2] = 188 + 4*10 \n",
    "print(x2.shape)\n",
    "print(x3.shape)\n",
    "print(x4.shape)\n",
    "print(x5.shape)\n",
    "print(x6.shape)\n",
    "print(x7.shape)\n",
    "print(x8.shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding layerについて"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n",
       "array([[[1., 2.],\n",
       "        [3., 4.],\n",
       "        [7., 8.]],\n",
       "\n",
       "       [[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.]]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# だいたい辞書みたいな感じ --\n",
    "initializer = tf.keras.initializers.Constant(\n",
    "    value=[\n",
    "        [1, 2],\n",
    "        [3, 4],\n",
    "        [5, 6],\n",
    "        [7, 8],\n",
    "        [9, 10],\n",
    "    ]\n",
    ")\n",
    "embedding_layer = tf.keras.layers.Embedding(5, 2, embeddings_initializer=initializer)\n",
    "embedding_layer(inputs=tf.constant([[0, 1, 3], [0, 1, 2]]))  # 対応するインデックスのinitializerを取り出す --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_45 (Embedding)    (None, None, 2)           10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# で、分散表現のinitalizer自体がパラメータになる。今は5*2なので、#Params = 10 --\n",
    "model = keras.Sequential(\n",
    "    embedding_layer\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerBlockについて\n",
    "* https://data-analytics.fun/2020/07/16/understanding-layer-normalization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## class TransformerBlock(layers.Layer):\n",
    "##     def __init__(self, embed_dim, feat_dim, num_heads, ff_dim, rate=0.1):\n",
    "##         super(TransformerBlock, self).__init__()\n",
    "##         self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "##         self.ffn = keras.Sequential(\n",
    "##             [\n",
    "##                 layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "##                 layers.Dense(feat_dim)\n",
    "##             ]\n",
    "##         )\n",
    "##         self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "##         self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "##         self.dropout1 = layers.Dropout(rate)\n",
    "##         self.dropout2 = layers.Dropout(rate)\n",
    "## \n",
    "##     def call(self, inputs, training):\n",
    "##         attn_output = self.att(inputs, inputs)\n",
    "##         attn_output = self.dropout1(attn_output, training=training)\n",
    "##         out1 = self.layernorm1(inputs + attn_output)\n",
    "##         ffn_output = self.ffn(out1)\n",
    "##         ffn_output = self.dropout2(ffn_output, training=training)\n",
    "##         return self.layernorm2(out1 + ffn_output)\n",
    "## \n",
    "## feat_dim = 188\n",
    "## embed_dim = 64 # embedding size for attention --\n",
    "## num_heads = 4 # number of attention heads --\n",
    "## ff_dim = 128 # hidden layer size in feed forward network inside transformer --\n",
    "## dropout_rate = 0.3\n",
    "## num_blocks = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13, 188)\n",
      "(1, 13, 188)\n",
      "(1, 13, 188)\n",
      "(1, 13, 188)\n",
      "(1, 13, 188)\n"
     ]
    }
   ],
   "source": [
    "#inp = layers.Input(shape=(13, 188))\n",
    "\n",
    "inp = tf.constant(np.ones(1*13*188).reshape(1, 13, 188), dtype=tf.float32)\n",
    "att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "att_out = att(inp, inp)\n",
    "\n",
    "ln1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "out1 = ln1(inp + att_out)\n",
    "\n",
    "ffn = keras.Sequential([\n",
    "    layers.Dense(ff_dim, activation=\"gelu\"),\n",
    "    layers.Dense(feat_dim)\n",
    "])\n",
    "\n",
    "ffn_out = ffn(out1)\n",
    "ln2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "out2 = ln2(out1 + ffn_out)\n",
    "\n",
    "\n",
    "print(inp.shape)\n",
    "print(att_out.shape)\n",
    "print(out1.shape)\n",
    "print(ffn_out.shape)\n",
    "print(out2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNormalizationについて --\n",
    "* https://keras.io/api/layers/normalization_layers/layer_normalization/\n",
    "* https://cvml-expertguide.net/terms/dl/layers/batch-normalization-layer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       "array([[ 0., 10.],\n",
       "       [20., 30.],\n",
       "       [40., 50.],\n",
       "       [60., 70.],\n",
       "       [80., 90.]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tf.constant(np.arange(10).reshape(5, 2)*10, dtype=tf.float32)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       "array([[-0.99998,  0.99998],\n",
       "       [-0.99998,  0.99998],\n",
       "       [-0.99998,  0.99998],\n",
       "       [-0.99998,  0.99998],\n",
       "       [-0.99998,  0.99998]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = layers.LayerNormalization(axis=1)\n",
    "output = layer(data)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       "array([[-1.4142127 , -1.4142127 ],\n",
       "       [-0.70710635, -0.70710635],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.70710635,  0.7071065 ],\n",
       "       [ 1.4142127 ,  1.4142128 ]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = layers.LayerNormalization(axis=0)\n",
    "output = layer(data)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c9dc2bb3660e5394dad5ee4570d30151623b1a4b0311416933d32e8c68e1128"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
